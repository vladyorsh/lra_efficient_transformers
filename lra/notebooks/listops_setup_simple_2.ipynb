{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0_XMgEPOh7c",
    "outputId": "503b8a32-1e30-4bf9-ff22-1cee08e2afe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 28 09:11:48 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGGnygXCDXLf",
    "outputId": "18aadbbf-fc34-41bc-e895-730006e968a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'long-range-arena'...\n",
      "remote: Enumerating objects: 474, done.\u001b[K\n",
      "remote: Counting objects: 100% (474/474), done.\u001b[K\n",
      "remote: Compressing objects: 100% (195/195), done.\u001b[K\n",
      "remote: Total 474 (delta 330), reused 418 (delta 278), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (474/474), 153.25 KiB | 1.00 MiB/s, done.\n",
      "Resolving deltas: 100% (330/330), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/google-research/long-range-arena.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SzR93fCuewGt",
    "outputId": "6e13ff2f-3d85-49f6-956e-e0e98aabeb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-28 09:11:52--  https://storage.googleapis.com/long-range-arena/lra_release.gz\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.153.128, 142.250.145.128, 74.125.143.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.153.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8288700910 (7.7G) [application/octet-stream]\n",
      "Saving to: ‘lra_release.gz’\n",
      "\n",
      "lra_release.gz      100%[===================>]   7.72G  32.9MB/s    in 3m 19s  \n",
      "\n",
      "2021-11-28 09:15:11 (39.8 MB/s) - ‘lra_release.gz’ saved [8288700910/8288700910]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/long-range-arena/lra_release.gz\n",
    "!gzip -d lra_release.gz\n",
    "!tar -xf lra_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrMqZQQmd-vh"
   },
   "outputs": [],
   "source": [
    "#Execute if A100 is the current GPU\n",
    "\n",
    "!pip3 install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxhp_XoYk9lj",
    "outputId": "c63197fb-204d-4256-c135-587c6d642e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_text\n",
      "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 13.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorflow<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.7.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.37.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.12.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.15.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.42.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (12.0.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.13.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.17.3)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.19.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.10.0.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.22.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.5.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (57.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.23.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.1)\n",
      "Installing collected packages: tensorflow-text\n",
      "Successfully installed tensorflow-text-2.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATXw-aRr0sg-",
    "outputId": "9980e4ee-d608-4dce-f2c3-e4840da6ead4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/long-range-arena\n",
      "INFO:tensorflow:/content/lra_release/listops-1000/basic_train.tsv\n",
      "INFO:tensorflow:/content/lra_release/listops-1000/basic_val.tsv\n",
      "INFO:tensorflow:/content/lra_release/listops-1000/basic_test.tsv\n",
      "INFO:tensorflow:Finished preprocessing\n",
      "INFO:tensorflow:Building vocab\n",
      "INFO:tensorflow:Processed 0\n",
      "INFO:tensorflow:Processed 1000\n",
      "INFO:tensorflow:Finished processing vocab size=15\n"
     ]
    }
   ],
   "source": [
    "%cd /content/long-range-arena\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lra_benchmarks.listops.input_pipeline import get_datasets\n",
    "\n",
    "batch_size=4\n",
    "accumulation_steps=32 // batch_size\n",
    "max_length=2000\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset, encoder = get_datasets(1, 'basic', data_dir='/content/lra_release/listops-1000/', batch_size=batch_size, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GV8HdFQJN8Pj",
    "outputId": "254e8312-d4c2-4168-a5b7-619ef47b1e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MIN [SM 6 7 [MIN 4 5 5 7 0 3 [MAX [MAX 1 [MED [MIN 0 9 8 [MED 4 [MAX 0 8 3 7 3 X 6 1 X 2 [MED 1 3 4 [SM 6 2 5 3 5 6 4 4 7 X 6 [SM 0 9 9 X X 5 8 2 X 7 0 1 8 8 [MAX 8 4 7 9 9 X X 8 [SM [SM 0 1 6 9 3 X 3 1 3 6 [SM 0 0 [MAX 4 [MIN 0 1 3 1 9 2 0 8 X 6 2 6 [MAX 0 8 9 0 3 2 3 4 6 4 X X 6 3 5 3 [MIN 5 2 4 [MAX 1 4 1 2 8 4 3 X 1 7 0 1 9 [MIN 6 9 3 2 0 X X [MAX 0 [SM 5 8 6 X [MIN 7 7 7 6 2 2 1 6 9 5 X [MAX 1 4 7 3 9 X 0 0 7 X X [SM 5 7 5 4 4 0 7 [MED 8 1 2 7 8 8 X [MED [MED 9 2 5 X 0 5 6 0 3 3 0 X 7 X [MIN 0 9 X 5 2 X X 8 6 1 7 X X 0 4 [MED 8 1 0 2 1 8 [SM 9 6 1 [MIN [SM 0 7 6 9 5 6 X [MAX 4 1 9 [MIN 5 8 [SM [MED 2 0 6 6 2 X 1 0 6 X [MAX [MAX 3 7 8 8 4 9 9 6 0 3 X 5 0 X X 7 0 1 X 8 [MAX 8 [SM 5 2 4 8 5 X 5 X X 7 [MAX 1 [MIN 0 8 1 0 X 1 3 [SM 9 2 5 1 3 X 9 [MIN 6 9 [MIN [MIN 8 7 [SM 9 4 4 0 1 X 2 3 [MIN 6 5 6 2 0 3 9 1 8 X 2 4 8 3 X [MAX 2 0 3 6 X 5 3 8 1 X [MAX 7 0 3 X X 9 3 X 3 8 X X 6 8 1 [MAX 2 5 0 [MAX 1 4 [MED 2 [MAX 7 [SM 7 8 5 9 X 8 [SM 0 [MIN 3 4 3 0 [MED 2 1 5 2 3 2 4 5 7 X 4 [MAX 4 1 9 5 9 6 5 4 2 9 X 5 5 7 X [SM 3 7 X [SM 4 [MAX 6 5 4 4 8 X 8 1 9 0 3 9 7 [MAX 1 6 8 9 9 1 7 X X 7 [MIN 2 [SM 7 5 0 2 6 5 5 6 X X 3 5 6 X 4 2 1 9 8 [MIN 4 4 9 [MED 5 3 5 [MED 5 8 5 9 3 9 6 X 7 X 1 6 X X 8 0 4 7 5 8 [MAX 8 0 0 [MAX 8 [MIN [MIN 2 1 5 2 9 6 4 X [MIN 1 0 X 2 7 [SM 4 5 1 2 X [MIN 1 3 2 6 8 1 7 8 7 1 X 4 X 5 X 2 9 5 7 [MIN 4 5 0 X 4 X 6 X X 4 4 X X 1 9 [MED 6 [MED 9 0 X 5 [MED 2 3 8 [MED 3 0 6 2 0 2 8 X X 7 5 6 8 X 5 X\n",
      "[SM 2 2 [MAX 5 [MAX [MED 8 8 9 3 6 3 [MIN 0 [SM 2 3 5 X 7 [SM 9 [SM [MIN 9 6 3 [MIN 1 1 9 9 7 7 5 X 3 1 7 [SM 8 3 5 3 8 5 0 X 6 [MIN 2 7 5 1 0 3 1 1 4 3 X X [MED [SM 9 9 0 X 6 8 9 [SM 9 6 1 1 X 6 2 [MAX 6 2 8 2 3 0 X X X [MED [MED 4 [MIN 8 6 1 9 1 X [MAX 3 8 8 8 9 X 9 5 2 X 4 2 [SM 9 2 5 X 9 [SM 4 [MIN 1 3 1 8 6 4 X 2 [MED 6 1 8 1 X 2 5 0 [MIN 1 7 X X 9 7 9 X X 6 7 [MAX 5 7 X 0 7 8 X [SM 8 2 [MIN 6 [MIN [MIN 3 5 8 6 3 7 [MIN 0 2 7 6 X [MED 8 3 6 6 4 4 X 9 [MIN 8 5 3 6 1 9 8 X X 7 9 8 8 7 [MED 4 1 [MIN 9 6 X 1 X X [MED [MAX 5 3 [MAX 8 5 6 3 0 X 2 3 [MAX 3 7 1 5 7 9 8 3 9 3 X X 2 5 3 X 4 [SM 2 8 5 1 [MAX 0 8 9 6 8 1 [MIN 0 7 1 8 9 5 2 X [MAX 5 1 8 8 0 4 X 2 X 7 0 6 X X X 7 X 0 [MAX 6 9 X [SM [MAX 3 [MIN 2 6 7 8 [MAX 4 [SM 1 6 2 4 4 X 6 [MED 7 4 X [MED 1 2 [MED 7 4 X 6 [MIN 1 1 X 7 1 7 X 1 X 1 3 1 X 3 3 8 3 8 [MIN 4 7 9 7 5 [MIN [SM [SM 6 1 8 3 1 3 9 0 2 3 X 6 [MAX 7 3 2 X [MED 2 7 3 6 2 0 5 9 9 X [SM 4 6 0 1 1 0 3 X 3 3 9 [MIN 8 9 2 0 5 8 5 3 X X 8 6 6 3 X [MED 5 9 [MIN [SM 9 4 0 4 9 3 X 8 [MED 3 3 7 0 6 9 0 X 0 [MED 7 3 7 4 0 6 4 4 8 2 X X 8 9 0 [MIN 3 5 X [MIN [SM 1 5 2 7 7 6 9 X 1 2 6 [MAX 2 9 4 3 X 5 [SM 8 4 5 1 6 0 4 5 X 1 X X X [SM [SM 0 [MAX [MAX 8 0 2 6 7 5 X 3 2 9 8 7 6 [MIN 5 3 6 9 X [SM 4 1 3 2 1 X X 8 [MED 8 1 2 2 1 0 4 6 [SM 9 6 1 8 0 9 7 X X 1 7 1 7 7 4 X 0 [MAX 0 2 8 6 9 [MIN 9 2 X 0 8 X 3 [MIN [MED 3 [MED 0 2 2 9 8 9 6 X 2 4 X [SM 7 2 5 6 8 [MIN 4 1 2 4 2 3 6 3 0 X X 9 7 [MIN [MED 6 6 5 X 1 [MED 7 1 9 5 3 0 6 9 2 9 X 1 [MAX 2 9 7 5 1 X 6 5 [MED 3 3 0 8 8 8 X [MAX 8 4 8 4 8 X 7 X 0 X 3 [MIN 2 1 0 [MED [MED 0 3 0 7 2 9 5 8 7 X 6 3 3 [MAX 7 6 1 0 2 7 6 X 3 X [SM 4 2 1 7 6 5 9 3 4 [SM 6 4 4 0 5 3 6 0 X X 5 X 2 X X 2 6 [SM 1 3 2 4 3 0 X 6 [MIN 8 8 6 4 [MED 1 3 4 X 0 X X X 9 [MAX 2 3 2 [MIN 4 [MIN 4 [MIN 3 6 [MAX [MIN [MED 6 0 3 7 2 1 7 6 X [MIN 0 8 5 0 2 6 8 6 0 9 X 6 [MED 4 4 4 8 2 7 4 0 X 6 9 2 0 [SM 1 5 X 8 X 3 3 [MIN 7 1 9 X [MED 9 5 5 4 9 [MED 9 3 8 5 3 2 4 X [MAX 6 8 1 6 8 6 3 5 X 8 1 7 X 8 [MIN 1 4 [MED 8 9 3 5 6 6 2 2 X [MIN 8 0 6 7 7 8 1 1 9 9 X 2 [MIN 1 6 7 3 9 8 7 7 7 3 X [SM 1 9 7 6 8 X 6 9 6 X 4 X 3 7 2 3 5 7 X 1 1 [MAX [MIN 4 1 4 0 [MAX [MAX 9 4 3 X 0 1 [SM 7 9 4 7 1 X [MIN 6 4 8 1 X 8 2 [MED 6 1 X 3 [MAX 5 2 5 8 9 8 X X 2 X 6 7 5 [MED 6 2 [MIN 7 [MAX 2 0 3 6 4 0 0 X [MIN 5 1 6 3 9 X 9 X [SM [MIN 4 5 1 1 3 2 X 0 9 3 X 8 3 3 8 [MIN 5 1 8 7 X X 2 X 1 [MIN 1 1 6 [SM 2 0 3 X 6 [MIN [SM 4 2 [SM 5 3 8 X 0 X [MAX 4 4 5 7 7 [MIN 5 9 3 7 3 4 8 X X X 6 1 [MIN 0 [MIN 4 5 8 2 4 6 9 8 X 5 [MIN [MIN 9 2 X 0 0 [MAX 2 1 X 3 [MED 6 0 3 4 3 9 5 X 5 X X X X 6 3 2 [MAX 3 9 X 6 5 5 X 5 2 [MIN 2 6 [MIN [SM [MED [MIN 7 3 [SM 8 0 8 3 6 X 7 4 9 4 5 [MIN 6 6 1 4 X 2 X 8 4 7 [MAX 6 6 [MED 5 3 6 9 2 8 5 2 7 X 9 7 9 3 5 X 0 X 6 2 8 3 X [MIN 4 6 6 7 [MIN 3 3 1 1 0 X X [SM 6 6 8 [MIN 4 4 3 5 4 X 0 X 9 [MED [MIN [MED 1 [MED 8 3 7 9 7 6 X 3 9 9 [MAX 3 8 1 0 X [MAX 5 1 1 3 6 0 4 X [MIN 4 9 4 6 3 2 2 X 1 1 X 0 [MAX 6 3 [MAX 1 5 4 9 8 9 6 5 X 8 X X 2 [SM [MAX 8 5 7 [MIN 3 2 2 X 6 3 X 4 4 [MED [SM 5 0 X [MED 4 8 4 1 9 9 6 3 X 4 [MED 1 0 1 4 6 3 8 X X [MAX [MED 3 7 9 9 8 0 0 3 X 1 4 5 X X 5 X 1 6 9 [MAX 7 3 5 7 1 [SM 7 1 [SM 3 3 4 X 3 X 6 7 3 1 X [MAX 4 8 5 2 9 9 X X X 9 6 X 5 1 1 8 X 7 [MAX [MIN 2 7 2 7 [MIN 0 2 9 [MAX 5 [MAX [MED [MAX [MED 4 0 3 7 X 6 3 X [SM 2 4 X [SM 0 0 2 4 8 6 X 6 9 [SM 2 [SM 3 1 8 7 4 7 6 7 6 2 X 1 3 X 3 0 1 8 X 7 9 1 2 9 8 [MAX 1 3 1 [MIN 3 2 X 4 7 X 5 X [MIN 5 6 X 9 6 6 [SM 2 7 [MED 2 [MED 2 4 8 6 4 1 X 5 2 1 1 3 [MIN 3 [MAX 2 9 4 3 8 5 3 6 0 3 X 4 4 3 4 5 1 2 2 X 6 X 5 [MAX 2 0 [SM 3 [MAX 7 5 4 9 1 9 3 9 7 X 0 1 5 [MIN 2 8 9 0 8 8 5 5 7 4 X X [MED 9 3 [MIN 1 2 4 2 9 X 6 [MAX 1 9 0 6 0 0 6 X 7 X [MIN [SM 2 2 4 X 1 5 9 0 4 X 8 [MAX 7 4 [MAX 7 0 8 5 1 8 X 3 1 6 1 [MIN 5 9 1 2 5 7 1 4 X 6 7 X [SM [MIN 4 7 6 7 9 5 4 5 3 4 X 1 7 9 1 3 0 2 X X [MIN 9 2 4 [MAX 6 5 0 6 6 0 4 [MIN 7 7 7 1 X X [MAX 7 [SM 5 6 3 X 0 X 3 3 6 [MAX [SM 1 3 0 1 X 8 8 6 6 0 8 0 7 X X X 7 4 X 7 1 [MED [SM 4 1 X 6 1 [MAX 1 0 X 7 X 3 7 X [MAX 3 3 9 8 6 [MAX 9 2 7 4 [SM 8 9 0 4 1 X [SM 7 7 6 8 4 3 1 3 X X 5 8 X 7 8 1 [MAX [MIN [MAX 8 6 [MAX [MAX 9 2 X [MED [MED 0 8 5 3 6 3 3 9 9 3 X 6 [MAX 5 6 3 1 7 2 1 5 6 X 3 0 1 [MIN 4 0 3 3 X 9 X 7 5 6 X 7 4 X 7 3 8 4 [SM [SM 7 3 0 2 2 2 5 2 0 8 X [MIN 6 8 0 6 [MIN 1 9 [MAX 7 1 8 8 8 8 0 0 0 X 9 X 7 X 0 9 5 1 X 3 [SM 9 4 [MAX 9 6 1 [SM [SM 0 4 1 3 4 8 8 X 4 [MED 5 3 6 3 9 7 X 0 1 9 8 2 3 X 6 8 [SM 5 6 1 [MED 3 1 6 3 4 4 5 6 3 7 X [MAX 8 1 5 7 7 X 4 1 X 6 2 X 5 0 [MIN [SM 2 [MED 8 2 4 3 5 5 2 7 X [MAX 0 8 X 5 [MIN 6 4 0 6 X 5 2 X 7 0 0 0 7 [MIN 2 1 8 2 2 X 1 6 2 X 1 X [MAX [MAX 5 [MED 7 [MIN 6 9 3 1 5 3 7 6 8 X 8 1 4 1 9 9 [MAX 3 7 8 3 5 X X 1 9 7 X 1 5 4 1 3 5 X X 9 9 X X 4 2 8 1 2 X 5 X\n",
      "[MED 4 9 2 4 6 4 8 [MED 8 4 7 2 [MED 8 6 6 [MIN 8 7 [MAX 8 0 5 8 [MED [MIN [MED [MED 9 8 X 6 [MED 1 1 4 3 0 4 6 X 6 [MED 0 4 0 0 0 3 9 6 X X [SM 9 [MAX 2 6 2 X X 6 2 7 5 0 6 9 1 X 6 X [MAX [MED 1 4 1 7 [MAX 4 5 5 5 8 1 6 5 4 X [MED [MED 4 5 2 8 9 7 X 0 4 [MAX 1 0 X 6 7 [MIN 6 0 7 2 9 2 X 0 X 2 3 6 2 X 5 4 [SM 3 4 3 8 8 X [MAX 6 3 1 7 7 0 X 0 8 [MED 9 [MAX 0 0 6 8 4 1 X [MIN [SM 5 2 4 8 3 5 6 6 9 X 4 [SM 0 0 0 3 0 0 7 6 0 X 4 9 3 0 4 X 5 2 X 0 6 X [MAX [MAX 5 9 [MIN 4 4 7 9 [MAX 9 1 X X 9 [MAX 5 8 4 [SM 9 9 3 8 8 5 9 7 4 9 X 0 6 8 5 1 [MAX 7 7 5 7 4 7 X X [MAX 3 [MED 5 1 2 3 8 X 7 [MIN 7 5 X 8 0 9 2 [SM 5 5 3 9 3 2 X X 8 X 0 [MAX 6 0 3 [MAX 0 2 0 [SM 6 5 0 2 7 3 7 X 1 X [MAX [SM 2 5 6 0 6 1 4 7 X 8 4 8 3 [MAX 4 7 X [MED 6 9 9 1 9 8 1 7 9 X X 0 5 X 0 9 X 4 [MAX 0 [SM 3 0 0 4 6 0 7 [SM 9 9 6 7 1 3 0 0 X X 7 3 5 [SM 7 2 1 9 X 1 5 [MIN 1 5 8 X X X 6 X [MIN 9 3 X 5 1 X [SM 0 [MIN 6 6 5 8 2 2 9 4 3 3 X [MIN 0 7 [MED 1 3 4 [MAX 4 8 6 5 0 4 3 X [MED [MAX 7 9 [MAX 2 [MED 9 1 9 4 5 2 8 X [MED 6 1 4 8 6 6 2 5 6 4 X 7 5 0 7 3 [SM 9 8 4 5 3 1 X 1 X 5 X 5 X 0 [MAX 2 [MIN [MIN [MAX 7 8 5 4 4 0 2 7 7 X 4 2 5 X 8 7 X 6 5 6 [MAX 0 5 3 9 3 0 0 3 2 4 X 9 9 8 X X 8 [MIN 3 0 7 2 [MIN [MED 0 8 X [SM 9 [MED 9 6 5 7 X 5 1 X 8 8 1 [MIN 0 2 [SM [MED 9 6 5 7 2 0 2 3 X 2 5 0 8 X [MAX 8 7 1 6 0 X [MIN 9 7 0 [SM 3 3 8 9 9 6 5 X X [MIN [MAX 1 7 3 8 7 6 4 9 0 5 X 6 [SM 4 6 6 X 1 3 6 8 X 5 0 X X 0 0 [MED 1 4 0 6 [MIN 0 3 0 X 9 4 3 2 X 1 X 0 [MIN [SM 0 6 9 3 [MAX 4 3 [MED [MAX 6 7 5 1 0 6 6 3 X 4 [SM 6 8 8 8 0 0 X 6 8 7 X 3 [MAX [MAX 9 3 0 2 1 8 9 4 6 9 X [SM 8 1 7 6 9 7 2 X 4 2 [MED 1 7 8 6 0 2 X 0 X 6 [MED 8 6 3 1 6 9 [MIN 0 2 3 5 1 X 2 9 X X 7 X 9 [MAX 6 3 [MAX 8 7 1 2 6 [MED 4 3 0 [MIN 6 0 4 3 0 2 1 0 9 1 X 5 [MED 7 9 X 4 [MED 6 2 0 3 8 8 8 5 X 6 X 0 3 8 5 X X 8 7 [MIN 1 9 [SM [MIN 4 [MED 1 3 1 8 7 0 X X [MAX [MAX 9 5 6 X 1 5 [SM 8 1 9 X 6 2 6 2 9 6 X 3 X X 8 [MED 2 6 [MED 1 5 8 [MIN 0 3 0 6 4 0 X X 8 7 X 4 [MED 1 6 7 [MED [MIN 4 0 2 2 [SM 6 2 6 8 X 8 X [MAX 9 [MED 8 9 X 4 3 [MED 0 6 0 6 8 2 7 6 0 5 X X 7 7 4 1 5 3 [MAX [MED 6 2 9 4 2 3 X 4 [MED 6 2 6 4 3 5 X 5 2 2 8 3 X X 3 0 X X X [SM 9 4 [MED 0 4 7 9 [MED [MED 7 5 7 8 5 [SM 8 0 7 5 0 1 X 7 X 3 X 0 2 [MED 9 0 6 2 X [MED 0 6 [SM 5 [SM [MED 4 4 0 1 6 3 0 1 3 4 X 0 8 9 5 2 [MED 9 2 2 9 3 6 6 1 X 2 X 9 [SM [SM 7 2 7 8 2 6 9 X 7 8 X 9 X 1 7 7 2 [SM 4 0 4 7 4 5 [MIN [MED 2 5 3 5 3 3 2 3 0 3 X [MIN 0 7 X X [SM 5 [SM 5 2 7 4 1 7 0 9 9 2 X [MAX 2 9 6 0 5 6 2 5 X 0 5 2 1 6 8 X 4 X [MIN 3 [MIN 6 [MED 8 7 4 2 X 2 7 9 4 8 [MIN 5 9 5 7 X 6 1 X 0 X 1 X [SM [SM [MAX [MAX 6 5 9 9 4 2 5 2 9 X 4 6 1 5 0 X 8 X 6 X X 9 1 [SM [MED [SM 0 9 X [SM 6 [MED 7 6 5 5 9 7 [MED 0 0 9 6 3 9 6 X [MIN 5 4 X X 9 9 [SM 4 2 3 1 3 X 4 1 7 X 3 1 5 9 [SM [MIN [MIN 7 4 X 6 [SM 2 1 8 5 1 7 4 X 6 6 [MIN 3 0 1 1 5 8 4 X 6 X 4 5 X 1 5 X 4 9 [MAX 7 2 [SM [MAX 5 4 7 6 [MIN 3 4 5 0 5 X 4 9 X [MIN 1 7 [MED 7 0 9 2 0 9 0 9 5 4 X 8 0 9 [MIN 6 8 6 9 1 X 0 0 X 0 5 9 [SM 0 7 7 2 [MAX 4 1 4 3 X 2 [MAX 1 2 0 1 X X 5 [MIN 7 2 2 7 0 8 5 0 2 [MED 1 3 1 6 1 2 8 6 X X X 8 8 1 X 9 3 [MAX 4 6 9 8 0 X 0 [MIN 1 4 [SM [MAX 5 [MIN 9 0 X X [MED 0 [MAX 8 4 7 9 1 4 7 7 X [MIN 6 4 3 4 9 4 2 5 4 X [MIN 2 2 5 9 5 5 5 3 X 5 2 7 6 X 2 4 9 3 5 9 X X X X 0 8 7 X 7 0 1 8 X X\n",
      "[MAX [MIN [MIN [MAX [MED [MIN 2 8 9 2 7 9 [MIN [MAX 6 1 9 5 5 4 4 [MIN 8 3 7 X X 7 3 X [SM 1 8 5 1 0 6 X X 7 [MED 0 [MED 7 2 2 6 5 6 [MAX 6 [MED 5 0 9 3 4 0 9 8 1 2 X 7 [MED 0 2 X 4 6 6 3 [MIN 5 0 0 5 9 2 7 X X 1 4 [MED 2 4 1 9 5 8 9 5 1 0 X X 4 [MIN 9 6 1 [SM [MED 9 6 5 X 6 2 8 7 5 X X 4 3 4 9 X 1 3 7 3 8 2 0 X 5 7 7 5 3 [MED 0 [MAX [SM [SM 9 7 5 6 2 X 1 5 X 4 1 3 2 3 4 [MIN [MED [SM 3 0 4 2 9 3 1 8 2 X 3 6 1 9 7 [MED 2 9 0 0 0 8 8 X 3 [SM 9 2 4 3 1 1 6 9 2 X X [MIN 9 8 [MAX 0 6 1 7 7 4 6 X 2 X 2 4 5 3 8 5 6 X 2 X 5 4 9 X X 3 0 [SM 3 [MIN 4 5 8 5 6 4 4 [MED 6 [SM 4 6 [SM 4 [MAX 8 8 5 8 7 3 X 2 0 [MIN 4 9 2 6 2 5 X 5 [MED 6 8 5 4 5 4 4 9 9 X 4 9 [MIN 1 7 6 7 X X 1 [MED 4 [MED 4 3 2 1 X 5 2 1 5 X 0 4 X X 7 4 X [MED 1 [MIN 8 4 [MIN 1 [MIN [MIN 6 7 3 3 4 2 8 X 4 4 3 2 7 2 2 1 X [MED [MED 6 0 6 2 2 X 7 X 5 1 4 8 1 6 X 3 [MED 6 1 4 0 X [MED 9 2 [MAX 8 [MIN 4 3 5 1 4 X 9 4 5 2 5 0 7 [MAX 2 3 9 1 6 1 6 X X 8 X 0 6 [MED 3 7 8 8 8 1 1 [MED 4 2 5 4 6 5 [MED 8 2 9 X [MIN 7 6 2 7 9 5 X [SM 2 2 1 4 9 0 X X X X 8 8 6 X [SM 0 [MIN 2 5 6 X 9 7 [MIN 4 5 9 9 8 9 6 5 X [MIN 0 [MED 0 8 5 [SM [MAX 2 6 6 5 5 X 2 X 9 [MAX [MAX 5 6 6 3 1 5 2 8 6 X 5 5 X 7 5 [MAX 0 [MIN 3 4 4 8 2 X 4 X X 2 7 1 8 [MED 6 7 0 X 0 5 X 1 [MIN 3 2 [SM 6 [MED [MIN 7 7 1 3 X 0 4 3 9 7 X 6 X 8 5 8 5 5 X 9 X [MED 0 0 [MED [MIN 2 3 7 7 X [MED 4 9 7 7 3 3 6 2 8 6 X 4 X 4 9 2 X [MIN 6 5 7 5 7 7 4 X 6 8 9 8 X [MED 1 0 8 X 2 2 5 7 X [MAX 8 [MED 9 4 8 4 0 [MIN 7 7 [MIN 7 0 6 7 8 [MAX [MED 0 9 1 [MED 7 5 4 1 8 X 2 X [SM 8 7 8 3 X 9 X 3 [MAX 7 8 7 [MIN 6 3 5 [MED 0 7 X 9 4 X 1 1 [MAX [SM 0 6 0 6 X 7 7 [MED 3 1 3 2 1 0 X X 8 X X 8 9 9 X 3 2 0 X [MIN 3 5 2 X [MAX [MED 4 [MIN 8 2 X [MIN 3 9 6 [SM 3 4 [MAX 1 8 7 1 8 9 2 [MAX 2 1 5 7 X [SM 8 6 9 X X 7 [SM 5 [MAX 9 5 7 0 X 1 4 X 5 2 0 [SM 3 3 8 8 5 4 X 7 X [MAX 2 5 5 [MED 3 [MED 7 1 3 9 0 X [MAX 4 7 8 3 5 0 4 2 2 5 X 0 X 6 9 4 [SM 1 6 X X 0 7 9 X 3 [MED 7 1 [MAX 8 5 8 [MED [MIN 8 2 0 4 4 3 8 1 7 8 X 5 8 3 3 7 [MAX 7 2 5 7 X X 0 7 X 1 8 8 1 4 4 [MAX 3 7 8 3 [MIN 0 [SM 6 3 5 1 4 1 8 2 6 5 X 0 9 8 6 X 9 2 3 [MIN 7 2 6 7 0 8 6 3 2 X [SM 0 4 [MAX 0 1 7 2 9 3 X 9 8 [MIN 2 8 2 3 4 7 9 6 X 8 X X X 1 9 2 X 5 2 [MAX 1 0 1 X [SM 6 5 6 3 [MIN [MED 7 [SM 4 5 5 8 4 5 X 2 X 2 8 8 [MAX 6 1 [MIN 5 [SM 1 5 6 5 X 8 2 [MED 1 1 7 7 1 X X 4 8 2 2 X 0 9 X [MIN 0 3 1 X [SM [MED 9 5 8 6 2 [SM 5 0 5 3 5 X [MIN 1 2 3 4 9 7 X 5 X 2 0 4 7 [MAX 8 8 2 5 9 9 [MAX 0 1 3 1 7 X 9 X X 3 1 X 0 [MAX 3 6 0 2 1 8 0 3 5 2 X 4 2 X [MIN 0 3 9 [SM 2 1 [MED 5 7 5 1 0 6 2 3 9 [MAX 0 [SM [MAX 6 5 9 0 3 3 X [SM 9 1 2 6 2 X 6 7 [MIN 9 5 0 3 1 3 2 9 7 2 X [MAX 1 9 6 9 8 5 2 2 9 2 X [MIN 7 5 2 2 4 9 6 5 8 X [MED 0 7 7 X [MAX 4 4 1 1 1 5 0 X 5 X [MIN 3 [MIN 9 5 7 5 6 X 1 1 4 X 2 2 [MIN 8 0 8 [MIN 8 0 1 1 5 2 X 9 [SM 5 5 4 0 0 7 9 4 5 X 5 9 X 3 [MIN [MIN 5 5 8 X 4 6 [MED 2 7 5 6 7 1 2 X 2 8 7 [MAX 0 1 0 3 3 3 7 7 7 X [SM 7 0 0 1 8 4 0 4 6 3 X X X X 6 [SM 7 [MED 0 7 4 4 [MAX 4 7 [MED 4 3 1 3 9 1 X 5 7 X [MIN 5 [SM 6 3 2 7 8 1 X 5 4 9 8 0 [MIN 6 2 6 6 6 6 3 X X 0 5 3 X 0 3 X 0 0 5 0 X 4 [MED 6 [MED [MAX 7 7 4 [MIN 2 9 X 9 [SM 5 5 5 7 X 8 0 1 X 1 2 X 3 3 5 5 [SM 5 4 [MAX [SM [MED 4 9 5 4 2 3 X [MIN 7 2 7 0 6 4 2 9 1 0 X [MED 4 2 6 2 8 0 5 8 1 X X 6 [MED 7 [MED 5 6 1 X 6 [SM 8 4 0 5 9 1 8 1 8 7 X 6 X [MIN 0 4 X 0 8 1 6 0 0 X 7 [MAX 9 [MIN 1 3 1 [MIN 7 2 2 5 9 7 X [MAX 9 5 4 3 7 4 6 4 9 X 8 4 1 X 2 X 6 0 X X 9 X 6 X 3 0 4 4 4 [MED 5 3 [MAX [MAX [MED 0 3 8 9 [MAX [MED [MAX 5 3 9 2 4 X 7 [MED 6 8 3 4 8 1 X 0 X 6 0 3 X [SM [MED 9 8 9 6 X 0 3 2 X 0 X 4 [MIN 4 3 [MED [MAX 5 4 3 2 3 0 3 [SM 9 5 4 8 6 7 6 4 8 X [SM 3 7 7 3 0 X 1 X 2 8 2 3 4 6 9 [MIN 5 1 4 7 3 0 2 [MED 7 5 4 6 5 X 4 0 X X X [MIN [MED 4 3 2 1 X 8 3 X X 1 X 4 [MAX [MED [MAX [MIN 6 [MIN [MED 1 9 3 X 8 4 X 1 [MED 8 2 [MIN 0 2 0 9 6 5 6 3 X 1 4 4 X 9 X [SM [MAX [MED 7 2 3 6 2 1 5 1 7 7 X 7 6 6 X 0 2 X [MED [MAX 2 6 4 X 2 X 9 5 2 X [MIN 2 0 1 0 6 5 4 5 6 9 X 0 6 X 5 1 1 [MAX 4 7 0 5 [MAX 5 [MIN [MAX 9 0 [SM 1 5 X 5 9 X 5 9 8 5 0 X 4 2 X 6 8 6 6 [MED 9 2 5 [MAX [MED 2 1 5 6 3 [MAX 6 8 3 8 0 1 3 X 4 [MIN 4 0 0 6 3 X X [MAX 6 5 8 5 [MAX 0 6 8 0 0 7 3 X 7 [MIN 0 6 0 1 8 0 4 X [MIN 2 4 1 7 5 X [MED 6 5 5 2 8 2 7 6 X 9 X 5 X X X [MED 9 0 8 [SM 7 7 [MIN 0 [MAX [MED 8 3 2 0 3 2 7 3 4 X [MED 3 2 3 2 1 1 9 8 X 0 6 [MAX 6 6 X 0 9 X X 6 X [SM 5 9 X 2 X 1 2 X 2 [MAX 0 0 9 X [MIN 6 1 2 2 3 1 X X X 1 X\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_dataset))['inputs']\n",
    "\n",
    "for i in range(min(4, batch_size)):\n",
    "  print(encoder.decode(sample[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4ZHtbE300dO"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TEmbedding(nn.Module):\n",
    "  def __init__(self, num_embeddings, hidden_dim, seq_length=1024, padding_idx=0):\n",
    "    super(TEmbedding, self).__init__()\n",
    "    \n",
    "    self.num_embeddings = num_embeddings\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.seq_length = seq_length\n",
    "    self.padding_idx = padding_idx\n",
    "\n",
    "    self.embedding = nn.Embedding(num_embeddings, hidden_dim, padding_idx)\n",
    "    self.pos_embeds  = nn.Parameter(torch.zeros(1, self.seq_length, self.hidden_dim))\n",
    "\n",
    "    self.cls = nn.Parameter(torch.zeros(1, 1, self.hidden_dim)) #!!!!!!! INIT WITH ANOTHER VALUE IF REQUIRED\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, seq_len = input.shape\n",
    "    \n",
    "    embed = self.embedding(input)\n",
    "    embed = embed + self.pos_embeds\n",
    "    embed = torch.cat([ self.cls.expand(batch_size, 1, -1), embed ], axis=1)\n",
    "\n",
    "    return embed\n",
    "    \n",
    "class TAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(TAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   =qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "    \n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "\n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "    q = torch.mul(q, 1. / torch.sqrt(torch.tensor(self.qkv_dim)))\n",
    "\n",
    "    qk = torch.matmul(q, k.transpose(-1, -2))\n",
    "    qk = nn.Softmax(dim=-1)(qk)\n",
    "\n",
    "    def assertion_function(tsr):\n",
    "      tsr = torch.sum(tsr, axis=-1)\n",
    "      tsr = tsr - torch.ones_like(tsr)\n",
    "      return torch.max(torch.abs(tsr)) < 1e-5\n",
    "\n",
    "    assert assertion_function(qk)\n",
    "\n",
    "    qk = self.dropout(qk) #Like in TF implementation; could be done before Softmax by random -inf addition\n",
    "\n",
    "    out = torch.matmul(qk, v)\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "\n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class HWLinear(nn.Module):\n",
    "  def __init__(self, num_heads, input_dim, output_dim, use_bias):\n",
    "    super(HWLinear, self).__init__()\n",
    "    \n",
    "    self.use_bias = use_bias\n",
    "    if use_bias:\n",
    "      self.bias   = nn.Parameter(torch.zeros( (1, num_heads, 1, output_dim)))\n",
    "\n",
    "    self.weight = nn.Parameter(torch.empty( (num_heads, input_dim, output_dim)))\n",
    "\n",
    "    def he_init(m):\n",
    "      s =  np.sqrt( 2. / input_dim )\n",
    "      m.data.normal_(0, s)\n",
    "\n",
    "    he_init(self.weight)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.matmul(x, self.weight)\n",
    "    if self.use_bias:\n",
    "      x += self.bias\n",
    "    return x\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "  def __init__(self, lambda_, objects=None):\n",
    "      super(Lambda, self).__init__()\n",
    "      self.lambda_ = lambda_\n",
    "      self.objects = objects\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.objects is not None:\n",
    "      return self.lambda_(self.objects, x)\n",
    "    return self.lambda_(x)\n",
    "\n",
    "class LKAAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(LKAAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   = qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "\n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "    \n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    #self.lka = nn.Sequential(\n",
    "    #  nn.Linear(self.head_dim, self.head_dim), nn.GELU(),\n",
    "    #  nn.Linear(self.head_dim, self.head_dim), nn.Softplus(beta=2.5),\n",
    "    #)\n",
    "\n",
    "    #256, 4, 16, 1024\n",
    "    #256, 64, 1, 1024\n",
    "    class AMGOLU(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, gate_rank, dropout_rate, gate_nonlinearity, kernel_nonlinearity, use_bias=False):\n",
    "        super(AMGOLU, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads= num_heads\n",
    "        \n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "\n",
    "        self.gate_weight_a = HWLinear(num_heads, self.head_dim, gate_rank, use_bias)\n",
    "        self.gate_weight_b = HWLinear(num_heads, gate_rank, self.head_dim, use_bias)\n",
    "\n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        self.gate_nonlinearity   = gate_nonlinearity\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        forward_info = self.orth_weight(x)\n",
    "        forward_info = self.kernel_nonlinearity(forward_info)\n",
    "\n",
    "        gate_info = self.gate_weight_a(x)\n",
    "        gate_info = self.gate_weight_b(gate_info)\n",
    "        gate_info = self.gate_nonlinearity(gate_info)\n",
    "\n",
    "        x = forward_info * gate_info\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "    class GatedOrthoKernel(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate=0.1, gate_nonlinearity=nn.Sigmoid(), kernel_nonlinearity=nn.Identity(), use_bias=False):\n",
    "        super(GatedOrthoKernel, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "        self.gate_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "\n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        self.gate_nonlinearity   = gate_nonlinearity\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.kernel_nonlinearity(self.orth_weight(x)) * self.gate_nonlinearity(self.gate_weight(x))\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "\n",
    "    class HeadWiseFF(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate, nonlinearity=nn.Identity(), use_bias=False, residual=False):\n",
    "        super(HeadWiseFF, self).__init__()\n",
    "        \n",
    "        head_dim = qkv_dim // num_heads\n",
    "\n",
    "        self.bias   = nn.Parameter(torch.empty( (1, num_heads, 1, head_dim)))\n",
    "        self.dropout= nn.Dropout(dropout_rate)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty( (num_heads, head_dim, head_dim)))\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "        #Orthogonal initialization\n",
    "        #Workaround with torch.stack, since Torch initializes a tensor as orthgonal by flattening its trailing dims and QR-factorizing the resulting 2d\n",
    "        \n",
    "        #self.weight = torch.stack([ nn.init.orthogonal_(torch.empty((head_dim, head_dim))) for _ in range(num_heads) ], dim=0)\n",
    "        #self.weight = nn.Parameter(self.weight)\n",
    "\n",
    "        bound = 1 / math.sqrt(head_dim)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.residual= residual\n",
    "\n",
    "      def forward(self, x):\n",
    "        \n",
    "        x, losses = x\n",
    "\n",
    "        bs, hd, seq, hdim = x.shape\n",
    "        y = self.dropout(x)\n",
    "        y = torch.matmul(y, self.weight) #BS, HD, SEQ, HDIM\n",
    "        if self.use_bias:\n",
    "          y += self.bias\n",
    "        y = self.nonlinearity(y)\n",
    "\n",
    "        #loss = torch.eye(hdim, device=self.weight.device).unsqueeze(0).expand(* self.weight.shape)\n",
    "        #loss = nn.MSELoss()(torch.matmul(self.weight, self.weight.transpose(-1, -2)), loss)\n",
    "        #loss *= LAMBDA\n",
    "\n",
    "        #losses.append(loss)\n",
    "\n",
    "        if self.residual:\n",
    "          return x + y, losses\n",
    "        return y, losses\n",
    "\n",
    "    self.lka = nn.Sequential(\n",
    "        \n",
    "        #AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        #AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        #AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Softplus(), False),\n",
    "        \n",
    "        #HeadWiseFF(self.num_heads, self.qkv_dim, dropout_rate, nn.Softplus(), use_bias=False),\n",
    "        \n",
    "        GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Softplus(), False)\n",
    "\n",
    "        #Lambda(lambda o, x: (o['act'](x[0]), x[1]), { 'act' : nn.Identity() })\n",
    "        \n",
    "    )\n",
    "\n",
    "    self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "    #BS x HEADS x SEQ x HEAD_DIM\n",
    "    \n",
    "    q, _ = self.lka((q, losses))\n",
    "    k, _ = self.lka((k, losses)) #Use this for var kernel\n",
    "\n",
    "    q = q / math.sqrt(self.head_dim)\n",
    "    k = k / math.sqrt(self.head_dim)\n",
    "\n",
    "    numerator = torch.matmul(k.unsqueeze(-1), v.unsqueeze(-2))\n",
    "    numerator = numerator.sum(axis=2)\n",
    "    numerator = torch.matmul(q, numerator)\n",
    "    \n",
    "    denominator = k.sum(axis=2).unsqueeze(-1)\n",
    "    denominator = q.matmul(denominator)\n",
    "\n",
    "    out = numerator / denominator\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "    \n",
    "    #TODO: INSERT DROPOUT\n",
    "    \n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(SimpleAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   =qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "\n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "    \n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "    #self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v) #BS x HEADS x SEQ x HEAD_DIM\n",
    "\n",
    "    _, _, seq_len, _ = q.shape\n",
    "\n",
    "    kv = torch.matmul(k.transpose(-1, -2), v)\n",
    "    kv *= 1 / math.sqrt(seq_len)\n",
    "    kv = self.dropout(kv)\n",
    "\n",
    "    out = torch.matmul(q, kv)\n",
    "    #out *= 1 / math.sqrt(self.head_dim)\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "    \n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    #out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class FtAttention(nn.Module):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(FtAttention, self).__init__()\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    return torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n",
    "\n",
    "class TBlock(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, mlp_dim, num_heads, dropout_rate):\n",
    "    super(TBlock, self).__init__()\n",
    "\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.qkv_dim  = qkv_dim\n",
    "    self.mlp_dim  = mlp_dim\n",
    "\n",
    "    self.layernorm_input = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "    self.layernorm_inter = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "\n",
    "    self.attention = TAttention(hidden_dim, qkv_dim, num_heads, dropout_rate)\n",
    "\n",
    "    self.ffn       = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, mlp_dim), nn.GELU(), nn.Dropout(dropout_rate),\n",
    "        nn.Linear(mlp_dim, hidden_dim), nn.Dropout(dropout_rate),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, input, losses=[]):\n",
    "    x = self.layernorm_input(input)\n",
    "    x = self.attention(x, losses)\n",
    "\n",
    "    x = input + x\n",
    "\n",
    "    y = self.layernorm_inter(x)\n",
    "    x = self.ffn(y) + x\n",
    "\n",
    "    return x\n",
    "\n",
    "class TClassifier(nn.Module):\n",
    "  def __init__(self, classes, hidden_dim, inter_dim, dropout_rate):\n",
    "    super(TClassifier, self).__init__()\n",
    "\n",
    "    self.layernorm = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "    self.dropout   = nn.Dropout(dropout_rate)\n",
    "\n",
    "    self.ffn       = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, inter_dim), nn.GELU(),\n",
    "    )\n",
    "    self.output    = nn.Linear(inter_dim, classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layernorm(x)\n",
    "    x = x[:, 0, :]\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    x = self.ffn(x)\n",
    "    logits = self.output(x)\n",
    "\n",
    "    return logits\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, classes, num_embeddings, seq_len, hidden_dim, qkv_dim, mlp_dim, num_heads, num_blocks, output_mlp_units, internal_dropout_rate=0.1, output_dropout_rate=0.0):\n",
    "    super(Transformer, self).__init__()\n",
    "    \n",
    "    self.embed_layer = TEmbedding(num_embeddings, hidden_dim, seq_len)\n",
    "    self.blocks      = nn.ModuleList([ TBlock(hidden_dim, qkv_dim, mlp_dim, num_heads, internal_dropout_rate) for _ in range(num_blocks) ])\n",
    "    self.classifier  = TClassifier(classes, hidden_dim, output_mlp_units, output_dropout_rate)\n",
    "\n",
    "  def forward(self, pixel_values):\n",
    "    additional_losses = []\n",
    "\n",
    "    x = self.embed_layer(pixel_values)\n",
    "    \n",
    "    for block in self.blocks:\n",
    "      x = block(x, additional_losses)\n",
    "    \n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return x, additional_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKl9SMDoCJeK"
   },
   "outputs": [],
   "source": [
    "def num_parameters(model):\n",
    "  return sum(list(map(\n",
    "      lambda x: np.prod(x[1].shape), model.named_parameters()\n",
    "  )))\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "def model_factory():\n",
    "  model = Transformer(\n",
    "    classes   =n_classes,\n",
    "    num_embeddings=encoder.vocab_size,\n",
    "    seq_len=max_length,\n",
    "    hidden_dim=512,\n",
    "    qkv_dim=512,\n",
    "    num_heads =8,\n",
    "    num_blocks=6,\n",
    "    mlp_dim=2048,\n",
    "    output_mlp_units=2048,\n",
    "    internal_dropout_rate=0.1,\n",
    "    output_dropout_rate=0.0\n",
    "  ).cuda()\n",
    "  \n",
    "  orig_count = num_parameters(model)\n",
    "\n",
    "  for block in model.blocks:\n",
    "    #block.attention = FtAttention()\n",
    "    #block.attention = LKAAttention(512, 512, 8, 0.1).cuda()\n",
    "    block.attention = SimpleAttention(512, 512, 8, 0.1).cuda()\n",
    "    ...\n",
    "  \n",
    "  new_count = num_parameters(model)\n",
    "  print(f'Original model {orig_count} params, new model {new_count} params, ratio {new_count / orig_count:.3}')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "l0P77jbqTEqK",
    "outputId": "fe19ed7c-39f1-4d5f-e5da-81e735d4b0f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model 21019658 params, new model 19443722 params, ratio 0.925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7facac15f1d0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def get_schedule(warmup_steps):\n",
    "  def lr_schedule(step):\n",
    "    return 1.0 * np.minimum(1.0, step / warmup_steps) / np.sqrt(np.maximum(step, warmup_steps))\n",
    "\n",
    "  return lr_schedule\n",
    "\n",
    "lr=0.005\n",
    "weight_decay=0.1\n",
    "warmup=1000\n",
    "\n",
    "\n",
    "def const_schedule(lr):\n",
    "  def lr_schedule(step):\n",
    "    return lr\n",
    "  return lr_schedule\n",
    "\n",
    "def training_setup():\n",
    "  model = model_factory()\n",
    "  criterion = nn.CrossEntropyLoss().cuda()\n",
    "  optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  schedule_func = get_schedule(warmup)\n",
    "  #schedule_func = const_schedule(1.0) #<--------- TEMPORARY\n",
    "  scheduler = LambdaLR(optimizer, schedule_func)\n",
    "\n",
    "  return model, criterion, optimizer, schedule_func, scheduler\n",
    "\n",
    "_, _, _, schedule_func, _ = training_setup()\n",
    "\n",
    "plt.plot([ lr * schedule_func(i) for i in range(15000) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_B6ohTx7wgH"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def save_model(model, optimizer, name='/content/drive/MyDrive/Work/Misc/lka-mini-base.tar'):\n",
    "  torch.save({\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              }, name)\n",
    "\n",
    "def progress_bar(len, total, current):\n",
    "  current_scaled = int(round(len * current / total))\n",
    "\n",
    "  s = '[' + '=' * (current_scaled - 1)\n",
    "  s += '>' if current != total else '='\n",
    "  s += '-' * (len - current_scaled) + ']'\n",
    "\n",
    "  return s\n",
    "\n",
    "def accuracy(model_output, labels):\n",
    "  model_output = model_output.argmax(dim=-1)\n",
    "\n",
    "  return (labels == model_output).float().mean().cpu().numpy()\n",
    "\n",
    "def train_model(model, name, train_dataset, valid_dataset, optimizer, criterion, scheduler, accumulation_steps, epochs, epoch_len=64, eps = 1e-5, skip_eval=0):\n",
    "  \n",
    "  best_acc = 0.0\n",
    "  train_datagen = iter(train_dataset)\n",
    "      \n",
    "  for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "      \n",
    "      #epoch start timestamp\n",
    "      t = time.time()\n",
    "\n",
    "      running_loss = 0.0\n",
    "      running_reg  = 0.0\n",
    "      running_acc  = 0.0\n",
    "\n",
    "      running_momentum = 0.99\n",
    "\n",
    "      epoch_loss = [  ]\n",
    "      epoch_reg  = [  ]\n",
    "      epoch_acc  = [  ]\n",
    "\n",
    "      model.train()\n",
    "\n",
    "      print(f'Epoch {epoch}')\n",
    "\n",
    "      process_inputs = lambda x: torch.Tensor(x.numpy()).to(torch.int64)\n",
    "\n",
    "      for i in range(epoch_len):\n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          #accumulate gradients for a certain amount of steps\n",
    "          for k in range(accumulation_steps):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            try:\n",
    "              data = next(train_datagen)\n",
    "            except StopIteration:\n",
    "              train_datagen = iter(train_dataset)\n",
    "              data = next(train_datagen)\n",
    "            except:\n",
    "              break\n",
    "            inputs, labels = data['inputs'], data['targets']\n",
    "            inputs, labels = process_inputs(inputs), process_inputs(labels)\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs, additional_losses = model(inputs)\n",
    "            loss = criterion(outputs + eps, labels)\n",
    "\n",
    "            if torch.any(torch.isnan(loss)):\n",
    "              print(loss)\n",
    "              return None\n",
    "\n",
    "            additional_losses = sum(additional_losses) if additional_losses else torch.Tensor([ 0.0 ]).cuda()\n",
    "            ((loss + additional_losses) / accumulation_steps).backward()\n",
    "\n",
    "            acc = accuracy(outputs, labels)\n",
    "\n",
    "            running_loss = running_loss * running_momentum + (1 - running_momentum) * loss.item()\n",
    "            running_loss_unb = running_loss / (1 - running_momentum ** (i * accumulation_steps + k + 1))\n",
    "\n",
    "            running_acc  = running_acc  * running_momentum + (1 - running_momentum) * acc\n",
    "            running_acc_unb = running_acc / (1 - running_momentum ** (i * accumulation_steps + k + 1))\n",
    "\n",
    "            running_reg  = running_reg  * running_momentum + (1 - running_momentum) * additional_losses.item()\n",
    "            running_reg_unb = running_reg / (1 - running_momentum ** (i * accumulation_steps + k + 1))\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc)\n",
    "            epoch_reg.append(additional_losses.item())\n",
    "\n",
    "          optimizer.step()\n",
    "\n",
    "          pbar = progress_bar(20, epoch_len, i + 1)\n",
    "\n",
    "          print(f'\\r{pbar} {i + 1}/{epoch_len}:', end='')\n",
    "          print(f' - running_loss: {running_loss_unb:.4f} - running_reg: {running_reg_unb:.6f} - running_acc: {running_acc_unb:.4f} - lr: {scheduler.get_last_lr()[0]:.5f}', end='')\n",
    "\n",
    "          scheduler.step()\n",
    "      \n",
    "      epoch_loss = np.mean(epoch_loss)\n",
    "      epoch_acc  = np.mean(epoch_acc)\n",
    "      epoch_reg  = np.mean(epoch_reg)\n",
    "      \n",
    "      print(f' - epoch_loss: {epoch_loss:.4f} - epoch_reg: {epoch_reg:.6f} - epoch_acc: {epoch_acc:.4f}', end='')\n",
    "\n",
    "      epoch_loss, epoch_acc, epoch_reg = [], [], []\n",
    "\n",
    "      \n",
    "      if epoch >= skip_eval:\n",
    "        model.eval()\n",
    "        valid_dataset.repeat()\n",
    "        valid_datagen = iter(valid_dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "          for i, data in enumerate(valid_datagen):\n",
    "\n",
    "            inputs, labels = data['inputs'], data['targets']\n",
    "            inputs, labels = process_inputs(inputs), process_inputs(labels)\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            outputs, aux_losses = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = accuracy(outputs, labels)\n",
    "            aux_losses = sum(aux_losses) if aux_losses else torch.Tensor([ 0.0 ]).cuda()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc)\n",
    "            epoch_reg.append(aux_losses.item())\n",
    "\n",
    "        epoch_loss, epoch_acc, epoch_reg = np.mean(epoch_loss), np.mean(epoch_acc), np.mean(epoch_reg)\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "          best_acc = epoch_acc\n",
    "          save_model(model, optimizer, name)\n",
    "      \n",
    "      else:\n",
    "        epoch_loss, epoch_acc, epoch_reg = 0.0, 0.0, 0.0\n",
    "\n",
    "      #epoch computing time\n",
    "      t = time.time() - t\n",
    "\n",
    "      print(f' - valid_loss: {epoch_loss:.4f} - valid_reg: {epoch_reg:.6f} - valid_acc: {epoch_acc:.4f} - epoch_time: {t:.4f} s')\n",
    " \n",
    "  checkpoint = torch.load(name)\n",
    "  return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egh-IbW76AN4"
   },
   "outputs": [],
   "source": [
    "def test(model, criterion, test_dataset):\n",
    "  epoch_loss, epoch_acc, epoch_reg = [], [], []\n",
    "\n",
    "  model.eval()\n",
    "  test_dataset.repeat()\n",
    "\n",
    "  process_inputs = lambda x: torch.Tensor(x.numpy()).to(torch.int64)\n",
    "\n",
    "  t = time.time()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, data in enumerate(iter(test_dataset)):\n",
    "      inputs, labels = data['inputs'], data['targets']\n",
    "      inputs, labels = process_inputs(inputs), process_inputs(labels)\n",
    "      inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "      outputs, aux_losses = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      acc = accuracy(outputs, labels)\n",
    "      aux_losses = sum(aux_losses) if aux_losses else torch.Tensor([ 0.0 ]).cuda()\n",
    "\n",
    "      epoch_loss.append(loss.item())\n",
    "      epoch_acc.append(acc)\n",
    "      epoch_reg.append(aux_losses.item())\n",
    "\n",
    "  t = time.time() - t\n",
    "\n",
    "  epoch_loss, epoch_acc, epoch_reg = np.mean(epoch_loss), np.mean(epoch_acc), np.mean(epoch_reg)\n",
    "\n",
    "  print(f' - test_loss: {epoch_loss:.4f} - test_reg: {epoch_reg:.6f} - test_acc: {epoch_acc:.4f} - test_time: {t:.4f} s')\n",
    "  return epoch_loss, epoch_reg, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4d0DhgWDbm3",
    "outputId": "5893354d-4713-44ae-dff3-2b404a2d5ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model 21019658 params, new model 19443722 params, ratio 0.925\n",
      "Epoch 0\n",
      "[====================] 50/50: - running_loss: 2.2741 - running_reg: 0.000000 - running_acc: 0.1695 - lr: 0.00001 - epoch_loss: 2.2920 - epoch_reg: 0.000000 - epoch_acc: 0.1450 - valid_loss: 2.2703 - valid_reg: 0.000000 - valid_acc: 0.1670 - epoch_time: 56.5754 s\n",
      "Epoch 1\n",
      "[====================] 50/50: - running_loss: 2.2692 - running_reg: 0.000000 - running_acc: 0.1521 - lr: 0.00002 - epoch_loss: 2.2665 - epoch_reg: 0.000000 - epoch_acc: 0.1581 - valid_loss: 2.2783 - valid_reg: 0.000000 - valid_acc: 0.1635 - epoch_time: 54.1498 s\n",
      "Epoch 2\n",
      "[====================] 50/50: - running_loss: 2.2639 - running_reg: 0.000000 - running_acc: 0.1640 - lr: 0.00002 - epoch_loss: 2.2512 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2778 - valid_reg: 0.000000 - valid_acc: 0.1710 - epoch_time: 54.8123 s\n",
      "Epoch 3\n",
      "[====================] 50/50: - running_loss: 2.2717 - running_reg: 0.000000 - running_acc: 0.1649 - lr: 0.00003 - epoch_loss: 2.2734 - epoch_reg: 0.000000 - epoch_acc: 0.1619 - valid_loss: 2.2686 - valid_reg: 0.000000 - valid_acc: 0.1575 - epoch_time: 54.1792 s\n",
      "Epoch 4\n",
      "[====================] 50/50: - running_loss: 2.3031 - running_reg: 0.000000 - running_acc: 0.1379 - lr: 0.00004 - epoch_loss: 2.2946 - epoch_reg: 0.000000 - epoch_acc: 0.1394 - valid_loss: 2.2593 - valid_reg: 0.000000 - valid_acc: 0.1720 - epoch_time: 54.7698 s\n",
      "Epoch 5\n",
      "[====================] 50/50: - running_loss: 2.2475 - running_reg: 0.000000 - running_acc: 0.1787 - lr: 0.00005 - epoch_loss: 2.2619 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 2.2683 - valid_reg: 0.000000 - valid_acc: 0.1780 - epoch_time: 54.8171 s\n",
      "Epoch 6\n",
      "[====================] 50/50: - running_loss: 2.2803 - running_reg: 0.000000 - running_acc: 0.1783 - lr: 0.00006 - epoch_loss: 2.2623 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2745 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.2016 s\n",
      "Epoch 7\n",
      "[====================] 50/50: - running_loss: 2.2675 - running_reg: 0.000000 - running_acc: 0.1682 - lr: 0.00006 - epoch_loss: 2.2727 - epoch_reg: 0.000000 - epoch_acc: 0.1575 - valid_loss: 2.2702 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1389 s\n",
      "Epoch 8\n",
      "[====================] 50/50: - running_loss: 2.2727 - running_reg: 0.000000 - running_acc: 0.1712 - lr: 0.00007 - epoch_loss: 2.2724 - epoch_reg: 0.000000 - epoch_acc: 0.1581 - valid_loss: 2.2744 - valid_reg: 0.000000 - valid_acc: 0.1610 - epoch_time: 54.0315 s\n",
      "Epoch 9\n",
      "[====================] 50/50: - running_loss: 2.2594 - running_reg: 0.000000 - running_acc: 0.1645 - lr: 0.00008 - epoch_loss: 2.2543 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2590 - valid_reg: 0.000000 - valid_acc: 0.1700 - epoch_time: 54.0944 s\n",
      "Epoch 10\n",
      "[====================] 50/50: - running_loss: 2.3141 - running_reg: 0.000000 - running_acc: 0.1520 - lr: 0.00009 - epoch_loss: 2.2892 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 2.2695 - valid_reg: 0.000000 - valid_acc: 0.1565 - epoch_time: 54.4583 s\n",
      "Epoch 11\n",
      "[====================] 50/50: - running_loss: 2.2608 - running_reg: 0.000000 - running_acc: 0.1651 - lr: 0.00009 - epoch_loss: 2.2584 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.4647 s\n",
      "Epoch 12\n",
      "[====================] 50/50: - running_loss: 2.2648 - running_reg: 0.000000 - running_acc: 0.1843 - lr: 0.00010 - epoch_loss: 2.2603 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2633 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.2269 s\n",
      "Epoch 13\n",
      "[====================] 50/50: - running_loss: 2.2718 - running_reg: 0.000000 - running_acc: 0.1644 - lr: 0.00011 - epoch_loss: 2.2742 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 2.2622 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.2186 s\n",
      "Epoch 14\n",
      "[====================] 50/50: - running_loss: 2.2707 - running_reg: 0.000000 - running_acc: 0.1532 - lr: 0.00012 - epoch_loss: 2.2694 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 2.2722 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.1808 s\n",
      "Epoch 15\n",
      "[====================] 50/50: - running_loss: 2.2689 - running_reg: 0.000000 - running_acc: 0.1520 - lr: 0.00013 - epoch_loss: 2.2676 - epoch_reg: 0.000000 - epoch_acc: 0.1569 - valid_loss: 2.2583 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 53.9775 s\n",
      "Epoch 16\n",
      "[====================] 50/50: - running_loss: 2.2619 - running_reg: 0.000000 - running_acc: 0.1750 - lr: 0.00013 - epoch_loss: 2.2486 - epoch_reg: 0.000000 - epoch_acc: 0.1900 - valid_loss: 2.2691 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.2216 s\n",
      "Epoch 17\n",
      "[====================] 50/50: - running_loss: 2.2728 - running_reg: 0.000000 - running_acc: 0.1707 - lr: 0.00014 - epoch_loss: 2.2638 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2638 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1604 s\n",
      "Epoch 18\n",
      "[====================] 50/50: - running_loss: 2.2579 - running_reg: 0.000000 - running_acc: 0.1561 - lr: 0.00015 - epoch_loss: 2.2650 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 2.2571 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.3958 s\n",
      "Epoch 19\n",
      "[====================] 50/50: - running_loss: 2.2526 - running_reg: 0.000000 - running_acc: 0.1698 - lr: 0.00016 - epoch_loss: 2.2643 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2709 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.2448 s\n",
      "Epoch 20\n",
      "[====================] 50/50: - running_loss: 2.2528 - running_reg: 0.000000 - running_acc: 0.1739 - lr: 0.00015 - epoch_loss: 2.2659 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2630 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.2660 s\n",
      "Epoch 21\n",
      "[====================] 50/50: - running_loss: 2.2603 - running_reg: 0.000000 - running_acc: 0.1669 - lr: 0.00015 - epoch_loss: 2.2585 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 2.2614 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.1443 s\n",
      "Epoch 22\n",
      "[====================] 50/50: - running_loss: 2.2490 - running_reg: 0.000000 - running_acc: 0.1771 - lr: 0.00015 - epoch_loss: 2.2652 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 2.2605 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.0801 s\n",
      "Epoch 23\n",
      "[====================] 50/50: - running_loss: 2.2718 - running_reg: 0.000000 - running_acc: 0.1463 - lr: 0.00014 - epoch_loss: 2.2561 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 2.2576 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.1529 s\n",
      "Epoch 24\n",
      "[====================] 50/50: - running_loss: 2.2109 - running_reg: 0.000000 - running_acc: 0.2259 - lr: 0.00014 - epoch_loss: 2.2257 - epoch_reg: 0.000000 - epoch_acc: 0.2019 - valid_loss: 2.2885 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.2752 s\n",
      "Epoch 25\n",
      "[====================] 50/50: - running_loss: 2.2487 - running_reg: 0.000000 - running_acc: 0.1774 - lr: 0.00014 - epoch_loss: 2.2580 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2605 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.2109 s\n",
      "Epoch 26\n",
      "[====================] 50/50: - running_loss: 2.2866 - running_reg: 0.000000 - running_acc: 0.1487 - lr: 0.00014 - epoch_loss: 2.2796 - epoch_reg: 0.000000 - epoch_acc: 0.1544 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.0680 s\n",
      "Epoch 27\n",
      "[====================] 50/50: - running_loss: 2.2531 - running_reg: 0.000000 - running_acc: 0.1643 - lr: 0.00013 - epoch_loss: 2.2582 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 2.2603 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1559 s\n",
      "Epoch 28\n",
      "[====================] 50/50: - running_loss: 2.2515 - running_reg: 0.000000 - running_acc: 0.1716 - lr: 0.00013 - epoch_loss: 2.2527 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 2.2699 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.2077 s\n",
      "Epoch 29\n",
      "[====================] 50/50: - running_loss: 2.2599 - running_reg: 0.000000 - running_acc: 0.1695 - lr: 0.00013 - epoch_loss: 2.2450 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 2.2674 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1243 s\n",
      "Epoch 30\n",
      "[====================] 50/50: - running_loss: 2.2699 - running_reg: 0.000000 - running_acc: 0.1480 - lr: 0.00013 - epoch_loss: 2.2611 - epoch_reg: 0.000000 - epoch_acc: 0.1556 - valid_loss: 2.2618 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.0139 s\n",
      "Epoch 31\n",
      "[====================] 50/50: - running_loss: 2.2636 - running_reg: 0.000000 - running_acc: 0.1562 - lr: 0.00013 - epoch_loss: 2.2589 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 2.2635 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.2137 s\n",
      "Epoch 32\n",
      "[====================] 50/50: - running_loss: 2.2547 - running_reg: 0.000000 - running_acc: 0.1685 - lr: 0.00012 - epoch_loss: 2.2507 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 2.2564 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.0023 s\n",
      "Epoch 33\n",
      "[====================] 50/50: - running_loss: 2.2655 - running_reg: 0.000000 - running_acc: 0.1606 - lr: 0.00012 - epoch_loss: 2.2492 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2621 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.1328 s\n",
      "Epoch 34\n",
      "[====================] 50/50: - running_loss: 2.2639 - running_reg: 0.000000 - running_acc: 0.1658 - lr: 0.00012 - epoch_loss: 2.2651 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 2.2608 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.0887 s\n",
      "Epoch 35\n",
      "[====================] 50/50: - running_loss: 2.2518 - running_reg: 0.000000 - running_acc: 0.1799 - lr: 0.00012 - epoch_loss: 2.2542 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2657 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1229 s\n",
      "Epoch 36\n",
      "[====================] 50/50: - running_loss: 2.2563 - running_reg: 0.000000 - running_acc: 0.1810 - lr: 0.00012 - epoch_loss: 2.2532 - epoch_reg: 0.000000 - epoch_acc: 0.1856 - valid_loss: 2.2562 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.0649 s\n",
      "Epoch 37\n",
      "[====================] 50/50: - running_loss: 2.2530 - running_reg: 0.000000 - running_acc: 0.1536 - lr: 0.00011 - epoch_loss: 2.2489 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2545 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1227 s\n",
      "Epoch 38\n",
      "[====================] 50/50: - running_loss: 2.2496 - running_reg: 0.000000 - running_acc: 0.1619 - lr: 0.00011 - epoch_loss: 2.2480 - epoch_reg: 0.000000 - epoch_acc: 0.1587 - valid_loss: 2.2574 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1577 s\n",
      "Epoch 39\n",
      "[====================] 50/50: - running_loss: 2.2403 - running_reg: 0.000000 - running_acc: 0.1878 - lr: 0.00011 - epoch_loss: 2.2316 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 2.2574 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 53.9451 s\n",
      "Epoch 40\n",
      "[====================] 50/50: - running_loss: 2.2645 - running_reg: 0.000000 - running_acc: 0.1685 - lr: 0.00011 - epoch_loss: 2.2589 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 2.2593 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.1984 s\n",
      "Epoch 41\n",
      "[====================] 50/50: - running_loss: 2.2580 - running_reg: 0.000000 - running_acc: 0.1732 - lr: 0.00011 - epoch_loss: 2.2544 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2611 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.2029 s\n",
      "Epoch 42\n",
      "[====================] 50/50: - running_loss: 2.2708 - running_reg: 0.000000 - running_acc: 0.1483 - lr: 0.00011 - epoch_loss: 2.2665 - epoch_reg: 0.000000 - epoch_acc: 0.1519 - valid_loss: 2.2613 - valid_reg: 0.000000 - valid_acc: 0.1690 - epoch_time: 54.1331 s\n",
      "Epoch 43\n",
      "[====================] 50/50: - running_loss: 2.2482 - running_reg: 0.000000 - running_acc: 0.1765 - lr: 0.00011 - epoch_loss: 2.2533 - epoch_reg: 0.000000 - epoch_acc: 0.1737 - valid_loss: 2.2606 - valid_reg: 0.000000 - valid_acc: 0.1680 - epoch_time: 54.2310 s\n",
      "Epoch 44\n",
      "[====================] 50/50: - running_loss: 2.2427 - running_reg: 0.000000 - running_acc: 0.1838 - lr: 0.00011 - epoch_loss: 2.2383 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2580 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.2562 s\n",
      "Epoch 45\n",
      "[====================] 50/50: - running_loss: 2.2600 - running_reg: 0.000000 - running_acc: 0.1617 - lr: 0.00010 - epoch_loss: 2.2569 - epoch_reg: 0.000000 - epoch_acc: 0.1688 - valid_loss: 2.2580 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.1012 s\n",
      "Epoch 46\n",
      "[====================] 50/50: - running_loss: 2.2435 - running_reg: 0.000000 - running_acc: 0.1738 - lr: 0.00010 - epoch_loss: 2.2425 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 2.2600 - valid_reg: 0.000000 - valid_acc: 0.1630 - epoch_time: 54.1051 s\n",
      "Epoch 47\n",
      "[====================] 50/50: - running_loss: 2.2765 - running_reg: 0.000000 - running_acc: 0.1492 - lr: 0.00010 - epoch_loss: 2.2659 - epoch_reg: 0.000000 - epoch_acc: 0.1587 - valid_loss: 2.2580 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1973 s\n",
      "Epoch 48\n",
      "[====================] 50/50: - running_loss: 2.2594 - running_reg: 0.000000 - running_acc: 0.1815 - lr: 0.00010 - epoch_loss: 2.2441 - epoch_reg: 0.000000 - epoch_acc: 0.1894 - valid_loss: 2.2613 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 53.9744 s\n",
      "Epoch 49\n",
      "[====================] 50/50: - running_loss: 2.2383 - running_reg: 0.000000 - running_acc: 0.1926 - lr: 0.00010 - epoch_loss: 2.2476 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 2.2646 - valid_reg: 0.000000 - valid_acc: 0.1600 - epoch_time: 54.0718 s\n",
      "Epoch 50\n",
      "[====================] 50/50: - running_loss: 2.2559 - running_reg: 0.000000 - running_acc: 0.1791 - lr: 0.00010 - epoch_loss: 2.2474 - epoch_reg: 0.000000 - epoch_acc: 0.1775 - valid_loss: 2.2644 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.4101 s\n",
      "Epoch 51\n",
      "[====================] 50/50: - running_loss: 2.2506 - running_reg: 0.000000 - running_acc: 0.1805 - lr: 0.00010 - epoch_loss: 2.2438 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.5512 s\n",
      "Epoch 52\n",
      "[====================] 50/50: - running_loss: 2.2605 - running_reg: 0.000000 - running_acc: 0.1698 - lr: 0.00010 - epoch_loss: 2.2582 - epoch_reg: 0.000000 - epoch_acc: 0.1581 - valid_loss: 2.2612 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.2841 s\n",
      "Epoch 53\n",
      "[====================] 50/50: - running_loss: 2.2633 - running_reg: 0.000000 - running_acc: 0.1653 - lr: 0.00010 - epoch_loss: 2.2641 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 2.2559 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1630 s\n",
      "Epoch 54\n",
      "[====================] 50/50: - running_loss: 2.2636 - running_reg: 0.000000 - running_acc: 0.1678 - lr: 0.00010 - epoch_loss: 2.2544 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 2.2591 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1158 s\n",
      "Epoch 55\n",
      "[====================] 50/50: - running_loss: 2.2826 - running_reg: 0.000000 - running_acc: 0.1403 - lr: 0.00009 - epoch_loss: 2.2733 - epoch_reg: 0.000000 - epoch_acc: 0.1431 - valid_loss: 2.2584 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.1676 s\n",
      "Epoch 56\n",
      "[====================] 50/50: - running_loss: 2.2760 - running_reg: 0.000000 - running_acc: 0.1585 - lr: 0.00009 - epoch_loss: 2.2683 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 2.2615 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.0414 s\n",
      "Epoch 57\n",
      "[====================] 50/50: - running_loss: 2.2743 - running_reg: 0.000000 - running_acc: 0.1513 - lr: 0.00009 - epoch_loss: 2.2680 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2595 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.2138 s\n",
      "Epoch 58\n",
      "[====================] 50/50: - running_loss: 2.2398 - running_reg: 0.000000 - running_acc: 0.1660 - lr: 0.00009 - epoch_loss: 2.2433 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 2.2573 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.3090 s\n",
      "Epoch 59\n",
      "[====================] 50/50: - running_loss: 2.2378 - running_reg: 0.000000 - running_acc: 0.1877 - lr: 0.00009 - epoch_loss: 2.2388 - epoch_reg: 0.000000 - epoch_acc: 0.1850 - valid_loss: 2.2594 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 52.9384 s\n",
      "Epoch 60\n",
      "[====================] 50/50: - running_loss: 2.2476 - running_reg: 0.000000 - running_acc: 0.1583 - lr: 0.00009 - epoch_loss: 2.2344 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 2.2557 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 56.3775 s\n",
      "Epoch 61\n",
      "[====================] 50/50: - running_loss: 2.2500 - running_reg: 0.000000 - running_acc: 0.1793 - lr: 0.00009 - epoch_loss: 2.2546 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 2.2648 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.2218 s\n",
      "Epoch 62\n",
      "[====================] 50/50: - running_loss: 2.2702 - running_reg: 0.000000 - running_acc: 0.1625 - lr: 0.00009 - epoch_loss: 2.2571 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2623 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.3314 s\n",
      "Epoch 63\n",
      "[====================] 50/50: - running_loss: 2.2461 - running_reg: 0.000000 - running_acc: 0.1650 - lr: 0.00009 - epoch_loss: 2.2541 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2585 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.3662 s\n",
      "Epoch 64\n",
      "[====================] 50/50: - running_loss: 2.2621 - running_reg: 0.000000 - running_acc: 0.1616 - lr: 0.00009 - epoch_loss: 2.2707 - epoch_reg: 0.000000 - epoch_acc: 0.1575 - valid_loss: 2.2669 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.3099 s\n",
      "Epoch 65\n",
      "[====================] 50/50: - running_loss: 2.2386 - running_reg: 0.000000 - running_acc: 0.1817 - lr: 0.00009 - epoch_loss: 2.2474 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 2.2585 - valid_reg: 0.000000 - valid_acc: 0.1600 - epoch_time: 54.3087 s\n",
      "Epoch 66\n",
      "[====================] 50/50: - running_loss: 2.2572 - running_reg: 0.000000 - running_acc: 0.1437 - lr: 0.00009 - epoch_loss: 2.2595 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 2.2600 - valid_reg: 0.000000 - valid_acc: 0.1590 - epoch_time: 54.1468 s\n",
      "Epoch 67\n",
      "[====================] 50/50: - running_loss: 2.2490 - running_reg: 0.000000 - running_acc: 0.1669 - lr: 0.00009 - epoch_loss: 2.2445 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2605 - valid_reg: 0.000000 - valid_acc: 0.1780 - epoch_time: 54.1640 s\n",
      "Epoch 68\n",
      "[====================] 50/50: - running_loss: 2.2696 - running_reg: 0.000000 - running_acc: 0.1699 - lr: 0.00009 - epoch_loss: 2.2570 - epoch_reg: 0.000000 - epoch_acc: 0.1806 - valid_loss: 2.2625 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.1726 s\n",
      "Epoch 69\n",
      "[====================] 50/50: - running_loss: 2.2519 - running_reg: 0.000000 - running_acc: 0.1776 - lr: 0.00008 - epoch_loss: 2.2589 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2583 - valid_reg: 0.000000 - valid_acc: 0.1730 - epoch_time: 53.9484 s\n",
      "Epoch 70\n",
      "[====================] 50/50: - running_loss: 2.2585 - running_reg: 0.000000 - running_acc: 0.1456 - lr: 0.00008 - epoch_loss: 2.2628 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 2.2588 - valid_reg: 0.000000 - valid_acc: 0.1735 - epoch_time: 54.1921 s\n",
      "Epoch 71\n",
      "[====================] 50/50: - running_loss: 2.2448 - running_reg: 0.000000 - running_acc: 0.1742 - lr: 0.00008 - epoch_loss: 2.2523 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2554 - valid_reg: 0.000000 - valid_acc: 0.1740 - epoch_time: 54.0468 s\n",
      "Epoch 72\n",
      "[====================] 50/50: - running_loss: 2.2468 - running_reg: 0.000000 - running_acc: 0.1705 - lr: 0.00008 - epoch_loss: 2.2508 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2598 - valid_reg: 0.000000 - valid_acc: 0.1720 - epoch_time: 54.0586 s\n",
      "Epoch 73\n",
      "[====================] 50/50: - running_loss: 2.2645 - running_reg: 0.000000 - running_acc: 0.1568 - lr: 0.00008 - epoch_loss: 2.2659 - epoch_reg: 0.000000 - epoch_acc: 0.1569 - valid_loss: 2.2588 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 53.9345 s\n",
      "Epoch 74\n",
      "[====================] 50/50: - running_loss: 2.2466 - running_reg: 0.000000 - running_acc: 0.1662 - lr: 0.00008 - epoch_loss: 2.2416 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 2.2573 - valid_reg: 0.000000 - valid_acc: 0.1865 - epoch_time: 54.7030 s\n",
      "Epoch 75\n",
      "[====================] 50/50: - running_loss: 2.2627 - running_reg: 0.000000 - running_acc: 0.1725 - lr: 0.00008 - epoch_loss: 2.2643 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2558 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.0344 s\n",
      "Epoch 76\n",
      "[====================] 50/50: - running_loss: 2.2336 - running_reg: 0.000000 - running_acc: 0.1753 - lr: 0.00008 - epoch_loss: 2.2421 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 2.2562 - valid_reg: 0.000000 - valid_acc: 0.1800 - epoch_time: 54.1513 s\n",
      "Epoch 77\n",
      "[====================] 50/50: - running_loss: 2.2493 - running_reg: 0.000000 - running_acc: 0.1668 - lr: 0.00008 - epoch_loss: 2.2474 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2573 - valid_reg: 0.000000 - valid_acc: 0.1730 - epoch_time: 54.1030 s\n",
      "Epoch 78\n",
      "[====================] 50/50: - running_loss: 2.2404 - running_reg: 0.000000 - running_acc: 0.1936 - lr: 0.00008 - epoch_loss: 2.2516 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 2.2614 - valid_reg: 0.000000 - valid_acc: 0.1760 - epoch_time: 53.9511 s\n",
      "Epoch 79\n",
      "[====================] 50/50: - running_loss: 2.2543 - running_reg: 0.000000 - running_acc: 0.1769 - lr: 0.00008 - epoch_loss: 2.2596 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 2.2564 - valid_reg: 0.000000 - valid_acc: 0.1810 - epoch_time: 53.9723 s\n",
      "Epoch 80\n",
      "[====================] 50/50: - running_loss: 2.2497 - running_reg: 0.000000 - running_acc: 0.1787 - lr: 0.00008 - epoch_loss: 2.2538 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2586 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.0989 s\n",
      "Epoch 81\n",
      "[====================] 50/50: - running_loss: 2.2321 - running_reg: 0.000000 - running_acc: 0.1970 - lr: 0.00008 - epoch_loss: 2.2522 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2623 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 53.9708 s\n",
      "Epoch 82\n",
      "[====================] 50/50: - running_loss: 2.2339 - running_reg: 0.000000 - running_acc: 0.1867 - lr: 0.00008 - epoch_loss: 2.2443 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 2.2593 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.2286 s\n",
      "Epoch 83\n",
      "[====================] 50/50: - running_loss: 2.2748 - running_reg: 0.000000 - running_acc: 0.1380 - lr: 0.00008 - epoch_loss: 2.2441 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2586 - valid_reg: 0.000000 - valid_acc: 0.1840 - epoch_time: 54.0813 s\n",
      "Epoch 84\n",
      "[====================] 50/50: - running_loss: 2.2531 - running_reg: 0.000000 - running_acc: 0.1748 - lr: 0.00008 - epoch_loss: 2.2409 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 2.2598 - valid_reg: 0.000000 - valid_acc: 0.1710 - epoch_time: 54.0561 s\n",
      "Epoch 85\n",
      "[====================] 50/50: - running_loss: 2.2527 - running_reg: 0.000000 - running_acc: 0.1698 - lr: 0.00008 - epoch_loss: 2.2486 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2568 - valid_reg: 0.000000 - valid_acc: 0.1600 - epoch_time: 54.0624 s\n",
      "Epoch 86\n",
      "[====================] 50/50: - running_loss: 2.2638 - running_reg: 0.000000 - running_acc: 0.1613 - lr: 0.00008 - epoch_loss: 2.2608 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2542 - valid_reg: 0.000000 - valid_acc: 0.1735 - epoch_time: 53.9712 s\n",
      "Epoch 87\n",
      "[====================] 50/50: - running_loss: 2.2537 - running_reg: 0.000000 - running_acc: 0.1624 - lr: 0.00008 - epoch_loss: 2.2541 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 2.2575 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.2334 s\n",
      "Epoch 88\n",
      "[====================] 50/50: - running_loss: 2.2554 - running_reg: 0.000000 - running_acc: 0.1659 - lr: 0.00007 - epoch_loss: 2.2596 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 2.2542 - valid_reg: 0.000000 - valid_acc: 0.1895 - epoch_time: 55.2189 s\n",
      "Epoch 89\n",
      "[====================] 50/50: - running_loss: 2.2209 - running_reg: 0.000000 - running_acc: 0.1966 - lr: 0.00007 - epoch_loss: 2.2349 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 2.2571 - valid_reg: 0.000000 - valid_acc: 0.1815 - epoch_time: 54.1151 s\n",
      "Epoch 90\n",
      "[====================] 50/50: - running_loss: 2.2506 - running_reg: 0.000000 - running_acc: 0.1687 - lr: 0.00007 - epoch_loss: 2.2440 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2543 - valid_reg: 0.000000 - valid_acc: 0.1735 - epoch_time: 54.1627 s\n",
      "Epoch 91\n",
      "[====================] 50/50: - running_loss: 2.2658 - running_reg: 0.000000 - running_acc: 0.1458 - lr: 0.00007 - epoch_loss: 2.2568 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 2.2552 - valid_reg: 0.000000 - valid_acc: 0.1790 - epoch_time: 54.2159 s\n",
      "Epoch 92\n",
      "[====================] 50/50: - running_loss: 2.2492 - running_reg: 0.000000 - running_acc: 0.1836 - lr: 0.00007 - epoch_loss: 2.2491 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 2.2549 - valid_reg: 0.000000 - valid_acc: 0.1910 - epoch_time: 54.7875 s\n",
      "Epoch 93\n",
      "[====================] 50/50: - running_loss: 2.2465 - running_reg: 0.000000 - running_acc: 0.1647 - lr: 0.00007 - epoch_loss: 2.2478 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2569 - valid_reg: 0.000000 - valid_acc: 0.1690 - epoch_time: 54.1656 s\n",
      "Epoch 94\n",
      "[====================] 50/50: - running_loss: 2.2565 - running_reg: 0.000000 - running_acc: 0.1933 - lr: 0.00007 - epoch_loss: 2.2428 - epoch_reg: 0.000000 - epoch_acc: 0.1944 - valid_loss: 2.2533 - valid_reg: 0.000000 - valid_acc: 0.1870 - epoch_time: 54.2880 s\n",
      "Epoch 95\n",
      "[====================] 50/50: - running_loss: 2.2242 - running_reg: 0.000000 - running_acc: 0.1959 - lr: 0.00007 - epoch_loss: 2.2422 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 2.2552 - valid_reg: 0.000000 - valid_acc: 0.1870 - epoch_time: 54.2496 s\n",
      "Epoch 96\n",
      "[====================] 50/50: - running_loss: 2.2455 - running_reg: 0.000000 - running_acc: 0.1929 - lr: 0.00007 - epoch_loss: 2.2510 - epoch_reg: 0.000000 - epoch_acc: 0.1875 - valid_loss: 2.2516 - valid_reg: 0.000000 - valid_acc: 0.1855 - epoch_time: 54.3701 s\n",
      "Epoch 97\n",
      "[====================] 50/50: - running_loss: 2.2451 - running_reg: 0.000000 - running_acc: 0.1773 - lr: 0.00007 - epoch_loss: 2.2541 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2540 - valid_reg: 0.000000 - valid_acc: 0.1790 - epoch_time: 54.4819 s\n",
      "Epoch 98\n",
      "[====================] 50/50: - running_loss: 2.2531 - running_reg: 0.000000 - running_acc: 0.1980 - lr: 0.00007 - epoch_loss: 2.2403 - epoch_reg: 0.000000 - epoch_acc: 0.1900 - valid_loss: 2.2601 - valid_reg: 0.000000 - valid_acc: 0.1730 - epoch_time: 54.5276 s\n",
      "Epoch 99\n",
      "[====================] 50/50: - running_loss: 2.2558 - running_reg: 0.000000 - running_acc: 0.1443 - lr: 0.00007 - epoch_loss: 2.2424 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2571 - valid_reg: 0.000000 - valid_acc: 0.1885 - epoch_time: 54.5942 s\n",
      "Epoch 100\n",
      "[====================] 50/50: - running_loss: 2.2546 - running_reg: 0.000000 - running_acc: 0.1839 - lr: 0.00007 - epoch_loss: 2.2600 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 2.2558 - valid_reg: 0.000000 - valid_acc: 0.1810 - epoch_time: 54.3488 s\n",
      "Epoch 101\n",
      "[====================] 50/50: - running_loss: 2.2160 - running_reg: 0.000000 - running_acc: 0.2213 - lr: 0.00007 - epoch_loss: 2.2383 - epoch_reg: 0.000000 - epoch_acc: 0.1956 - valid_loss: 2.2594 - valid_reg: 0.000000 - valid_acc: 0.1885 - epoch_time: 54.3970 s\n",
      "Epoch 102\n",
      "[====================] 50/50: - running_loss: 2.2657 - running_reg: 0.000000 - running_acc: 0.1558 - lr: 0.00007 - epoch_loss: 2.2531 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2568 - valid_reg: 0.000000 - valid_acc: 0.1850 - epoch_time: 54.4692 s\n",
      "Epoch 103\n",
      "[====================] 50/50: - running_loss: 2.2376 - running_reg: 0.000000 - running_acc: 0.2006 - lr: 0.00007 - epoch_loss: 2.2445 - epoch_reg: 0.000000 - epoch_acc: 0.1956 - valid_loss: 2.2593 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.3318 s\n",
      "Epoch 104\n",
      "[====================] 50/50: - running_loss: 2.2544 - running_reg: 0.000000 - running_acc: 0.1787 - lr: 0.00007 - epoch_loss: 2.2430 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 2.2687 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.3965 s\n",
      "Epoch 105\n",
      "[====================] 50/50: - running_loss: 2.2324 - running_reg: 0.000000 - running_acc: 0.1719 - lr: 0.00007 - epoch_loss: 2.2379 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 2.2581 - valid_reg: 0.000000 - valid_acc: 0.1770 - epoch_time: 54.3290 s\n",
      "Epoch 106\n",
      "[====================] 50/50: - running_loss: 2.2431 - running_reg: 0.000000 - running_acc: 0.1738 - lr: 0.00007 - epoch_loss: 2.2543 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 2.2600 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.3642 s\n",
      "Epoch 107\n",
      "[====================] 50/50: - running_loss: 2.2497 - running_reg: 0.000000 - running_acc: 0.1925 - lr: 0.00007 - epoch_loss: 2.2420 - epoch_reg: 0.000000 - epoch_acc: 0.1900 - valid_loss: 2.2647 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.4338 s\n",
      "Epoch 108\n",
      "[====================] 50/50: - running_loss: 2.2542 - running_reg: 0.000000 - running_acc: 0.1785 - lr: 0.00007 - epoch_loss: 2.2425 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 2.2629 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 54.2067 s\n",
      "Epoch 109\n",
      "[====================] 50/50: - running_loss: 2.2488 - running_reg: 0.000000 - running_acc: 0.1706 - lr: 0.00007 - epoch_loss: 2.2473 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2543 - valid_reg: 0.000000 - valid_acc: 0.1795 - epoch_time: 54.1622 s\n",
      "Epoch 110\n",
      "[====================] 50/50: - running_loss: 2.2348 - running_reg: 0.000000 - running_acc: 0.1946 - lr: 0.00007 - epoch_loss: 2.2448 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 2.2557 - valid_reg: 0.000000 - valid_acc: 0.1720 - epoch_time: 54.3204 s\n",
      "Epoch 111\n",
      "[====================] 50/50: - running_loss: 2.2630 - running_reg: 0.000000 - running_acc: 0.1767 - lr: 0.00007 - epoch_loss: 2.2598 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 2.2594 - valid_reg: 0.000000 - valid_acc: 0.1760 - epoch_time: 54.2824 s\n",
      "Epoch 112\n",
      "[====================] 50/50: - running_loss: 2.2385 - running_reg: 0.000000 - running_acc: 0.1757 - lr: 0.00007 - epoch_loss: 2.2471 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2525 - valid_reg: 0.000000 - valid_acc: 0.1865 - epoch_time: 54.1437 s\n",
      "Epoch 113\n",
      "[====================] 50/50: - running_loss: 2.2369 - running_reg: 0.000000 - running_acc: 0.1945 - lr: 0.00007 - epoch_loss: 2.2460 - epoch_reg: 0.000000 - epoch_acc: 0.1863 - valid_loss: 2.2552 - valid_reg: 0.000000 - valid_acc: 0.1865 - epoch_time: 54.0819 s\n",
      "Epoch 114\n",
      "[====================] 50/50: - running_loss: 2.2521 - running_reg: 0.000000 - running_acc: 0.1876 - lr: 0.00007 - epoch_loss: 2.2533 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 2.2541 - valid_reg: 0.000000 - valid_acc: 0.1800 - epoch_time: 54.0766 s\n",
      "Epoch 115\n",
      "[====================] 50/50: - running_loss: 2.2500 - running_reg: 0.000000 - running_acc: 0.1833 - lr: 0.00007 - epoch_loss: 2.2593 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2542 - valid_reg: 0.000000 - valid_acc: 0.1865 - epoch_time: 54.0933 s\n",
      "Epoch 116\n",
      "[====================] 50/50: - running_loss: 2.2764 - running_reg: 0.000000 - running_acc: 0.1592 - lr: 0.00007 - epoch_loss: 2.2622 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2605 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.0394 s\n",
      "Epoch 117\n",
      "[====================] 50/50: - running_loss: 2.2646 - running_reg: 0.000000 - running_acc: 0.1649 - lr: 0.00007 - epoch_loss: 2.2635 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2526 - valid_reg: 0.000000 - valid_acc: 0.1865 - epoch_time: 53.9242 s\n",
      "Epoch 118\n",
      "[====================] 50/50: - running_loss: 2.2690 - running_reg: 0.000000 - running_acc: 0.1688 - lr: 0.00006 - epoch_loss: 2.2529 - epoch_reg: 0.000000 - epoch_acc: 0.1825 - valid_loss: 2.2544 - valid_reg: 0.000000 - valid_acc: 0.1895 - epoch_time: 54.1364 s\n",
      "Epoch 119\n",
      "[====================] 50/50: - running_loss: 2.2633 - running_reg: 0.000000 - running_acc: 0.1646 - lr: 0.00006 - epoch_loss: 2.2544 - epoch_reg: 0.000000 - epoch_acc: 0.1725 - valid_loss: 2.2608 - valid_reg: 0.000000 - valid_acc: 0.1760 - epoch_time: 53.0622 s\n",
      "Epoch 120\n",
      "[====================] 50/50: - running_loss: 2.2416 - running_reg: 0.000000 - running_acc: 0.1794 - lr: 0.00006 - epoch_loss: 2.2453 - epoch_reg: 0.000000 - epoch_acc: 0.1775 - valid_loss: 2.2540 - valid_reg: 0.000000 - valid_acc: 0.1850 - epoch_time: 56.0849 s\n",
      "Epoch 121\n",
      "[====================] 50/50: - running_loss: 2.2444 - running_reg: 0.000000 - running_acc: 0.1918 - lr: 0.00006 - epoch_loss: 2.2506 - epoch_reg: 0.000000 - epoch_acc: 0.1819 - valid_loss: 2.2573 - valid_reg: 0.000000 - valid_acc: 0.1605 - epoch_time: 54.2516 s\n",
      "Epoch 122\n",
      "[====================] 50/50: - running_loss: 2.2463 - running_reg: 0.000000 - running_acc: 0.1811 - lr: 0.00006 - epoch_loss: 2.2449 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2555 - valid_reg: 0.000000 - valid_acc: 0.1755 - epoch_time: 54.3088 s\n",
      "Epoch 123\n",
      "[====================] 50/50: - running_loss: 2.2557 - running_reg: 0.000000 - running_acc: 0.1628 - lr: 0.00006 - epoch_loss: 2.2572 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 2.2610 - valid_reg: 0.000000 - valid_acc: 0.1800 - epoch_time: 54.3414 s\n",
      "Epoch 124\n",
      "[====================] 50/50: - running_loss: 2.2645 - running_reg: 0.000000 - running_acc: 0.1641 - lr: 0.00006 - epoch_loss: 2.2627 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2532 - valid_reg: 0.000000 - valid_acc: 0.1750 - epoch_time: 54.0057 s\n",
      "Epoch 125\n",
      "[====================] 50/50: - running_loss: 2.2299 - running_reg: 0.000000 - running_acc: 0.1980 - lr: 0.00006 - epoch_loss: 2.2379 - epoch_reg: 0.000000 - epoch_acc: 0.1944 - valid_loss: 2.2521 - valid_reg: 0.000000 - valid_acc: 0.1895 - epoch_time: 54.0951 s\n",
      "Epoch 126\n",
      "[====================] 50/50: - running_loss: 2.2553 - running_reg: 0.000000 - running_acc: 0.1970 - lr: 0.00006 - epoch_loss: 2.2525 - epoch_reg: 0.000000 - epoch_acc: 0.1931 - valid_loss: 2.2499 - valid_reg: 0.000000 - valid_acc: 0.1875 - epoch_time: 54.0628 s\n",
      "Epoch 127\n",
      "[====================] 50/50: - running_loss: 2.2705 - running_reg: 0.000000 - running_acc: 0.1723 - lr: 0.00006 - epoch_loss: 2.2628 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2543 - valid_reg: 0.000000 - valid_acc: 0.1810 - epoch_time: 53.9227 s\n",
      "Epoch 128\n",
      "[====================] 50/50: - running_loss: 2.2454 - running_reg: 0.000000 - running_acc: 0.1904 - lr: 0.00006 - epoch_loss: 2.2379 - epoch_reg: 0.000000 - epoch_acc: 0.1937 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1700 - epoch_time: 54.0815 s\n",
      "Epoch 129\n",
      "[====================] 50/50: - running_loss: 2.2576 - running_reg: 0.000000 - running_acc: 0.1804 - lr: 0.00006 - epoch_loss: 2.2506 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2593 - valid_reg: 0.000000 - valid_acc: 0.1735 - epoch_time: 54.0382 s\n",
      "Epoch 130\n",
      "[====================] 50/50: - running_loss: 2.2605 - running_reg: 0.000000 - running_acc: 0.1837 - lr: 0.00006 - epoch_loss: 2.2566 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 2.2604 - valid_reg: 0.000000 - valid_acc: 0.1645 - epoch_time: 53.9345 s\n",
      "Epoch 131\n",
      "[====================] 50/50: - running_loss: 2.2624 - running_reg: 0.000000 - running_acc: 0.1799 - lr: 0.00006 - epoch_loss: 2.2562 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 2.2597 - valid_reg: 0.000000 - valid_acc: 0.1800 - epoch_time: 54.0099 s\n",
      "Epoch 132\n",
      "[====================] 50/50: - running_loss: 2.2604 - running_reg: 0.000000 - running_acc: 0.1766 - lr: 0.00006 - epoch_loss: 2.2616 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2579 - valid_reg: 0.000000 - valid_acc: 0.1745 - epoch_time: 53.9543 s\n",
      "Epoch 133\n",
      "[====================] 50/50: - running_loss: 2.2413 - running_reg: 0.000000 - running_acc: 0.1998 - lr: 0.00006 - epoch_loss: 2.2468 - epoch_reg: 0.000000 - epoch_acc: 0.1925 - valid_loss: 2.2564 - valid_reg: 0.000000 - valid_acc: 0.1605 - epoch_time: 54.0683 s\n",
      "Epoch 134\n",
      "[====================] 50/50: - running_loss: 2.2572 - running_reg: 0.000000 - running_acc: 0.1684 - lr: 0.00006 - epoch_loss: 2.2573 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 2.2518 - valid_reg: 0.000000 - valid_acc: 0.1915 - epoch_time: 54.6375 s\n",
      "Epoch 135\n",
      "[====================] 50/50: - running_loss: 2.2553 - running_reg: 0.000000 - running_acc: 0.1778 - lr: 0.00006 - epoch_loss: 2.2536 - epoch_reg: 0.000000 - epoch_acc: 0.1819 - valid_loss: 2.2519 - valid_reg: 0.000000 - valid_acc: 0.1845 - epoch_time: 54.1206 s\n",
      "Epoch 136\n",
      "[====================] 50/50: - running_loss: 2.2532 - running_reg: 0.000000 - running_acc: 0.1916 - lr: 0.00006 - epoch_loss: 2.2487 - epoch_reg: 0.000000 - epoch_acc: 0.1887 - valid_loss: 2.2596 - valid_reg: 0.000000 - valid_acc: 0.1670 - epoch_time: 53.9953 s\n",
      "Epoch 137\n",
      "[====================] 50/50: - running_loss: 2.2608 - running_reg: 0.000000 - running_acc: 0.1771 - lr: 0.00006 - epoch_loss: 2.2508 - epoch_reg: 0.000000 - epoch_acc: 0.1900 - valid_loss: 2.2552 - valid_reg: 0.000000 - valid_acc: 0.1855 - epoch_time: 54.0866 s\n",
      "Epoch 138\n",
      "[====================] 50/50: - running_loss: 2.2452 - running_reg: 0.000000 - running_acc: 0.1898 - lr: 0.00006 - epoch_loss: 2.2380 - epoch_reg: 0.000000 - epoch_acc: 0.1906 - valid_loss: 2.2585 - valid_reg: 0.000000 - valid_acc: 0.1765 - epoch_time: 54.1213 s\n",
      "Epoch 139\n",
      "[====================] 50/50: - running_loss: 2.2379 - running_reg: 0.000000 - running_acc: 0.1958 - lr: 0.00006 - epoch_loss: 2.2478 - epoch_reg: 0.000000 - epoch_acc: 0.1894 - valid_loss: 2.2580 - valid_reg: 0.000000 - valid_acc: 0.1790 - epoch_time: 53.9611 s\n",
      "Epoch 140\n",
      "[====================] 50/50: - running_loss: 2.2431 - running_reg: 0.000000 - running_acc: 0.1859 - lr: 0.00006 - epoch_loss: 2.2479 - epoch_reg: 0.000000 - epoch_acc: 0.1944 - valid_loss: 2.2559 - valid_reg: 0.000000 - valid_acc: 0.1785 - epoch_time: 54.1262 s\n",
      "Epoch 141\n",
      "[====================] 50/50: - running_loss: 2.2449 - running_reg: 0.000000 - running_acc: 0.1967 - lr: 0.00006 - epoch_loss: 2.2501 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 2.2523 - valid_reg: 0.000000 - valid_acc: 0.1775 - epoch_time: 53.9400 s\n",
      "Epoch 142\n",
      "[====================] 50/50: - running_loss: 2.2411 - running_reg: 0.000000 - running_acc: 0.1846 - lr: 0.00006 - epoch_loss: 2.2465 - epoch_reg: 0.000000 - epoch_acc: 0.1775 - valid_loss: 2.2569 - valid_reg: 0.000000 - valid_acc: 0.1780 - epoch_time: 54.1148 s\n",
      "Epoch 143\n",
      "[====================] 50/50: - running_loss: 2.2521 - running_reg: 0.000000 - running_acc: 0.1810 - lr: 0.00006 - epoch_loss: 2.2570 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2538 - valid_reg: 0.000000 - valid_acc: 0.1690 - epoch_time: 54.0366 s\n",
      "Epoch 144\n",
      "[====================] 50/50: - running_loss: 2.2657 - running_reg: 0.000000 - running_acc: 0.1813 - lr: 0.00006 - epoch_loss: 2.2491 - epoch_reg: 0.000000 - epoch_acc: 0.1975 - valid_loss: 2.2566 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 53.9728 s\n",
      "Epoch 145\n",
      "[====================] 50/50: - running_loss: 2.2272 - running_reg: 0.000000 - running_acc: 0.1926 - lr: 0.00006 - epoch_loss: 2.2379 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 2.2550 - valid_reg: 0.000000 - valid_acc: 0.1805 - epoch_time: 53.9893 s\n",
      "Epoch 146\n",
      "[====================] 50/50: - running_loss: 2.2512 - running_reg: 0.000000 - running_acc: 0.1801 - lr: 0.00006 - epoch_loss: 2.2565 - epoch_reg: 0.000000 - epoch_acc: 0.1737 - valid_loss: 2.2540 - valid_reg: 0.000000 - valid_acc: 0.1740 - epoch_time: 54.0328 s\n",
      "Epoch 147\n",
      "[====================] 50/50: - running_loss: 2.2485 - running_reg: 0.000000 - running_acc: 0.1756 - lr: 0.00006 - epoch_loss: 2.2577 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 2.2530 - valid_reg: 0.000000 - valid_acc: 0.1750 - epoch_time: 53.9967 s\n",
      "Epoch 148\n",
      "[====================] 50/50: - running_loss: 2.2568 - running_reg: 0.000000 - running_acc: 0.1666 - lr: 0.00006 - epoch_loss: 2.2548 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 2.2537 - valid_reg: 0.000000 - valid_acc: 0.1705 - epoch_time: 54.1060 s\n",
      "Epoch 149\n",
      "[====================] 50/50: - running_loss: 2.2430 - running_reg: 0.000000 - running_acc: 0.1925 - lr: 0.00006 - epoch_loss: 2.2359 - epoch_reg: 0.000000 - epoch_acc: 0.1994 - valid_loss: 2.2555 - valid_reg: 0.000000 - valid_acc: 0.1800 - epoch_time: 54.1224 s\n",
      "Epoch 150\n",
      "[====================] 50/50: - running_loss: 2.2445 - running_reg: 0.000000 - running_acc: 0.1858 - lr: 0.00006 - epoch_loss: 2.2395 - epoch_reg: 0.000000 - epoch_acc: 0.1912 - valid_loss: 2.2543 - valid_reg: 0.000000 - valid_acc: 0.1860 - epoch_time: 54.1712 s\n",
      "Epoch 151\n",
      "[====================] 50/50: - running_loss: 2.2527 - running_reg: 0.000000 - running_acc: 0.1705 - lr: 0.00006 - epoch_loss: 2.2474 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2557 - valid_reg: 0.000000 - valid_acc: 0.1745 - epoch_time: 54.2089 s\n",
      "Epoch 152\n",
      "[====================] 50/50: - running_loss: 2.2493 - running_reg: 0.000000 - running_acc: 0.1918 - lr: 0.00006 - epoch_loss: 2.2595 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2509 - valid_reg: 0.000000 - valid_acc: 0.1800 - epoch_time: 54.0736 s\n",
      "Epoch 153\n",
      "[====================] 50/50: - running_loss: 2.2558 - running_reg: 0.000000 - running_acc: 0.1881 - lr: 0.00006 - epoch_loss: 2.2494 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 2.2525 - valid_reg: 0.000000 - valid_acc: 0.1780 - epoch_time: 54.1029 s\n",
      "Epoch 154\n",
      "[====================] 50/50: - running_loss: 2.2257 - running_reg: 0.000000 - running_acc: 0.1912 - lr: 0.00006 - epoch_loss: 2.2357 - epoch_reg: 0.000000 - epoch_acc: 0.1912 - valid_loss: 2.2549 - valid_reg: 0.000000 - valid_acc: 0.1710 - epoch_time: 54.1074 s\n",
      "Epoch 155\n",
      "[====================] 50/50: - running_loss: 2.2498 - running_reg: 0.000000 - running_acc: 0.1857 - lr: 0.00006 - epoch_loss: 2.2508 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 2.2554 - valid_reg: 0.000000 - valid_acc: 0.1710 - epoch_time: 54.0890 s\n",
      "Epoch 156\n",
      "[====================] 50/50: - running_loss: 2.2345 - running_reg: 0.000000 - running_acc: 0.1955 - lr: 0.00006 - epoch_loss: 2.2456 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 2.2562 - valid_reg: 0.000000 - valid_acc: 0.1790 - epoch_time: 54.0126 s\n",
      "Epoch 157\n",
      "[====================] 50/50: - running_loss: 2.2354 - running_reg: 0.000000 - running_acc: 0.1999 - lr: 0.00006 - epoch_loss: 2.2336 - epoch_reg: 0.000000 - epoch_acc: 0.2037 - valid_loss: 2.2541 - valid_reg: 0.000000 - valid_acc: 0.1735 - epoch_time: 53.9572 s\n",
      "Epoch 158\n",
      "[====================] 50/50: - running_loss: 2.2289 - running_reg: 0.000000 - running_acc: 0.1989 - lr: 0.00006 - epoch_loss: 2.2264 - epoch_reg: 0.000000 - epoch_acc: 0.1969 - valid_loss: 2.2591 - valid_reg: 0.000000 - valid_acc: 0.1765 - epoch_time: 54.0709 s\n",
      "Epoch 159\n",
      "[====================] 50/50: - running_loss: 2.2358 - running_reg: 0.000000 - running_acc: 0.1977 - lr: 0.00006 - epoch_loss: 2.2426 - epoch_reg: 0.000000 - epoch_acc: 0.1969 - valid_loss: 2.2631 - valid_reg: 0.000000 - valid_acc: 0.1605 - epoch_time: 53.8996 s\n",
      "Epoch 160\n",
      "[====================] 50/50: - running_loss: 2.2433 - running_reg: 0.000000 - running_acc: 0.1812 - lr: 0.00006 - epoch_loss: 2.2514 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 2.2557 - valid_reg: 0.000000 - valid_acc: 0.1665 - epoch_time: 53.9774 s\n",
      "Epoch 161\n",
      "[====================] 50/50: - running_loss: 2.2479 - running_reg: 0.000000 - running_acc: 0.1917 - lr: 0.00006 - epoch_loss: 2.2435 - epoch_reg: 0.000000 - epoch_acc: 0.1937 - valid_loss: 2.2567 - valid_reg: 0.000000 - valid_acc: 0.1710 - epoch_time: 53.9781 s\n",
      "Epoch 162\n",
      "[====================] 50/50: - running_loss: 2.2484 - running_reg: 0.000000 - running_acc: 0.1682 - lr: 0.00006 - epoch_loss: 2.2458 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 2.2623 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 54.2121 s\n",
      "Epoch 163\n",
      "[====================] 50/50: - running_loss: 2.2794 - running_reg: 0.000000 - running_acc: 0.1380 - lr: 0.00006 - epoch_loss: 2.2688 - epoch_reg: 0.000000 - epoch_acc: 0.1475 - valid_loss: 2.2590 - valid_reg: 0.000000 - valid_acc: 0.1770 - epoch_time: 54.3686 s\n",
      "Epoch 164\n",
      "[====================] 50/50: - running_loss: 2.2730 - running_reg: 0.000000 - running_acc: 0.1635 - lr: 0.00006 - epoch_loss: 2.2446 - epoch_reg: 0.000000 - epoch_acc: 0.1856 - valid_loss: 2.2554 - valid_reg: 0.000000 - valid_acc: 0.1845 - epoch_time: 54.1162 s\n",
      "Epoch 165\n",
      "[====================] 50/50: - running_loss: 2.2285 - running_reg: 0.000000 - running_acc: 0.2038 - lr: 0.00005 - epoch_loss: 2.2327 - epoch_reg: 0.000000 - epoch_acc: 0.1975 - valid_loss: 2.2560 - valid_reg: 0.000000 - valid_acc: 0.1770 - epoch_time: 54.2303 s\n",
      "Epoch 166\n",
      "[====================] 50/50: - running_loss: 2.2506 - running_reg: 0.000000 - running_acc: 0.1909 - lr: 0.00005 - epoch_loss: 2.2438 - epoch_reg: 0.000000 - epoch_acc: 0.1887 - valid_loss: 2.2545 - valid_reg: 0.000000 - valid_acc: 0.1830 - epoch_time: 54.0663 s\n",
      "Epoch 167\n",
      "[====================] 50/50: - running_loss: 2.2428 - running_reg: 0.000000 - running_acc: 0.1769 - lr: 0.00005 - epoch_loss: 2.2488 - epoch_reg: 0.000000 - epoch_acc: 0.1806 - valid_loss: 2.2578 - valid_reg: 0.000000 - valid_acc: 0.1805 - epoch_time: 54.0679 s\n",
      "Epoch 168\n",
      "[====================] 50/50: - running_loss: 2.2406 - running_reg: 0.000000 - running_acc: 0.1736 - lr: 0.00005 - epoch_loss: 2.2506 - epoch_reg: 0.000000 - epoch_acc: 0.1688 - valid_loss: 2.2582 - valid_reg: 0.000000 - valid_acc: 0.1730 - epoch_time: 54.0789 s\n",
      "Epoch 169\n",
      "[====================] 50/50: - running_loss: 2.2675 - running_reg: 0.000000 - running_acc: 0.1671 - lr: 0.00005 - epoch_loss: 2.2553 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2623 - valid_reg: 0.000000 - valid_acc: 0.1725 - epoch_time: 54.2039 s\n",
      "Epoch 170\n",
      "[====================] 50/50: - running_loss: 2.2586 - running_reg: 0.000000 - running_acc: 0.1993 - lr: 0.00005 - epoch_loss: 2.2458 - epoch_reg: 0.000000 - epoch_acc: 0.2000 - valid_loss: 2.2551 - valid_reg: 0.000000 - valid_acc: 0.1710 - epoch_time: 54.0898 s\n",
      "Epoch 171\n",
      "[====================] 50/50: - running_loss: 2.2411 - running_reg: 0.000000 - running_acc: 0.1875 - lr: 0.00005 - epoch_loss: 2.2448 - epoch_reg: 0.000000 - epoch_acc: 0.1850 - valid_loss: 2.2531 - valid_reg: 0.000000 - valid_acc: 0.1720 - epoch_time: 54.0454 s\n",
      "Epoch 172\n",
      "[====================] 50/50: - running_loss: 2.2559 - running_reg: 0.000000 - running_acc: 0.1745 - lr: 0.00005 - epoch_loss: 2.2544 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2561 - valid_reg: 0.000000 - valid_acc: 0.1765 - epoch_time: 54.1777 s\n",
      "Epoch 173\n",
      "[====================] 50/50: - running_loss: 2.2617 - running_reg: 0.000000 - running_acc: 0.1571 - lr: 0.00005 - epoch_loss: 2.2579 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2608 - valid_reg: 0.000000 - valid_acc: 0.1790 - epoch_time: 81.7800 s\n",
      "Epoch 174\n",
      "[====================] 50/50: - running_loss: 2.2452 - running_reg: 0.000000 - running_acc: 0.1691 - lr: 0.00005 - epoch_loss: 2.2403 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2548 - valid_reg: 0.000000 - valid_acc: 0.1830 - epoch_time: 54.1310 s\n",
      "Epoch 175\n",
      "[====================] 50/50: - running_loss: 2.2409 - running_reg: 0.000000 - running_acc: 0.1952 - lr: 0.00005 - epoch_loss: 2.2370 - epoch_reg: 0.000000 - epoch_acc: 0.1894 - valid_loss: 2.2531 - valid_reg: 0.000000 - valid_acc: 0.1835 - epoch_time: 53.9468 s\n",
      "Epoch 176\n",
      "[====================] 50/50: - running_loss: 2.2369 - running_reg: 0.000000 - running_acc: 0.1951 - lr: 0.00005 - epoch_loss: 2.2425 - epoch_reg: 0.000000 - epoch_acc: 0.1863 - valid_loss: 2.2535 - valid_reg: 0.000000 - valid_acc: 0.1840 - epoch_time: 54.2081 s\n",
      "Epoch 177\n",
      "[====================] 50/50: - running_loss: 2.2582 - running_reg: 0.000000 - running_acc: 0.1731 - lr: 0.00005 - epoch_loss: 2.2574 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2584 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 53.9917 s\n",
      "Epoch 178\n",
      "[====================] 50/50: - running_loss: 2.2620 - running_reg: 0.000000 - running_acc: 0.1621 - lr: 0.00005 - epoch_loss: 2.2444 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2587 - valid_reg: 0.000000 - valid_acc: 0.1625 - epoch_time: 53.9579 s\n",
      "Epoch 179\n",
      "[====================] 50/50: - running_loss: 2.2546 - running_reg: 0.000000 - running_acc: 0.1650 - lr: 0.00005 - epoch_loss: 2.2586 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2561 - valid_reg: 0.000000 - valid_acc: 0.1675 - epoch_time: 52.9852 s\n",
      "Epoch 180\n",
      "[====================] 50/50: - running_loss: 2.2644 - running_reg: 0.000000 - running_acc: 0.1768 - lr: 0.00005 - epoch_loss: 2.2528 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2572 - valid_reg: 0.000000 - valid_acc: 0.1710 - epoch_time: 56.1788 s\n",
      "Epoch 181\n",
      "[====================] 50/50: - running_loss: 2.2451 - running_reg: 0.000000 - running_acc: 0.1795 - lr: 0.00005 - epoch_loss: 2.2464 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2560 - valid_reg: 0.000000 - valid_acc: 0.1725 - epoch_time: 54.0178 s\n",
      "Epoch 182\n",
      "[====================] 50/50: - running_loss: 2.2707 - running_reg: 0.000000 - running_acc: 0.1647 - lr: 0.00005 - epoch_loss: 2.2591 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2540 - valid_reg: 0.000000 - valid_acc: 0.1670 - epoch_time: 53.8176 s\n",
      "Epoch 183\n",
      "[====================] 50/50: - running_loss: 2.2544 - running_reg: 0.000000 - running_acc: 0.1803 - lr: 0.00005 - epoch_loss: 2.2465 - epoch_reg: 0.000000 - epoch_acc: 0.1931 - valid_loss: 2.2575 - valid_reg: 0.000000 - valid_acc: 0.1610 - epoch_time: 53.9097 s\n",
      "Epoch 184\n",
      "[====================] 50/50: - running_loss: 2.2322 - running_reg: 0.000000 - running_acc: 0.2048 - lr: 0.00005 - epoch_loss: 2.2426 - epoch_reg: 0.000000 - epoch_acc: 0.1950 - valid_loss: 2.2524 - valid_reg: 0.000000 - valid_acc: 0.1915 - epoch_time: 54.0102 s\n",
      "Epoch 185\n",
      "[====================] 50/50: - running_loss: 2.2443 - running_reg: 0.000000 - running_acc: 0.1892 - lr: 0.00005 - epoch_loss: 2.2523 - epoch_reg: 0.000000 - epoch_acc: 0.1850 - valid_loss: 2.2527 - valid_reg: 0.000000 - valid_acc: 0.1820 - epoch_time: 54.1607 s\n",
      "Epoch 186\n",
      "[====================] 50/50: - running_loss: 2.2448 - running_reg: 0.000000 - running_acc: 0.1847 - lr: 0.00005 - epoch_loss: 2.2439 - epoch_reg: 0.000000 - epoch_acc: 0.1944 - valid_loss: 2.2511 - valid_reg: 0.000000 - valid_acc: 0.1950 - epoch_time: 54.5984 s\n",
      "Epoch 187\n",
      "[====================] 50/50: - running_loss: 2.2526 - running_reg: 0.000000 - running_acc: 0.1979 - lr: 0.00005 - epoch_loss: 2.2465 - epoch_reg: 0.000000 - epoch_acc: 0.1912 - valid_loss: 2.2487 - valid_reg: 0.000000 - valid_acc: 0.1905 - epoch_time: 81.1004 s\n",
      "Epoch 188\n",
      "[====================] 50/50: - running_loss: 2.2659 - running_reg: 0.000000 - running_acc: 0.1769 - lr: 0.00005 - epoch_loss: 2.2492 - epoch_reg: 0.000000 - epoch_acc: 0.1894 - valid_loss: 2.2531 - valid_reg: 0.000000 - valid_acc: 0.1785 - epoch_time: 53.9366 s\n",
      "Epoch 189\n",
      "[====================] 50/50: - running_loss: 2.2701 - running_reg: 0.000000 - running_acc: 0.1568 - lr: 0.00005 - epoch_loss: 2.2586 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2536 - valid_reg: 0.000000 - valid_acc: 0.1815 - epoch_time: 53.8628 s\n",
      "Epoch 190\n",
      "[====================] 50/50: - running_loss: 2.2644 - running_reg: 0.000000 - running_acc: 0.1706 - lr: 0.00005 - epoch_loss: 2.2602 - epoch_reg: 0.000000 - epoch_acc: 0.1819 - valid_loss: 2.2520 - valid_reg: 0.000000 - valid_acc: 0.1870 - epoch_time: 53.9028 s\n",
      "Epoch 191\n",
      "[====================] 50/50: - running_loss: 2.2512 - running_reg: 0.000000 - running_acc: 0.1917 - lr: 0.00005 - epoch_loss: 2.2590 - epoch_reg: 0.000000 - epoch_acc: 0.1825 - valid_loss: 2.2525 - valid_reg: 0.000000 - valid_acc: 0.1765 - epoch_time: 53.8684 s\n",
      "Epoch 192\n",
      "[====================] 50/50: - running_loss: 2.2346 - running_reg: 0.000000 - running_acc: 0.2016 - lr: 0.00005 - epoch_loss: 2.2389 - epoch_reg: 0.000000 - epoch_acc: 0.1931 - valid_loss: 2.2518 - valid_reg: 0.000000 - valid_acc: 0.1785 - epoch_time: 54.0287 s\n",
      "Epoch 193\n",
      "[====================] 50/50: - running_loss: 2.2312 - running_reg: 0.000000 - running_acc: 0.2082 - lr: 0.00005 - epoch_loss: 2.2320 - epoch_reg: 0.000000 - epoch_acc: 0.1937 - valid_loss: 2.2546 - valid_reg: 0.000000 - valid_acc: 0.1790 - epoch_time: 54.0125 s\n",
      "Epoch 194\n",
      "[====================] 50/50: - running_loss: 2.2350 - running_reg: 0.000000 - running_acc: 0.2000 - lr: 0.00005 - epoch_loss: 2.2354 - epoch_reg: 0.000000 - epoch_acc: 0.1963 - valid_loss: 2.2573 - valid_reg: 0.000000 - valid_acc: 0.1810 - epoch_time: 54.1356 s\n",
      "Epoch 195\n",
      "[====================] 50/50: - running_loss: 2.2396 - running_reg: 0.000000 - running_acc: 0.1746 - lr: 0.00005 - epoch_loss: 2.2504 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2549 - valid_reg: 0.000000 - valid_acc: 0.1870 - epoch_time: 53.9569 s\n",
      "Epoch 196\n",
      "[====================] 50/50: - running_loss: 2.2397 - running_reg: 0.000000 - running_acc: 0.1970 - lr: 0.00005 - epoch_loss: 2.2392 - epoch_reg: 0.000000 - epoch_acc: 0.2013 - valid_loss: 2.2492 - valid_reg: 0.000000 - valid_acc: 0.1845 - epoch_time: 54.0723 s\n",
      "Epoch 197\n",
      "[====================] 50/50: - running_loss: 2.2598 - running_reg: 0.000000 - running_acc: 0.1814 - lr: 0.00005 - epoch_loss: 2.2558 - epoch_reg: 0.000000 - epoch_acc: 0.1875 - valid_loss: 2.2505 - valid_reg: 0.000000 - valid_acc: 0.1815 - epoch_time: 54.0789 s\n",
      "Epoch 198\n",
      "[====================] 50/50: - running_loss: 2.2505 - running_reg: 0.000000 - running_acc: 0.1781 - lr: 0.00005 - epoch_loss: 2.2478 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 2.2548 - valid_reg: 0.000000 - valid_acc: 0.1700 - epoch_time: 53.9960 s\n",
      "Epoch 199\n",
      "[====================] 50/50: - running_loss: 2.2294 - running_reg: 0.000000 - running_acc: 0.1874 - lr: 0.00005 - epoch_loss: 2.2428 - epoch_reg: 0.000000 - epoch_acc: 0.1737 - valid_loss: 2.2531 - valid_reg: 0.000000 - valid_acc: 0.1895 - epoch_time: 54.0026 s\n",
      "Epoch 200\n",
      "[====================] 50/50: - running_loss: 2.2472 - running_reg: 0.000000 - running_acc: 0.1894 - lr: 0.00005 - epoch_loss: 2.2431 - epoch_reg: 0.000000 - epoch_acc: 0.1963 - valid_loss: 2.2519 - valid_reg: 0.000000 - valid_acc: 0.1855 - epoch_time: 54.0726 s\n",
      "Epoch 201\n",
      "[====================] 50/50: - running_loss: 2.2588 - running_reg: 0.000000 - running_acc: 0.1711 - lr: 0.00005 - epoch_loss: 2.2522 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2525 - valid_reg: 0.000000 - valid_acc: 0.1815 - epoch_time: 54.1515 s\n",
      "Epoch 202\n",
      "[====================] 50/50: - running_loss: 2.2548 - running_reg: 0.000000 - running_acc: 0.1714 - lr: 0.00005 - epoch_loss: 2.2486 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 2.2536 - valid_reg: 0.000000 - valid_acc: 0.1815 - epoch_time: 54.5684 s\n",
      "Epoch 203\n",
      "[====================] 50/50: - running_loss: 2.2464 - running_reg: 0.000000 - running_acc: 0.2025 - lr: 0.00005 - epoch_loss: 2.2531 - epoch_reg: 0.000000 - epoch_acc: 0.1906 - valid_loss: 2.2537 - valid_reg: 0.000000 - valid_acc: 0.1780 - epoch_time: 54.1933 s\n",
      "Epoch 204\n",
      "[====================] 50/50: - running_loss: 2.2215 - running_reg: 0.000000 - running_acc: 0.2132 - lr: 0.00005 - epoch_loss: 2.2353 - epoch_reg: 0.000000 - epoch_acc: 0.2013 - valid_loss: 2.2660 - valid_reg: 0.000000 - valid_acc: 0.1810 - epoch_time: 54.0337 s\n",
      "Epoch 205\n",
      "[====================] 50/50: - running_loss: 2.2363 - running_reg: 0.000000 - running_acc: 0.1941 - lr: 0.00005 - epoch_loss: 2.2473 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2498 - valid_reg: 0.000000 - valid_acc: 0.1960 - epoch_time: 54.6924 s\n",
      "Epoch 206\n",
      "[====================] 50/50: - running_loss: 2.2327 - running_reg: 0.000000 - running_acc: 0.2118 - lr: 0.00005 - epoch_loss: 2.2414 - epoch_reg: 0.000000 - epoch_acc: 0.1963 - valid_loss: 2.2505 - valid_reg: 0.000000 - valid_acc: 0.1900 - epoch_time: 54.2469 s\n",
      "Epoch 207\n",
      "[====================] 50/50: - running_loss: 2.2210 - running_reg: 0.000000 - running_acc: 0.2165 - lr: 0.00005 - epoch_loss: 2.2220 - epoch_reg: 0.000000 - epoch_acc: 0.2081 - valid_loss: 2.2521 - valid_reg: 0.000000 - valid_acc: 0.1725 - epoch_time: 54.1397 s\n",
      "Epoch 208\n",
      "[====================] 50/50: - running_loss: 2.2543 - running_reg: 0.000000 - running_acc: 0.1763 - lr: 0.00005 - epoch_loss: 2.2408 - epoch_reg: 0.000000 - epoch_acc: 0.1881 - valid_loss: 2.2488 - valid_reg: 0.000000 - valid_acc: 0.1825 - epoch_time: 54.1582 s\n",
      "Epoch 209\n",
      "[====================] 50/50: - running_loss: 2.2509 - running_reg: 0.000000 - running_acc: 0.1925 - lr: 0.00005 - epoch_loss: 2.2460 - epoch_reg: 0.000000 - epoch_acc: 0.1944 - valid_loss: 2.2489 - valid_reg: 0.000000 - valid_acc: 0.1835 - epoch_time: 54.2611 s\n",
      "Epoch 210\n",
      "[====================] 50/50: - running_loss: 2.2145 - running_reg: 0.000000 - running_acc: 0.2226 - lr: 0.00005 - epoch_loss: 2.2238 - epoch_reg: 0.000000 - epoch_acc: 0.2181 - valid_loss: 2.2663 - valid_reg: 0.000000 - valid_acc: 0.1775 - epoch_time: 54.1129 s\n",
      "Epoch 211\n",
      "[====================] 50/50: - running_loss: 2.2644 - running_reg: 0.000000 - running_acc: 0.1759 - lr: 0.00005 - epoch_loss: 2.2502 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2598 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 53.9311 s\n",
      "Epoch 212\n",
      "[====================] 50/50: - running_loss: 2.2590 - running_reg: 0.000000 - running_acc: 0.1893 - lr: 0.00005 - epoch_loss: 2.2561 - epoch_reg: 0.000000 - epoch_acc: 0.1806 - valid_loss: 2.2515 - valid_reg: 0.000000 - valid_acc: 0.1805 - epoch_time: 54.6569 s\n",
      "Epoch 213\n",
      "[====================] 50/50: - running_loss: 2.2318 - running_reg: 0.000000 - running_acc: 0.1990 - lr: 0.00005 - epoch_loss: 2.2367 - epoch_reg: 0.000000 - epoch_acc: 0.1963 - valid_loss: 2.2482 - valid_reg: 0.000000 - valid_acc: 0.1880 - epoch_time: 54.5310 s\n",
      "Epoch 214\n",
      "[====================] 50/50: - running_loss: 2.2350 - running_reg: 0.000000 - running_acc: 0.2026 - lr: 0.00005 - epoch_loss: 2.2296 - epoch_reg: 0.000000 - epoch_acc: 0.2094 - valid_loss: 2.2471 - valid_reg: 0.000000 - valid_acc: 0.1955 - epoch_time: 54.2073 s\n",
      "Epoch 215\n",
      "[====================] 50/50: - running_loss: 2.2264 - running_reg: 0.000000 - running_acc: 0.2183 - lr: 0.00005 - epoch_loss: 2.2406 - epoch_reg: 0.000000 - epoch_acc: 0.1969 - valid_loss: 2.2495 - valid_reg: 0.000000 - valid_acc: 0.1940 - epoch_time: 54.2179 s\n",
      "Epoch 216\n",
      "[====================] 50/50: - running_loss: 2.2424 - running_reg: 0.000000 - running_acc: 0.1978 - lr: 0.00005 - epoch_loss: 2.2292 - epoch_reg: 0.000000 - epoch_acc: 0.2100 - valid_loss: 2.2500 - valid_reg: 0.000000 - valid_acc: 0.1745 - epoch_time: 54.3660 s\n",
      "Epoch 217\n",
      "[====================] 50/50: - running_loss: 2.2485 - running_reg: 0.000000 - running_acc: 0.1834 - lr: 0.00005 - epoch_loss: 2.2402 - epoch_reg: 0.000000 - epoch_acc: 0.1881 - valid_loss: 2.2459 - valid_reg: 0.000000 - valid_acc: 0.1860 - epoch_time: 54.5386 s\n",
      "Epoch 218\n",
      "[====================] 50/50: - running_loss: 2.2489 - running_reg: 0.000000 - running_acc: 0.1929 - lr: 0.00005 - epoch_loss: 2.2509 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 2.2513 - valid_reg: 0.000000 - valid_acc: 0.1855 - epoch_time: 54.6990 s\n",
      "Epoch 219\n",
      "[====================] 50/50: - running_loss: 2.2297 - running_reg: 0.000000 - running_acc: 0.1931 - lr: 0.00005 - epoch_loss: 2.2337 - epoch_reg: 0.000000 - epoch_acc: 0.2025 - valid_loss: 2.2441 - valid_reg: 0.000000 - valid_acc: 0.2010 - epoch_time: 55.2036 s\n",
      "Epoch 220\n",
      "[====================] 50/50: - running_loss: 2.2366 - running_reg: 0.000000 - running_acc: 0.1983 - lr: 0.00005 - epoch_loss: 2.2378 - epoch_reg: 0.000000 - epoch_acc: 0.1950 - valid_loss: 2.2437 - valid_reg: 0.000000 - valid_acc: 0.1995 - epoch_time: 54.2905 s\n",
      "Epoch 221\n",
      "[====================] 50/50: - running_loss: 2.2480 - running_reg: 0.000000 - running_acc: 0.1834 - lr: 0.00005 - epoch_loss: 2.2605 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 2.2532 - valid_reg: 0.000000 - valid_acc: 0.1870 - epoch_time: 54.0223 s\n",
      "Epoch 222\n",
      "[====================] 50/50: - running_loss: 2.2406 - running_reg: 0.000000 - running_acc: 0.2050 - lr: 0.00005 - epoch_loss: 2.2303 - epoch_reg: 0.000000 - epoch_acc: 0.2050 - valid_loss: 2.2491 - valid_reg: 0.000000 - valid_acc: 0.1895 - epoch_time: 54.2124 s\n",
      "Epoch 223\n",
      "[====================] 50/50: - running_loss: 2.2340 - running_reg: 0.000000 - running_acc: 0.2132 - lr: 0.00005 - epoch_loss: 2.2322 - epoch_reg: 0.000000 - epoch_acc: 0.2062 - valid_loss: 2.2481 - valid_reg: 0.000000 - valid_acc: 0.1945 - epoch_time: 54.3509 s\n",
      "Epoch 224\n",
      "[====================] 50/50: - running_loss: 2.2200 - running_reg: 0.000000 - running_acc: 0.2132 - lr: 0.00005 - epoch_loss: 2.2236 - epoch_reg: 0.000000 - epoch_acc: 0.2100 - valid_loss: 2.2401 - valid_reg: 0.000000 - valid_acc: 0.2035 - epoch_time: 55.1519 s\n",
      "Epoch 225\n",
      "[====================] 50/50: - running_loss: 2.2298 - running_reg: 0.000000 - running_acc: 0.2118 - lr: 0.00005 - epoch_loss: 2.2400 - epoch_reg: 0.000000 - epoch_acc: 0.2000 - valid_loss: 2.2344 - valid_reg: 0.000000 - valid_acc: 0.2145 - epoch_time: 55.1280 s\n",
      "Epoch 226\n",
      "[====================] 50/50: - running_loss: 2.2381 - running_reg: 0.000000 - running_acc: 0.2023 - lr: 0.00005 - epoch_loss: 2.2318 - epoch_reg: 0.000000 - epoch_acc: 0.2019 - valid_loss: 2.2289 - valid_reg: 0.000000 - valid_acc: 0.2105 - epoch_time: 54.6209 s\n",
      "Epoch 227\n",
      "[====================] 50/50: - running_loss: 2.2024 - running_reg: 0.000000 - running_acc: 0.2259 - lr: 0.00005 - epoch_loss: 2.2065 - epoch_reg: 0.000000 - epoch_acc: 0.2244 - valid_loss: 2.2308 - valid_reg: 0.000000 - valid_acc: 0.2070 - epoch_time: 54.0679 s\n",
      "Epoch 228\n",
      "[====================] 50/50: - running_loss: 2.2062 - running_reg: 0.000000 - running_acc: 0.2298 - lr: 0.00005 - epoch_loss: 2.2250 - epoch_reg: 0.000000 - epoch_acc: 0.2181 - valid_loss: 2.2383 - valid_reg: 0.000000 - valid_acc: 0.1855 - epoch_time: 54.2837 s\n",
      "Epoch 229\n",
      "[====================] 50/50: - running_loss: 2.2234 - running_reg: 0.000000 - running_acc: 0.2081 - lr: 0.00005 - epoch_loss: 2.2204 - epoch_reg: 0.000000 - epoch_acc: 0.2062 - valid_loss: 2.2090 - valid_reg: 0.000000 - valid_acc: 0.2455 - epoch_time: 54.9473 s\n",
      "Epoch 230\n",
      "[====================] 50/50: - running_loss: 2.2372 - running_reg: 0.000000 - running_acc: 0.1997 - lr: 0.00005 - epoch_loss: 2.2364 - epoch_reg: 0.000000 - epoch_acc: 0.1963 - valid_loss: 2.2212 - valid_reg: 0.000000 - valid_acc: 0.2045 - epoch_time: 54.4250 s\n",
      "Epoch 231\n",
      "[====================] 50/50: - running_loss: 2.1823 - running_reg: 0.000000 - running_acc: 0.2358 - lr: 0.00005 - epoch_loss: 2.2046 - epoch_reg: 0.000000 - epoch_acc: 0.2200 - valid_loss: 2.2015 - valid_reg: 0.000000 - valid_acc: 0.2305 - epoch_time: 54.6994 s\n",
      "Epoch 232\n",
      "[====================] 50/50: - running_loss: 2.2315 - running_reg: 0.000000 - running_acc: 0.2059 - lr: 0.00005 - epoch_loss: 2.2241 - epoch_reg: 0.000000 - epoch_acc: 0.2181 - valid_loss: 2.2130 - valid_reg: 0.000000 - valid_acc: 0.1965 - epoch_time: 54.6663 s\n",
      "Epoch 233\n",
      "[====================] 50/50: - running_loss: 2.1885 - running_reg: 0.000000 - running_acc: 0.2360 - lr: 0.00005 - epoch_loss: 2.2023 - epoch_reg: 0.000000 - epoch_acc: 0.2281 - valid_loss: 2.1250 - valid_reg: 0.000000 - valid_acc: 0.2530 - epoch_time: 54.6696 s\n",
      "Epoch 234\n",
      "[====================] 50/50: - running_loss: 2.0849 - running_reg: 0.000000 - running_acc: 0.2591 - lr: 0.00005 - epoch_loss: 2.0795 - epoch_reg: 0.000000 - epoch_acc: 0.2700 - valid_loss: 2.0222 - valid_reg: 0.000000 - valid_acc: 0.2990 - epoch_time: 54.6612 s\n",
      "Epoch 235\n",
      "[====================] 50/50: - running_loss: 1.9395 - running_reg: 0.000000 - running_acc: 0.3240 - lr: 0.00005 - epoch_loss: 1.9835 - epoch_reg: 0.000000 - epoch_acc: 0.3088 - valid_loss: 1.9264 - valid_reg: 0.000000 - valid_acc: 0.3085 - epoch_time: 54.9088 s\n",
      "Epoch 236\n",
      "[====================] 50/50: - running_loss: 1.9683 - running_reg: 0.000000 - running_acc: 0.3038 - lr: 0.00005 - epoch_loss: 1.9558 - epoch_reg: 0.000000 - epoch_acc: 0.3131 - valid_loss: 1.9028 - valid_reg: 0.000000 - valid_acc: 0.3170 - epoch_time: 55.1125 s\n",
      "Epoch 237\n",
      "[====================] 50/50: - running_loss: 1.9325 - running_reg: 0.000000 - running_acc: 0.3264 - lr: 0.00005 - epoch_loss: 1.8984 - epoch_reg: 0.000000 - epoch_acc: 0.3363 - valid_loss: 1.8712 - valid_reg: 0.000000 - valid_acc: 0.3215 - epoch_time: 55.0014 s\n",
      "Epoch 238\n",
      "[====================] 50/50: - running_loss: 1.8351 - running_reg: 0.000000 - running_acc: 0.3496 - lr: 0.00005 - epoch_loss: 1.8510 - epoch_reg: 0.000000 - epoch_acc: 0.3394 - valid_loss: 1.9335 - valid_reg: 0.000000 - valid_acc: 0.3155 - epoch_time: 54.5486 s\n",
      "Epoch 239\n",
      "[====================] 50/50: - running_loss: 1.9159 - running_reg: 0.000000 - running_acc: 0.3066 - lr: 0.00005 - epoch_loss: 1.8875 - epoch_reg: 0.000000 - epoch_acc: 0.3250 - valid_loss: 1.7855 - valid_reg: 0.000000 - valid_acc: 0.3545 - epoch_time: 54.0094 s\n",
      "Epoch 240\n",
      "[====================] 50/50: - running_loss: 1.8411 - running_reg: 0.000000 - running_acc: 0.3452 - lr: 0.00005 - epoch_loss: 1.8263 - epoch_reg: 0.000000 - epoch_acc: 0.3444 - valid_loss: 1.8294 - valid_reg: 0.000000 - valid_acc: 0.3300 - epoch_time: 56.7985 s\n",
      "Epoch 241\n",
      "[====================] 50/50: - running_loss: 1.8551 - running_reg: 0.000000 - running_acc: 0.3285 - lr: 0.00005 - epoch_loss: 1.8775 - epoch_reg: 0.000000 - epoch_acc: 0.3175 - valid_loss: 1.9308 - valid_reg: 0.000000 - valid_acc: 0.3330 - epoch_time: 54.8750 s\n",
      "Epoch 242\n",
      "[====================] 50/50: - running_loss: 1.8227 - running_reg: 0.000000 - running_acc: 0.3565 - lr: 0.00005 - epoch_loss: 1.8136 - epoch_reg: 0.000000 - epoch_acc: 0.3481 - valid_loss: 1.7694 - valid_reg: 0.000000 - valid_acc: 0.3460 - epoch_time: 54.8839 s\n",
      "Epoch 243\n",
      "[====================] 50/50: - running_loss: 1.8284 - running_reg: 0.000000 - running_acc: 0.3330 - lr: 0.00005 - epoch_loss: 1.8002 - epoch_reg: 0.000000 - epoch_acc: 0.3537 - valid_loss: 1.8333 - valid_reg: 0.000000 - valid_acc: 0.3505 - epoch_time: 54.8523 s\n",
      "Epoch 244\n",
      "[====================] 50/50: - running_loss: 1.8147 - running_reg: 0.000000 - running_acc: 0.3384 - lr: 0.00005 - epoch_loss: 1.7988 - epoch_reg: 0.000000 - epoch_acc: 0.3506 - valid_loss: 1.8273 - valid_reg: 0.000000 - valid_acc: 0.3465 - epoch_time: 54.9736 s\n",
      "Epoch 245\n",
      "[====================] 50/50: - running_loss: 1.8168 - running_reg: 0.000000 - running_acc: 0.3333 - lr: 0.00005 - epoch_loss: 1.7908 - epoch_reg: 0.000000 - epoch_acc: 0.3487 - valid_loss: 1.7669 - valid_reg: 0.000000 - valid_acc: 0.3635 - epoch_time: 55.4654 s\n",
      "Epoch 246\n",
      "[====================] 50/50: - running_loss: 1.7709 - running_reg: 0.000000 - running_acc: 0.3567 - lr: 0.00004 - epoch_loss: 1.7660 - epoch_reg: 0.000000 - epoch_acc: 0.3669 - valid_loss: 1.7655 - valid_reg: 0.000000 - valid_acc: 0.3560 - epoch_time: 54.7966 s\n",
      "Epoch 247\n",
      "[====================] 50/50: - running_loss: 1.7281 - running_reg: 0.000000 - running_acc: 0.3920 - lr: 0.00004 - epoch_loss: 1.7592 - epoch_reg: 0.000000 - epoch_acc: 0.3738 - valid_loss: 1.7705 - valid_reg: 0.000000 - valid_acc: 0.3605 - epoch_time: 54.9763 s\n",
      "Epoch 248\n",
      "[====================] 50/50: - running_loss: 1.8103 - running_reg: 0.000000 - running_acc: 0.3529 - lr: 0.00004 - epoch_loss: 1.7666 - epoch_reg: 0.000000 - epoch_acc: 0.3631 - valid_loss: 1.9554 - valid_reg: 0.000000 - valid_acc: 0.3265 - epoch_time: 54.7487 s\n",
      "Epoch 249\n",
      "[====================] 50/50: - running_loss: 1.7866 - running_reg: 0.000000 - running_acc: 0.3538 - lr: 0.00004 - epoch_loss: 1.8128 - epoch_reg: 0.000000 - epoch_acc: 0.3594 - valid_loss: 1.7555 - valid_reg: 0.000000 - valid_acc: 0.3545 - epoch_time: 54.8435 s\n",
      "Epoch 250\n",
      "[====================] 50/50: - running_loss: 1.7994 - running_reg: 0.000000 - running_acc: 0.3498 - lr: 0.00004 - epoch_loss: 1.7933 - epoch_reg: 0.000000 - epoch_acc: 0.3569 - valid_loss: 1.7682 - valid_reg: 0.000000 - valid_acc: 0.3500 - epoch_time: 54.5874 s\n",
      "Epoch 251\n",
      "[====================] 50/50: - running_loss: 1.7541 - running_reg: 0.000000 - running_acc: 0.3912 - lr: 0.00004 - epoch_loss: 1.7929 - epoch_reg: 0.000000 - epoch_acc: 0.3531 - valid_loss: 1.7574 - valid_reg: 0.000000 - valid_acc: 0.3570 - epoch_time: 54.7782 s\n",
      "Epoch 252\n",
      "[====================] 50/50: - running_loss: 1.7373 - running_reg: 0.000000 - running_acc: 0.3584 - lr: 0.00004 - epoch_loss: 1.7864 - epoch_reg: 0.000000 - epoch_acc: 0.3475 - valid_loss: 1.7559 - valid_reg: 0.000000 - valid_acc: 0.3540 - epoch_time: 54.5688 s\n",
      "Epoch 253\n",
      "[====================] 50/50: - running_loss: 1.8204 - running_reg: 0.000000 - running_acc: 0.3518 - lr: 0.00004 - epoch_loss: 1.7966 - epoch_reg: 0.000000 - epoch_acc: 0.3525 - valid_loss: 1.8066 - valid_reg: 0.000000 - valid_acc: 0.3560 - epoch_time: 54.7200 s\n",
      "Epoch 254\n",
      "[====================] 50/50: - running_loss: 1.7747 - running_reg: 0.000000 - running_acc: 0.3589 - lr: 0.00004 - epoch_loss: 1.7351 - epoch_reg: 0.000000 - epoch_acc: 0.3769 - valid_loss: 1.7432 - valid_reg: 0.000000 - valid_acc: 0.3570 - epoch_time: 54.4424 s\n",
      "Epoch 255\n",
      "[====================] 50/50: - running_loss: 1.7803 - running_reg: 0.000000 - running_acc: 0.3408 - lr: 0.00004 - epoch_loss: 1.7659 - epoch_reg: 0.000000 - epoch_acc: 0.3506 - valid_loss: 1.7524 - valid_reg: 0.000000 - valid_acc: 0.3605 - epoch_time: 54.7609 s\n",
      "Epoch 256\n",
      "[====================] 50/50: - running_loss: 1.8228 - running_reg: 0.000000 - running_acc: 0.3485 - lr: 0.00004 - epoch_loss: 1.7925 - epoch_reg: 0.000000 - epoch_acc: 0.3644 - valid_loss: 1.7546 - valid_reg: 0.000000 - valid_acc: 0.3565 - epoch_time: 54.6172 s\n",
      "Epoch 257\n",
      "[====================] 50/50: - running_loss: 1.8024 - running_reg: 0.000000 - running_acc: 0.3549 - lr: 0.00004 - epoch_loss: 1.7743 - epoch_reg: 0.000000 - epoch_acc: 0.3569 - valid_loss: 1.7586 - valid_reg: 0.000000 - valid_acc: 0.3515 - epoch_time: 54.5920 s\n",
      "Epoch 258\n",
      "[====================] 50/50: - running_loss: 1.7755 - running_reg: 0.000000 - running_acc: 0.3433 - lr: 0.00004 - epoch_loss: 1.7886 - epoch_reg: 0.000000 - epoch_acc: 0.3469 - valid_loss: 1.7442 - valid_reg: 0.000000 - valid_acc: 0.3540 - epoch_time: 54.4008 s\n",
      "Epoch 259\n",
      "[====================] 50/50: - running_loss: 1.7254 - running_reg: 0.000000 - running_acc: 0.3777 - lr: 0.00004 - epoch_loss: 1.7661 - epoch_reg: 0.000000 - epoch_acc: 0.3669 - valid_loss: 1.7586 - valid_reg: 0.000000 - valid_acc: 0.3605 - epoch_time: 54.1392 s\n",
      "Epoch 260\n",
      "[====================] 50/50: - running_loss: 1.7391 - running_reg: 0.000000 - running_acc: 0.3578 - lr: 0.00004 - epoch_loss: 1.7427 - epoch_reg: 0.000000 - epoch_acc: 0.3625 - valid_loss: 1.7695 - valid_reg: 0.000000 - valid_acc: 0.3600 - epoch_time: 54.1067 s\n",
      "Epoch 261\n",
      "[====================] 50/50: - running_loss: 1.8340 - running_reg: 0.000000 - running_acc: 0.3289 - lr: 0.00004 - epoch_loss: 1.7825 - epoch_reg: 0.000000 - epoch_acc: 0.3431 - valid_loss: 1.7478 - valid_reg: 0.000000 - valid_acc: 0.3570 - epoch_time: 54.1254 s\n",
      "Epoch 262\n",
      "[====================] 50/50: - running_loss: 1.7552 - running_reg: 0.000000 - running_acc: 0.3676 - lr: 0.00004 - epoch_loss: 1.7499 - epoch_reg: 0.000000 - epoch_acc: 0.3669 - valid_loss: 1.8128 - valid_reg: 0.000000 - valid_acc: 0.3335 - epoch_time: 54.0874 s\n",
      "Epoch 263\n",
      "[====================] 50/50: - running_loss: 1.7860 - running_reg: 0.000000 - running_acc: 0.3536 - lr: 0.00004 - epoch_loss: 1.7925 - epoch_reg: 0.000000 - epoch_acc: 0.3506 - valid_loss: 1.7560 - valid_reg: 0.000000 - valid_acc: 0.3630 - epoch_time: 54.2004 s\n",
      "Epoch 264\n",
      "[====================] 50/50: - running_loss: 1.8203 - running_reg: 0.000000 - running_acc: 0.3227 - lr: 0.00004 - epoch_loss: 1.7790 - epoch_reg: 0.000000 - epoch_acc: 0.3450 - valid_loss: 1.7796 - valid_reg: 0.000000 - valid_acc: 0.3545 - epoch_time: 54.1817 s\n",
      "Epoch 265\n",
      "[====================] 50/50: - running_loss: 1.7103 - running_reg: 0.000000 - running_acc: 0.3856 - lr: 0.00004 - epoch_loss: 1.7562 - epoch_reg: 0.000000 - epoch_acc: 0.3713 - valid_loss: 1.7373 - valid_reg: 0.000000 - valid_acc: 0.3580 - epoch_time: 54.1454 s\n",
      "Epoch 266\n",
      "[====================] 50/50: - running_loss: 1.7249 - running_reg: 0.000000 - running_acc: 0.3789 - lr: 0.00004 - epoch_loss: 1.7445 - epoch_reg: 0.000000 - epoch_acc: 0.3769 - valid_loss: 1.7378 - valid_reg: 0.000000 - valid_acc: 0.3645 - epoch_time: 55.1320 s\n",
      "Epoch 267\n",
      "[====================] 50/50: - running_loss: 1.8169 - running_reg: 0.000000 - running_acc: 0.3255 - lr: 0.00004 - epoch_loss: 1.8014 - epoch_reg: 0.000000 - epoch_acc: 0.3356 - valid_loss: 1.9172 - valid_reg: 0.000000 - valid_acc: 0.3105 - epoch_time: 54.6212 s\n",
      "Epoch 268\n",
      "[====================] 50/50: - running_loss: 1.7713 - running_reg: 0.000000 - running_acc: 0.3626 - lr: 0.00004 - epoch_loss: 1.8025 - epoch_reg: 0.000000 - epoch_acc: 0.3363 - valid_loss: 1.7495 - valid_reg: 0.000000 - valid_acc: 0.3475 - epoch_time: 54.6877 s\n",
      "Epoch 269\n",
      "[====================] 50/50: - running_loss: 1.7052 - running_reg: 0.000000 - running_acc: 0.3546 - lr: 0.00004 - epoch_loss: 1.7383 - epoch_reg: 0.000000 - epoch_acc: 0.3531 - valid_loss: 1.7451 - valid_reg: 0.000000 - valid_acc: 0.3595 - epoch_time: 54.7664 s\n",
      "Epoch 270\n",
      "[====================] 50/50: - running_loss: 1.7729 - running_reg: 0.000000 - running_acc: 0.3468 - lr: 0.00004 - epoch_loss: 1.7664 - epoch_reg: 0.000000 - epoch_acc: 0.3388 - valid_loss: 1.7382 - valid_reg: 0.000000 - valid_acc: 0.3485 - epoch_time: 54.8879 s\n",
      "Epoch 271\n",
      "[====================] 50/50: - running_loss: 1.7987 - running_reg: 0.000000 - running_acc: 0.3374 - lr: 0.00004 - epoch_loss: 1.7605 - epoch_reg: 0.000000 - epoch_acc: 0.3475 - valid_loss: 1.7475 - valid_reg: 0.000000 - valid_acc: 0.3655 - epoch_time: 55.2782 s\n",
      "Epoch 272\n",
      "[====================] 50/50: - running_loss: 1.7531 - running_reg: 0.000000 - running_acc: 0.3572 - lr: 0.00004 - epoch_loss: 1.7534 - epoch_reg: 0.000000 - epoch_acc: 0.3481 - valid_loss: 1.7372 - valid_reg: 0.000000 - valid_acc: 0.3450 - epoch_time: 54.6449 s\n",
      "Epoch 273\n",
      "[====================] 50/50: - running_loss: 1.7381 - running_reg: 0.000000 - running_acc: 0.3770 - lr: 0.00004 - epoch_loss: 1.7394 - epoch_reg: 0.000000 - epoch_acc: 0.3688 - valid_loss: 1.7519 - valid_reg: 0.000000 - valid_acc: 0.3570 - epoch_time: 54.5334 s\n",
      "Epoch 274\n",
      "[====================] 50/50: - running_loss: 1.7259 - running_reg: 0.000000 - running_acc: 0.3992 - lr: 0.00004 - epoch_loss: 1.7459 - epoch_reg: 0.000000 - epoch_acc: 0.3812 - valid_loss: 1.8191 - valid_reg: 0.000000 - valid_acc: 0.3560 - epoch_time: 54.5587 s\n",
      "Epoch 275\n",
      "[====================] 50/50: - running_loss: 1.7523 - running_reg: 0.000000 - running_acc: 0.3729 - lr: 0.00004 - epoch_loss: 1.7488 - epoch_reg: 0.000000 - epoch_acc: 0.3750 - valid_loss: 1.7913 - valid_reg: 0.000000 - valid_acc: 0.3545 - epoch_time: 54.6709 s\n",
      "Epoch 276\n",
      "[====================] 50/50: - running_loss: 1.7116 - running_reg: 0.000000 - running_acc: 0.3671 - lr: 0.00004 - epoch_loss: 1.7319 - epoch_reg: 0.000000 - epoch_acc: 0.3619 - valid_loss: 1.7285 - valid_reg: 0.000000 - valid_acc: 0.3665 - epoch_time: 55.1672 s\n",
      "Epoch 277\n",
      "[====================] 50/50: - running_loss: 1.6836 - running_reg: 0.000000 - running_acc: 0.3900 - lr: 0.00004 - epoch_loss: 1.7180 - epoch_reg: 0.000000 - epoch_acc: 0.3713 - valid_loss: 1.7345 - valid_reg: 0.000000 - valid_acc: 0.3495 - epoch_time: 54.5080 s\n",
      "Epoch 278\n",
      "[====================] 50/50: - running_loss: 1.7172 - running_reg: 0.000000 - running_acc: 0.3511 - lr: 0.00004 - epoch_loss: 1.7392 - epoch_reg: 0.000000 - epoch_acc: 0.3519 - valid_loss: 1.7303 - valid_reg: 0.000000 - valid_acc: 0.3460 - epoch_time: 54.5916 s\n",
      "Epoch 279\n",
      "[====================] 50/50: - running_loss: 1.6929 - running_reg: 0.000000 - running_acc: 0.3795 - lr: 0.00004 - epoch_loss: 1.7212 - epoch_reg: 0.000000 - epoch_acc: 0.3606 - valid_loss: 1.7373 - valid_reg: 0.000000 - valid_acc: 0.3600 - epoch_time: 54.5567 s\n",
      "Epoch 280\n",
      "[====================] 50/50: - running_loss: 1.7976 - running_reg: 0.000000 - running_acc: 0.3384 - lr: 0.00004 - epoch_loss: 1.7886 - epoch_reg: 0.000000 - epoch_acc: 0.3419 - valid_loss: 1.7211 - valid_reg: 0.000000 - valid_acc: 0.3505 - epoch_time: 54.6323 s\n",
      "Epoch 281\n",
      "[====================] 50/50: - running_loss: 1.7966 - running_reg: 0.000000 - running_acc: 0.3448 - lr: 0.00004 - epoch_loss: 1.7836 - epoch_reg: 0.000000 - epoch_acc: 0.3469 - valid_loss: 1.7616 - valid_reg: 0.000000 - valid_acc: 0.3610 - epoch_time: 54.6394 s\n",
      "Epoch 282\n",
      "[====================] 50/50: - running_loss: 1.7687 - running_reg: 0.000000 - running_acc: 0.3394 - lr: 0.00004 - epoch_loss: 1.7505 - epoch_reg: 0.000000 - epoch_acc: 0.3462 - valid_loss: 1.7475 - valid_reg: 0.000000 - valid_acc: 0.3630 - epoch_time: 54.6004 s\n",
      "Epoch 283\n",
      "[====================] 50/50: - running_loss: 1.7476 - running_reg: 0.000000 - running_acc: 0.3630 - lr: 0.00004 - epoch_loss: 1.7367 - epoch_reg: 0.000000 - epoch_acc: 0.3681 - valid_loss: 1.7280 - valid_reg: 0.000000 - valid_acc: 0.3515 - epoch_time: 81.9116 s\n",
      "Epoch 284\n",
      "[====================] 50/50: - running_loss: 1.7086 - running_reg: 0.000000 - running_acc: 0.3590 - lr: 0.00004 - epoch_loss: 1.7443 - epoch_reg: 0.000000 - epoch_acc: 0.3562 - valid_loss: 1.7213 - valid_reg: 0.000000 - valid_acc: 0.3580 - epoch_time: 54.5603 s\n",
      "Epoch 285\n",
      "[====================] 50/50: - running_loss: 1.7022 - running_reg: 0.000000 - running_acc: 0.3686 - lr: 0.00004 - epoch_loss: 1.7067 - epoch_reg: 0.000000 - epoch_acc: 0.3725 - valid_loss: 1.7253 - valid_reg: 0.000000 - valid_acc: 0.3545 - epoch_time: 54.4331 s\n",
      "Epoch 286\n",
      "[====================] 50/50: - running_loss: 1.6839 - running_reg: 0.000000 - running_acc: 0.3921 - lr: 0.00004 - epoch_loss: 1.7132 - epoch_reg: 0.000000 - epoch_acc: 0.3638 - valid_loss: 1.7264 - valid_reg: 0.000000 - valid_acc: 0.3635 - epoch_time: 54.5921 s\n",
      "Epoch 287\n",
      "[====================] 50/50: - running_loss: 1.7333 - running_reg: 0.000000 - running_acc: 0.3627 - lr: 0.00004 - epoch_loss: 1.7447 - epoch_reg: 0.000000 - epoch_acc: 0.3600 - valid_loss: 1.7248 - valid_reg: 0.000000 - valid_acc: 0.3455 - epoch_time: 54.5444 s\n",
      "Epoch 288\n",
      "[====================] 50/50: - running_loss: 1.6974 - running_reg: 0.000000 - running_acc: 0.3688 - lr: 0.00004 - epoch_loss: 1.7289 - epoch_reg: 0.000000 - epoch_acc: 0.3644 - valid_loss: 1.7400 - valid_reg: 0.000000 - valid_acc: 0.3480 - epoch_time: 54.4415 s\n",
      "Epoch 289\n",
      "[====================] 50/50: - running_loss: 1.7330 - running_reg: 0.000000 - running_acc: 0.3458 - lr: 0.00004 - epoch_loss: 1.7679 - epoch_reg: 0.000000 - epoch_acc: 0.3413 - valid_loss: 1.7230 - valid_reg: 0.000000 - valid_acc: 0.3610 - epoch_time: 54.5898 s\n",
      "Epoch 290\n",
      "[====================] 50/50: - running_loss: 1.7229 - running_reg: 0.000000 - running_acc: 0.3451 - lr: 0.00004 - epoch_loss: 1.7472 - epoch_reg: 0.000000 - epoch_acc: 0.3350 - valid_loss: 1.7552 - valid_reg: 0.000000 - valid_acc: 0.3450 - epoch_time: 54.5161 s\n",
      "Epoch 291\n",
      "[====================] 50/50: - running_loss: 1.7813 - running_reg: 0.000000 - running_acc: 0.3531 - lr: 0.00004 - epoch_loss: 1.7483 - epoch_reg: 0.000000 - epoch_acc: 0.3587 - valid_loss: 1.7589 - valid_reg: 0.000000 - valid_acc: 0.3620 - epoch_time: 54.6185 s\n",
      "Epoch 292\n",
      "[====================] 50/50: - running_loss: 1.7979 - running_reg: 0.000000 - running_acc: 0.3291 - lr: 0.00004 - epoch_loss: 1.8236 - epoch_reg: 0.000000 - epoch_acc: 0.3194 - valid_loss: 1.7244 - valid_reg: 0.000000 - valid_acc: 0.3620 - epoch_time: 54.5517 s\n",
      "Epoch 293\n",
      "[====================] 50/50: - running_loss: 1.7357 - running_reg: 0.000000 - running_acc: 0.3668 - lr: 0.00004 - epoch_loss: 1.7209 - epoch_reg: 0.000000 - epoch_acc: 0.3675 - valid_loss: 1.7222 - valid_reg: 0.000000 - valid_acc: 0.3615 - epoch_time: 54.7094 s\n",
      "Epoch 294\n",
      "[====================] 50/50: - running_loss: 1.7409 - running_reg: 0.000000 - running_acc: 0.3475 - lr: 0.00004 - epoch_loss: 1.7599 - epoch_reg: 0.000000 - epoch_acc: 0.3444 - valid_loss: 1.7402 - valid_reg: 0.000000 - valid_acc: 0.3600 - epoch_time: 54.5095 s\n",
      "Epoch 295\n",
      "[====================] 50/50: - running_loss: 1.7739 - running_reg: 0.000000 - running_acc: 0.3356 - lr: 0.00004 - epoch_loss: 1.7410 - epoch_reg: 0.000000 - epoch_acc: 0.3544 - valid_loss: 1.7537 - valid_reg: 0.000000 - valid_acc: 0.3570 - epoch_time: 54.4274 s\n",
      "Epoch 296\n",
      "[====================] 50/50: - running_loss: 1.7542 - running_reg: 0.000000 - running_acc: 0.3583 - lr: 0.00004 - epoch_loss: 1.7328 - epoch_reg: 0.000000 - epoch_acc: 0.3713 - valid_loss: 1.7308 - valid_reg: 0.000000 - valid_acc: 0.3605 - epoch_time: 54.7947 s\n",
      "Epoch 297\n",
      "[====================] 50/50: - running_loss: 1.7617 - running_reg: 0.000000 - running_acc: 0.3395 - lr: 0.00004 - epoch_loss: 1.7562 - epoch_reg: 0.000000 - epoch_acc: 0.3419 - valid_loss: 1.7562 - valid_reg: 0.000000 - valid_acc: 0.3590 - epoch_time: 54.5263 s\n",
      "Epoch 298\n",
      "[====================] 50/50: - running_loss: 1.7632 - running_reg: 0.000000 - running_acc: 0.3500 - lr: 0.00004 - epoch_loss: 1.7532 - epoch_reg: 0.000000 - epoch_acc: 0.3669 - valid_loss: 1.8003 - valid_reg: 0.000000 - valid_acc: 0.3360 - epoch_time: 54.6498 s\n",
      "Epoch 299\n",
      "[====================] 50/50: - running_loss: 1.7079 - running_reg: 0.000000 - running_acc: 0.3832 - lr: 0.00004 - epoch_loss: 1.7268 - epoch_reg: 0.000000 - epoch_acc: 0.3800 - valid_loss: 1.7197 - valid_reg: 0.000000 - valid_acc: 0.3605 - epoch_time: 53.1470 s\n",
      " - test_loss: 1.7014 - test_reg: 0.000000 - test_acc: 0.3675 - test_time: 16.2548 s\n",
      "\n",
      "Total accuracy: 0.3675\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = [  ]\n",
    "\n",
    "for i in range(1): ####!!!!!!!!!!!!!!\n",
    "  path = 'model_to_test_' + str(i) + '.b'\n",
    "\n",
    "  model, criterion, optimizer, schedule_func, scheduler = training_setup()\n",
    "\n",
    "  checkpoint = train_model(model, path, train_dataset, valid_dataset, optimizer, criterion, scheduler, accumulation_steps, 300, 50, skip_eval=0)\n",
    "  \n",
    "  if checkpoint is None:\n",
    "    break\n",
    "  \n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  \n",
    "  _, _, acc = test(model, criterion, test_dataset)\n",
    "  test_accuracy.append(acc)\n",
    "\n",
    "test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "print(f'\\nTotal accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDRtrE7DmQ2L"
   },
   "outputs": [],
   "source": [
    "lops Simple len norm: 3710, 3730, 3715, 3745, 3675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "195OBZ0KV40u"
   },
   "outputs": [],
   "source": [
    "lops Simple len norm: 2040, 1975, 1970, 3215, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11l3-Q0xOfFH"
   },
   "outputs": [],
   "source": [
    "lops Simple: 0.1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzhAXdelDO3K"
   },
   "outputs": [],
   "source": [
    "lops 1 x OGLU: 0.1835, 0.1830, 0.1835, 0.1820, 0.1815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74SAO5c66OPS"
   },
   "outputs": [],
   "source": [
    "3 x AOGLU l=0.0: 0.6610, 0.6571, 0.6601, 0.6568, 0.6622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmO4BN8-gn7T"
   },
   "outputs": [],
   "source": [
    "FFN l=0.005: 0.6585, 0.6530, 0.6589, 0.6601, 0.6594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEIV04heCzdU"
   },
   "outputs": [],
   "source": [
    "FFN noorth: 0.6582, 0.6456, 0.6605, 0.6591, 0.5521, | 0.6580, 0.6530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGJ6X9biR032"
   },
   "outputs": [],
   "source": [
    "2 x OGLU l=0.1: 0.6596, 0.6591, 0.6578, 0.6635, 0.6601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZsyktpj_Ayo"
   },
   "outputs": [],
   "source": [
    "2 x OGLU l=0.01:0.6639, 0.6594, 0.6591, 0.6586, 0.6625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7_yUbfTdrCR",
    "outputId": "19835d4c-a559-424b-b274-cb1dea6d1ec3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fc6193882d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output[0].detach()\n",
    "    return hook\n",
    "\n",
    "process_inputs = lambda x: torch.Tensor(x.numpy()).to(torch.int64)\n",
    "\n",
    "list(model.blocks[-1].attention.lka.modules())[0][0].register_forward_hook(get_activation('1'))\n",
    "#list(model.blocks[-1].attention.lka.modules())[0][1].register_forward_hook(get_activation('2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhQjEjjecyMN",
    "outputId": "e12c6dfc-e241-4711-c5f2-b54807d891e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': tensor([[[[0.4846, 0.6129, 1.0817,  ..., 0.8458, 0.8182, 0.6113],\n",
       "           [0.6741, 1.0226, 0.9306,  ..., 0.6382, 0.4864, 0.4956],\n",
       "           [0.7144, 0.7925, 0.9776,  ..., 0.8392, 0.8342, 0.6923],\n",
       "           ...,\n",
       "           [0.6351, 0.4956, 0.8323,  ..., 1.1306, 0.8587, 0.7959],\n",
       "           [0.6259, 1.0617, 0.9905,  ..., 0.7249, 0.8088, 0.8153],\n",
       "           [0.7734, 0.8578, 0.5455,  ..., 0.5648, 0.7239, 0.6971]],\n",
       " \n",
       "          [[1.1219, 0.8068, 0.7288,  ..., 0.5271, 1.1042, 0.6128],\n",
       "           [0.8955, 0.9000, 1.3224,  ..., 0.5096, 0.5674, 1.1289],\n",
       "           [0.7283, 0.8557, 0.4822,  ..., 0.8176, 0.3683, 0.6196],\n",
       "           ...,\n",
       "           [0.6962, 0.6160, 0.4413,  ..., 1.0298, 0.8417, 0.8641],\n",
       "           [1.0189, 0.9670, 0.5587,  ..., 0.9078, 0.7291, 0.7901],\n",
       "           [0.6431, 0.5184, 0.5831,  ..., 0.6299, 0.6459, 0.6050]],\n",
       " \n",
       "          [[0.6520, 0.8644, 0.8144,  ..., 0.5583, 0.4932, 0.5764],\n",
       "           [0.3789, 0.7886, 0.5084,  ..., 0.7604, 0.5583, 0.4504],\n",
       "           [0.8306, 0.9530, 0.6154,  ..., 0.7198, 0.6716, 0.8316],\n",
       "           ...,\n",
       "           [0.7072, 0.7885, 0.6645,  ..., 0.5373, 0.8313, 0.7544],\n",
       "           [0.9306, 0.6810, 0.7918,  ..., 0.8676, 0.8027, 0.7283],\n",
       "           [0.6329, 0.6154, 0.7098,  ..., 0.4776, 0.7722, 0.6611]],\n",
       " \n",
       "          [[0.7786, 0.9671, 0.7944,  ..., 0.6249, 0.8022, 0.5712],\n",
       "           [0.5801, 0.6983, 0.5575,  ..., 0.6784, 0.7260, 0.7423],\n",
       "           [0.7165, 0.7591, 0.9826,  ..., 0.4811, 0.4956, 0.8089],\n",
       "           ...,\n",
       "           [0.9886, 0.6557, 0.4287,  ..., 1.0515, 0.5590, 1.2194],\n",
       "           [0.8846, 0.6401, 0.7460,  ..., 0.5371, 0.7053, 0.6377],\n",
       "           [0.9957, 0.2716, 0.6007,  ..., 0.9580, 0.3327, 0.6476]]],\n",
       " \n",
       " \n",
       "         [[[0.8930, 0.6364, 0.6370,  ..., 0.6555, 0.5743, 0.8378],\n",
       "           [0.8831, 0.9794, 0.8412,  ..., 0.5675, 0.6064, 0.7537],\n",
       "           [0.8718, 0.6853, 0.8128,  ..., 0.8434, 0.6892, 0.5453],\n",
       "           ...,\n",
       "           [0.7026, 0.5422, 0.3716,  ..., 0.4875, 0.7247, 0.8934],\n",
       "           [1.2645, 0.7219, 0.6805,  ..., 0.7243, 1.0154, 0.7983],\n",
       "           [0.5913, 0.8728, 0.8055,  ..., 0.9002, 0.7161, 0.8851]],\n",
       " \n",
       "          [[0.4333, 0.6424, 0.4917,  ..., 0.6138, 0.8224, 1.0296],\n",
       "           [0.5902, 1.0217, 0.7909,  ..., 0.6172, 0.5229, 0.6886],\n",
       "           [0.9553, 0.6320, 0.3309,  ..., 1.1288, 0.4941, 0.7927],\n",
       "           ...,\n",
       "           [0.5305, 0.5709, 1.0517,  ..., 0.5401, 0.4635, 1.2611],\n",
       "           [0.5408, 0.6843, 1.0863,  ..., 0.7544, 0.3629, 0.8321],\n",
       "           [0.7382, 0.7683, 0.7612,  ..., 0.7868, 0.5918, 0.5935]],\n",
       " \n",
       "          [[0.7341, 0.9329, 1.0449,  ..., 0.8102, 1.0817, 0.8769],\n",
       "           [0.6250, 0.6260, 0.9342,  ..., 0.7469, 0.5003, 0.5282],\n",
       "           [0.6004, 0.5281, 1.0101,  ..., 0.8523, 0.6920, 0.7553],\n",
       "           ...,\n",
       "           [0.9517, 0.9641, 0.9220,  ..., 0.4221, 0.4426, 0.8048],\n",
       "           [0.4548, 0.5333, 0.5736,  ..., 1.1812, 0.5742, 0.6966],\n",
       "           [0.7830, 0.4640, 0.5448,  ..., 0.6196, 0.5784, 0.7986]],\n",
       " \n",
       "          [[0.7263, 0.8433, 0.6435,  ..., 0.6081, 0.9357, 1.0626],\n",
       "           [0.3649, 0.7983, 0.5728,  ..., 0.4686, 0.9862, 0.7960],\n",
       "           [0.6301, 0.7797, 0.4217,  ..., 0.6837, 0.6317, 0.9112],\n",
       "           ...,\n",
       "           [0.5928, 0.6314, 0.7262,  ..., 0.6301, 0.5901, 1.0862],\n",
       "           [0.5278, 0.4373, 0.4281,  ..., 0.7741, 0.5059, 1.1810],\n",
       "           [0.5299, 0.4793, 0.6697,  ..., 0.5519, 0.4068, 0.5940]]],\n",
       " \n",
       " \n",
       "         [[[0.5173, 0.4609, 0.3646,  ..., 0.7049, 0.8529, 1.0608],\n",
       "           [0.8809, 0.4967, 0.6903,  ..., 0.6765, 0.4130, 0.8474],\n",
       "           [0.4907, 0.7993, 0.4250,  ..., 0.7400, 0.7169, 0.8319],\n",
       "           ...,\n",
       "           [0.7427, 0.6648, 0.7337,  ..., 0.6867, 0.7303, 0.6022],\n",
       "           [0.9273, 0.7987, 1.1405,  ..., 0.6633, 0.4618, 0.4035],\n",
       "           [0.7829, 0.7052, 0.7513,  ..., 0.7967, 0.3077, 0.5792]],\n",
       " \n",
       "          [[0.5744, 0.5170, 0.5111,  ..., 0.6387, 1.4359, 0.6881],\n",
       "           [0.5406, 0.7437, 0.6796,  ..., 0.6085, 0.3997, 0.6651],\n",
       "           [0.9269, 0.7365, 0.6759,  ..., 0.4442, 0.9492, 0.7859],\n",
       "           ...,\n",
       "           [0.5625, 1.0477, 0.6989,  ..., 0.3193, 0.9648, 0.9948],\n",
       "           [0.8559, 0.5630, 0.6819,  ..., 0.7938, 1.0081, 0.7846],\n",
       "           [0.7721, 0.6051, 0.9171,  ..., 0.8199, 0.5124, 0.6793]],\n",
       " \n",
       "          [[0.5939, 0.7135, 0.6993,  ..., 0.8693, 0.7109, 0.4790],\n",
       "           [1.2267, 0.5138, 0.8688,  ..., 0.6919, 0.4456, 1.1771],\n",
       "           [0.8001, 0.5119, 0.4499,  ..., 0.7845, 1.0466, 0.6150],\n",
       "           ...,\n",
       "           [0.9662, 0.5447, 0.5725,  ..., 0.7900, 0.4642, 0.8714],\n",
       "           [0.6128, 1.4435, 1.0496,  ..., 0.4937, 0.8764, 0.7703],\n",
       "           [1.1477, 0.4474, 1.1052,  ..., 0.4511, 0.7057, 1.1471]],\n",
       " \n",
       "          [[0.5751, 0.7363, 0.5543,  ..., 1.2802, 1.0292, 0.9101],\n",
       "           [0.8339, 0.7821, 0.4958,  ..., 0.6689, 0.7094, 1.2240],\n",
       "           [0.6334, 0.6162, 1.0532,  ..., 0.5831, 0.9784, 0.4457],\n",
       "           ...,\n",
       "           [0.9673, 0.5089, 0.5119,  ..., 0.7646, 0.6643, 0.6362],\n",
       "           [0.6229, 0.8128, 0.6808,  ..., 0.8241, 0.7474, 0.5344],\n",
       "           [0.5732, 0.8781, 0.8293,  ..., 0.5295, 0.3167, 0.6894]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.5139, 0.9585, 0.7048,  ..., 0.4706, 1.1845, 0.8306],\n",
       "           [0.6485, 0.6921, 0.7455,  ..., 0.8412, 0.9490, 0.7054],\n",
       "           [0.8574, 0.5853, 0.8126,  ..., 0.8271, 0.9612, 0.7024],\n",
       "           ...,\n",
       "           [0.8571, 0.9179, 1.1562,  ..., 1.0475, 0.6699, 0.6514],\n",
       "           [0.5395, 0.8354, 0.7111,  ..., 0.8484, 0.9584, 0.6190],\n",
       "           [0.8390, 0.5624, 0.5619,  ..., 0.7656, 0.6843, 0.4733]],\n",
       " \n",
       "          [[1.1947, 0.8580, 0.8976,  ..., 0.7886, 0.7540, 0.7705],\n",
       "           [0.8955, 0.8698, 0.7398,  ..., 0.6024, 0.6055, 0.7405],\n",
       "           [0.5969, 0.4527, 0.7469,  ..., 0.5325, 0.7581, 0.8009],\n",
       "           ...,\n",
       "           [0.6167, 0.6102, 0.5931,  ..., 0.9055, 1.1287, 0.8764],\n",
       "           [0.8777, 0.9808, 0.7232,  ..., 0.9431, 0.2721, 0.6570],\n",
       "           [0.5581, 0.4597, 0.7226,  ..., 0.7046, 0.5034, 0.6325]],\n",
       " \n",
       "          [[0.6878, 0.8854, 0.5886,  ..., 0.6195, 0.8855, 0.6604],\n",
       "           [0.7996, 0.5236, 0.8830,  ..., 0.6311, 0.6650, 0.7740],\n",
       "           [0.5857, 0.6802, 0.6618,  ..., 0.4279, 0.5291, 0.7272],\n",
       "           ...,\n",
       "           [0.6958, 0.6722, 0.4040,  ..., 0.7566, 0.5490, 0.2878],\n",
       "           [0.6755, 0.7320, 0.6415,  ..., 0.5381, 0.8886, 1.0171],\n",
       "           [0.7361, 0.7721, 0.8607,  ..., 0.8243, 0.6769, 0.7289]],\n",
       " \n",
       "          [[0.5134, 1.0933, 0.5616,  ..., 0.5353, 0.6832, 0.6531],\n",
       "           [0.7765, 0.6428, 0.7305,  ..., 0.3585, 0.7072, 0.5546],\n",
       "           [0.8771, 0.5484, 0.7344,  ..., 0.6258, 0.7448, 0.3164],\n",
       "           ...,\n",
       "           [0.6185, 0.7809, 1.2525,  ..., 0.9899, 0.4716, 0.6864],\n",
       "           [0.8044, 0.7140, 0.3929,  ..., 0.5716, 0.2625, 0.5923],\n",
       "           [0.7273, 0.9700, 0.7583,  ..., 0.4546, 0.6063, 0.9399]]],\n",
       " \n",
       " \n",
       "         [[[0.4228, 0.8486, 0.9001,  ..., 0.6750, 0.6145, 0.3459],\n",
       "           [0.9708, 0.7545, 0.9327,  ..., 0.6346, 0.7672, 0.7432],\n",
       "           [0.7026, 0.8030, 0.7853,  ..., 0.5845, 0.6818, 0.5152],\n",
       "           ...,\n",
       "           [0.8206, 0.4788, 1.2461,  ..., 0.7114, 0.4759, 0.6254],\n",
       "           [0.6418, 0.6923, 0.8943,  ..., 0.7203, 0.6995, 0.7107],\n",
       "           [0.6397, 0.5745, 1.0160,  ..., 0.8132, 0.7279, 0.5532]],\n",
       " \n",
       "          [[0.7658, 0.6374, 0.7984,  ..., 0.7095, 0.7758, 0.7840],\n",
       "           [0.7832, 0.4628, 0.3681,  ..., 0.7452, 0.7778, 0.7669],\n",
       "           [0.7413, 0.6951, 0.5761,  ..., 1.0948, 0.4968, 0.4787],\n",
       "           ...,\n",
       "           [0.9395, 1.0545, 0.6344,  ..., 0.7220, 0.4989, 0.7187],\n",
       "           [0.7035, 0.8188, 0.5433,  ..., 0.7804, 0.5496, 0.5223],\n",
       "           [0.8402, 0.7453, 0.3474,  ..., 0.6197, 0.7948, 0.6134]],\n",
       " \n",
       "          [[0.3603, 1.1036, 0.5891,  ..., 0.8252, 0.4258, 0.3955],\n",
       "           [0.7111, 0.9473, 0.4409,  ..., 0.8261, 0.5765, 0.4974],\n",
       "           [0.6300, 0.8217, 1.0144,  ..., 0.9546, 1.1750, 1.1056],\n",
       "           ...,\n",
       "           [0.8961, 0.4505, 0.7297,  ..., 0.8524, 0.4372, 1.0410],\n",
       "           [0.7747, 0.5040, 0.6784,  ..., 0.5566, 1.1498, 0.7962],\n",
       "           [0.6596, 1.1634, 0.4957,  ..., 0.7880, 0.5380, 0.6310]],\n",
       " \n",
       "          [[0.5279, 0.7320, 0.9941,  ..., 0.8691, 1.0518, 0.7016],\n",
       "           [0.7898, 0.6883, 0.5457,  ..., 0.3695, 0.5490, 0.7430],\n",
       "           [0.5119, 0.8078, 0.5855,  ..., 0.5621, 0.9829, 0.7496],\n",
       "           ...,\n",
       "           [0.5684, 1.1142, 0.7967,  ..., 0.5090, 0.4609, 0.7501],\n",
       "           [0.4333, 0.6514, 0.8165,  ..., 0.9520, 0.8970, 1.1366],\n",
       "           [0.4308, 0.3284, 0.8361,  ..., 0.8206, 1.0035, 0.4980]]],\n",
       " \n",
       " \n",
       "         [[[0.7467, 0.6446, 0.5154,  ..., 0.5798, 0.6406, 1.1936],\n",
       "           [0.4474, 0.9590, 0.5592,  ..., 0.7921, 0.7791, 0.6336],\n",
       "           [0.9450, 0.5910, 0.7239,  ..., 0.7865, 0.7471, 0.5572],\n",
       "           ...,\n",
       "           [0.5536, 0.6727, 1.0070,  ..., 0.6674, 1.2189, 0.7011],\n",
       "           [0.4842, 0.5871, 0.6954,  ..., 0.7043, 0.9860, 0.7895],\n",
       "           [0.7020, 0.7433, 0.8664,  ..., 0.5529, 0.7889, 0.8005]],\n",
       " \n",
       "          [[0.7196, 0.3207, 0.9218,  ..., 0.6925, 0.9509, 0.4740],\n",
       "           [0.9858, 1.0358, 0.5459,  ..., 0.9464, 0.4016, 0.5775],\n",
       "           [0.7281, 0.9938, 0.8871,  ..., 0.6001, 0.8402, 1.0374],\n",
       "           ...,\n",
       "           [0.8533, 1.0131, 0.5425,  ..., 0.8832, 0.4754, 0.7555],\n",
       "           [0.7653, 1.0432, 0.6744,  ..., 0.5842, 0.7185, 0.8080],\n",
       "           [0.4853, 0.7610, 0.5558,  ..., 0.8416, 0.6044, 0.9784]],\n",
       " \n",
       "          [[0.6224, 1.0267, 0.9031,  ..., 0.6286, 0.7636, 0.6316],\n",
       "           [0.9949, 0.5400, 0.3923,  ..., 0.7494, 0.4017, 0.5003],\n",
       "           [1.2642, 0.4963, 0.8992,  ..., 0.8460, 1.0868, 1.3053],\n",
       "           ...,\n",
       "           [0.6211, 0.5373, 0.8102,  ..., 0.8368, 0.7892, 1.0315],\n",
       "           [0.5378, 0.6785, 0.8107,  ..., 0.6532, 0.7171, 0.5774],\n",
       "           [0.8034, 0.6444, 0.9854,  ..., 0.5444, 0.5273, 0.7480]],\n",
       " \n",
       "          [[0.6030, 0.6670, 0.5706,  ..., 0.5578, 0.6974, 0.9863],\n",
       "           [0.8439, 0.4847, 0.7106,  ..., 0.5610, 0.7778, 0.4506],\n",
       "           [0.7859, 0.3478, 1.0372,  ..., 0.7525, 1.0591, 0.2940],\n",
       "           ...,\n",
       "           [0.6795, 0.5807, 0.8654,  ..., 0.8432, 0.8197, 0.6898],\n",
       "           [0.9770, 0.5094, 0.6678,  ..., 0.7466, 0.6382, 0.5085],\n",
       "           [0.5039, 0.6764, 0.6206,  ..., 0.7895, 1.3537, 0.8089]]]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(process_inputs(next(iter(train_dataset))['inputs']).cuda())\n",
    "activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wE_DENScSGHL",
    "outputId": "f1b8fb1d-a8a5-45a4-eb9b-07bdd2c8f3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model 3464194 params, new model 3663874 params, ratio 1.06\n"
     ]
    }
   ],
   "source": [
    "model, criterion, _, _, _ = training_setup()\n",
    "tmp = next(iter(train_dataset))\n",
    "outputs, aux_losses = model(process_inputs(tmp['inputs']).cuda())\n",
    "\n",
    "aux_losses = sum(aux_losses)\n",
    "(criterion(outputs, process_inputs(tmp['targets']).cuda()) + aux_losses).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_XSqOYYTsDT",
    "outputId": "c9a66ce8-a2e3-4f96-f423-7240192ad3e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-2.6187e-03, -5.7948e-04, -3.2907e-04,  6.2941e-04, -7.1119e-04,\n",
       "           -4.3961e-04,  2.6415e-03,  9.3061e-04,  2.9606e-03, -2.6636e-04,\n",
       "           -1.7387e-03,  1.6021e-03,  9.3624e-04,  1.1273e-03, -3.3982e-04,\n",
       "           -4.0739e-04, -2.0149e-03,  9.0991e-04, -1.0498e-03, -2.7959e-03,\n",
       "            2.5112e-04, -9.3633e-04,  6.2485e-04, -1.3612e-03, -2.9829e-04,\n",
       "           -2.0080e-04,  7.8189e-04,  7.8589e-04, -5.3443e-03, -3.7092e-03,\n",
       "           -2.3028e-04,  3.1588e-03, -2.2930e-04,  3.9736e-03,  4.0218e-04,\n",
       "            1.0651e-03,  7.7839e-03,  1.8654e-04, -3.1373e-03,  1.4372e-03,\n",
       "            4.1014e-03, -1.0334e-03, -8.9334e-04, -2.3530e-04, -1.0687e-03,\n",
       "           -2.1504e-03, -1.1092e-03,  1.7172e-05,  1.2050e-03, -1.5283e-03,\n",
       "            1.4103e-04, -2.1979e-03, -5.9292e-05,  2.5531e-03, -4.6291e-03,\n",
       "            8.9993e-04,  5.3557e-05, -1.8944e-03,  6.1842e-04, -3.0434e-03,\n",
       "           -2.6042e-03, -2.7910e-03,  1.9914e-03,  3.3585e-03]],\n",
       "\n",
       "         [[ 2.7830e-03,  6.5157e-04,  1.3587e-03,  2.6377e-03,  2.6050e-03,\n",
       "            1.3299e-04, -1.4192e-03,  4.3132e-04,  1.1622e-03,  1.8455e-03,\n",
       "           -6.5035e-03,  3.9105e-03,  3.6151e-03, -3.1340e-04, -1.1248e-03,\n",
       "           -9.3471e-03, -1.1168e-03, -1.2783e-03,  1.4718e-03, -1.7144e-03,\n",
       "           -1.5742e-03, -2.5472e-03, -3.9025e-04, -7.1958e-03,  2.0620e-03,\n",
       "            1.1221e-03,  4.0981e-03, -3.0099e-03, -2.8092e-03,  1.3933e-03,\n",
       "            1.2233e-03, -2.4611e-03, -7.4985e-03,  2.3598e-03,  1.5210e-04,\n",
       "            2.4839e-03, -1.8429e-03, -2.0700e-05,  2.3278e-03,  1.8919e-03,\n",
       "           -1.5871e-03, -5.1121e-03, -2.0640e-03, -5.9370e-03, -4.9204e-03,\n",
       "           -9.9108e-03, -1.5696e-03, -8.8422e-04, -2.8757e-03, -1.3572e-03,\n",
       "            3.9639e-04,  1.5128e-03, -8.5317e-04, -2.9370e-03, -1.1195e-03,\n",
       "            2.7233e-03, -4.3108e-03, -3.5303e-03,  2.7051e-03,  2.5042e-03,\n",
       "            7.5812e-04,  6.9706e-03, -1.0707e-03,  1.2404e-03]],\n",
       "\n",
       "         [[-3.8659e-03,  1.0261e-02,  2.7044e-03,  1.9696e-03, -1.4937e-04,\n",
       "           -5.3674e-03,  2.8987e-04,  5.6101e-04,  2.4013e-03,  1.1758e-03,\n",
       "           -3.7829e-03, -3.6720e-03,  5.8277e-04, -1.9636e-03,  3.7169e-03,\n",
       "            2.5876e-04, -5.9365e-03, -1.8047e-03,  3.1761e-03, -3.4872e-03,\n",
       "            2.8748e-03, -2.8431e-03,  6.1902e-03,  5.2007e-03,  6.6551e-03,\n",
       "            4.4296e-04,  3.4092e-04,  1.0645e-03,  8.4146e-03,  3.3517e-03,\n",
       "            4.8221e-03,  3.3170e-03, -3.9772e-05, -5.3822e-03, -1.1954e-03,\n",
       "           -2.6741e-05, -3.7010e-03,  4.8294e-03, -1.1520e-04, -1.9767e-04,\n",
       "           -6.4628e-03, -5.5500e-04,  3.8438e-03, -1.9857e-03,  3.4086e-03,\n",
       "            1.0389e-04,  3.8679e-03, -5.5254e-03, -1.2043e-03,  2.0952e-03,\n",
       "           -2.3705e-03,  5.6132e-03,  3.6401e-03, -5.2342e-03, -3.7908e-04,\n",
       "            1.2614e-03, -2.6454e-03, -4.1037e-03,  4.6147e-04,  3.8222e-03,\n",
       "            3.9316e-03,  1.1161e-03, -1.8010e-03, -9.9954e-03]],\n",
       "\n",
       "         [[ 8.1668e-04,  1.3635e-03,  4.4353e-04, -2.8064e-03,  2.3828e-04,\n",
       "            1.7759e-03,  2.0466e-04, -1.8902e-03,  2.3943e-04, -1.3520e-03,\n",
       "            2.3867e-03, -1.2658e-03, -1.5047e-03,  1.0777e-03, -4.2223e-03,\n",
       "            1.9186e-03, -2.0456e-03,  7.1799e-04,  5.4871e-04,  1.6659e-04,\n",
       "           -2.3044e-03, -3.2187e-03,  1.3099e-03, -1.5698e-03, -1.4281e-03,\n",
       "            2.3084e-03, -3.8172e-04, -1.0892e-03,  1.9725e-04, -1.0847e-03,\n",
       "            7.4281e-05, -9.6833e-04, -2.6583e-03, -1.4236e-03, -4.5038e-03,\n",
       "           -4.5519e-06, -1.1274e-03,  4.4086e-04,  8.0243e-05,  2.8444e-03,\n",
       "            1.6535e-03,  5.1128e-04,  1.4300e-03,  4.1248e-03, -3.1418e-03,\n",
       "           -4.7238e-03,  1.9891e-03,  2.6389e-03, -1.0230e-03, -1.4871e-03,\n",
       "            3.6018e-03,  1.1650e-03,  3.4906e-04,  1.1271e-04,  9.0374e-04,\n",
       "           -1.7273e-03,  3.5433e-05, -3.7748e-05, -1.9616e-03, -1.0358e-03,\n",
       "           -2.2366e-03, -3.8164e-04, -2.1304e-04,  7.8375e-04]]]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.blocks[-1].attention.lka.modules())[0][0].output_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJI-xf2xaLqc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "listops_setup_simple_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
