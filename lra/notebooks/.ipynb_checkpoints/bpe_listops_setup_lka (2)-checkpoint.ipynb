{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0_XMgEPOh7c",
    "outputId": "3d2d7496-5e7f-49c9-c42d-6dca3a0ac8e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 26 20:32:11 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P0    22W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGGnygXCDXLf",
    "outputId": "6e32c2fa-1392-4022-cac5-90a53ee124d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'long-range-arena'...\n",
      "remote: Enumerating objects: 474, done.\u001b[K\n",
      "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
      "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
      "remote: Total 474 (delta 145), reused 137 (delta 137), pack-reused 316\u001b[K\n",
      "Receiving objects: 100% (474/474), 145.27 KiB | 630.00 KiB/s, done.\n",
      "Resolving deltas: 100% (328/328), done.\n",
      "Collecting tensorflow_text\n",
      "  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 12.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
      "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.21.6)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (57.4.0)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 67.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.17.3)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.14.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (4.2.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.5.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.44.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (2.8.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (13.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.37.1)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.2.0)\n",
      "Installing collected packages: tf-estimator-nightly, tensorflow-text\n",
      "Successfully installed tensorflow-text-2.8.2 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/google-research/long-range-arena.git\n",
    "!pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gR2UvkWsbc8C",
    "outputId": "ca62eac0-4996-42c6-af03-622fb6ecb8d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-26 20:32:20--  https://storage.googleapis.com/long-range-arena/lra_release.gz\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 108.177.126.128, 108.177.127.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8288700910 (7.7G) [application/octet-stream]\n",
      "Saving to: ‘lra_release.gz’\n",
      "\n",
      "lra_release.gz      100%[===================>]   7.72G  49.9MB/s    in 2m 51s  \n",
      "\n",
      "2022-04-26 20:35:12 (46.3 MB/s) - ‘lra_release.gz’ saved [8288700910/8288700910]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/long-range-arena/lra_release.gz\n",
    "!gzip -d lra_release.gz\n",
    "!tar -xf lra_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SrMqZQQmd-vh",
    "outputId": "7a492d76-3032-4171-c7e9-ea4799d772d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
      "\u001b[K     |█████████████                   | 834.1 MB 1.2 MB/s eta 0:16:29tcmalloc: large alloc 1147494400 bytes == 0x55f7e38a4000 @  0x7fde7f8da615 0x55f7a98b617c 0x55f7a999647a 0x55f7a98b8f9d 0x55f7a99aad4d 0x55f7a992cec8 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a992cd30 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a99abb76 0x55f7a9928d95 0x55f7a99abb76 0x55f7a9928d95 0x55f7a99abb76 0x55f7a9928d95 0x55f7a98bace9 0x55f7a98fe579 0x55f7a98b9902 0x55f7a992cc4d 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a99288f6 0x55f7a98ba7aa 0x55f7a9928b4f 0x55f7a9927a2e\n",
      "\u001b[K     |████████████████▌               | 1055.7 MB 1.2 MB/s eta 0:14:00tcmalloc: large alloc 1434370048 bytes == 0x55f827efa000 @  0x7fde7f8da615 0x55f7a98b617c 0x55f7a999647a 0x55f7a98b8f9d 0x55f7a99aad4d 0x55f7a992cec8 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a992cd30 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a99abb76 0x55f7a9928d95 0x55f7a99abb76 0x55f7a9928d95 0x55f7a99abb76 0x55f7a9928d95 0x55f7a98bace9 0x55f7a98fe579 0x55f7a98b9902 0x55f7a992cc4d 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a99288f6 0x55f7a98ba7aa 0x55f7a9928b4f 0x55f7a9927a2e\n",
      "\u001b[K     |█████████████████████           | 1336.2 MB 1.2 MB/s eta 0:10:04tcmalloc: large alloc 1792966656 bytes == 0x55f7acd2c000 @  0x7fde7f8da615 0x55f7a98b617c 0x55f7a999647a 0x55f7a98b8f9d 0x55f7a99aad4d 0x55f7a992cec8 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a992cd30 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a99abb76 0x55f7a9928d95 0x55f7a99abb76 0x55f7a9928d95 0x55f7a99abb76 0x55f7a9928d95 0x55f7a98bace9 0x55f7a98fe579 0x55f7a98b9902 0x55f7a992cc4d 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a99288f6 0x55f7a98ba7aa 0x55f7a9928b4f 0x55f7a9927a2e\n",
      "\u001b[K     |██████████████████████████▌     | 1691.1 MB 1.2 MB/s eta 0:05:04tcmalloc: large alloc 2241208320 bytes == 0x55f817b14000 @  0x7fde7f8da615 0x55f7a98b617c 0x55f7a999647a 0x55f7a98b8f9d 0x55f7a99aad4d 0x55f7a992cec8 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a992cd30 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a99abb76 0x55f7a9928d95 0x55f7a99abb76 0x55f7a9928d95 0x55f7a99abb76 0x55f7a9928d95 0x55f7a98bace9 0x55f7a98fe579 0x55f7a98b9902 0x55f7a992cc4d 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a99288f6 0x55f7a98ba7aa 0x55f7a9928b4f 0x55f7a9927a2e\n",
      "\u001b[K     |████████████████████████████████| 2041.3 MB 1.1 MB/s eta 0:00:01tcmalloc: large alloc 2041315328 bytes == 0x55f89d476000 @  0x7fde7f8d91e7 0x55f7a98ec407 0x55f7a98b617c 0x55f7a999647a 0x55f7a98b8f9d 0x55f7a99aad4d 0x55f7a992cec8 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9928b4f 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9928b4f 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9928b4f 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9928b4f 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9928b4f 0x55f7a98ba7aa 0x55f7a9928b4f 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a9927a2e\n",
      "tcmalloc: large alloc 2551644160 bytes == 0x55f98b3ba000 @  0x7fde7f8da615 0x55f7a98b617c 0x55f7a999647a 0x55f7a98b8f9d 0x55f7a99aad4d 0x55f7a992cec8 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9928b4f 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9928b4f 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9928b4f 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9928b4f 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9928b4f 0x55f7a98ba7aa 0x55f7a9928b4f 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a9927a2e 0x55f7a98ba88a 0x55f7a9929719 0x55f7a9927a2e 0x55f7a98baf21\n",
      "\u001b[K     |████████████████████████████████| 2041.3 MB 7.9 kB/s \n",
      "\u001b[?25hCollecting torchvision==0.10.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (20.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6 MB 56.2 MB/s \n",
      "\u001b[?25hCollecting torchaudio==0.9.1\n",
      "  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 5.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1+cu111) (4.1.1)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.1+cu111) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.1+cu111) (1.21.6)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.0+cu111\n",
      "    Uninstalling torch-1.10.0+cu111:\n",
      "      Successfully uninstalled torch-1.10.0+cu111\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.11.1+cu111\n",
      "    Uninstalling torchvision-0.11.1+cu111:\n",
      "      Successfully uninstalled torchvision-0.11.1+cu111\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.10.0+cu111\n",
      "    Uninstalling torchaudio-0.10.0+cu111:\n",
      "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.1+cu111 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.9.1+cu111 torchaudio-0.9.1 torchvision-0.10.1+cu111\n"
     ]
    }
   ],
   "source": [
    "#Execute if A100 is the current GPU\n",
    "\n",
    "!pip3 install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATXw-aRr0sg-",
    "outputId": "58c455f3-0563-4e85-e067-95bc3fc426fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/long-range-arena\n",
      "INFO:tensorflow:/content/lra_release/listops-1000/basic_train.tsv\n",
      "INFO:tensorflow:/content/lra_release/listops-1000/basic_val.tsv\n",
      "INFO:tensorflow:/content/lra_release/listops-1000/basic_test.tsv\n",
      "INFO:tensorflow:Finished preprocessing\n",
      "INFO:tensorflow:Building vocab\n",
      "INFO:tensorflow:Processed 0\n",
      "INFO:tensorflow:Processed 1000\n",
      "INFO:tensorflow:Finished processing vocab size=15\n"
     ]
    }
   ],
   "source": [
    "%cd /content/long-range-arena\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lra_benchmarks.listops.input_pipeline import get_datasets\n",
    "\n",
    "batch_size=4\n",
    "accumulation_steps=32 // batch_size\n",
    "max_length=2000\n",
    "\n",
    "LAMBDA = 0.0\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset, encoder = get_datasets(1, 'basic', data_dir='/content/lra_release/listops-1000/', batch_size=batch_size, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1Ec_MO12HhV",
    "outputId": "63515fd0-b2fc-467f-bf8d-ef777dc2951f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MIN [MIN 8 9 7 6 1 9 4 [MED 3 [MED 0 0 5 2 9 2 6 7 7 7 X [SM [MED 2 [MAX 2 5 3 4 [MIN [MED 2 [MED 4 8 3 X [MIN 6 0 7 3 9 5 X [MAX 9 2 5 4 6 X 9 X [SM 1 6 0 3 3 6 8 X 1 4 4 4 3 [MED 9 0 [MIN 9 2 3 2 8 4 0 X X 8 [SM [SM 3 6 5 9 1 2 8 9 X [MIN 1 0 6 X X X 2 2 X [SM 0 1 6 5 [MAX 8 7 3 X 7 9 X 1 1 1 9 X 9 X 4 6 8 5 [MIN 9 [MED [MED 1 6 6 9 [MED 4 1 6 [MAX [MAX 3 6 0 6 4 X 4 2 3 [MAX 8 6 0 7 4 8 X 8 9 X 1 7 8 X 5 9 3 [MAX [MED [MIN 4 1 1 3 4 7 6 4 0 2 X [MAX 9 2 4 3 6 6 6 9 8 4 X 5 2 [SM 4 7 X [MED 4 8 0 6 2 7 3 6 6 X X 9 5 X X 6 3 8 8 1 X 6 4 1 6 9 2 [MED 7 0 [MAX 0 9 [SM [MIN 9 4 4 [MED 8 8 8 0 9 6 9 2 0 0 X 3 7 X [MIN [MAX 2 1 9 8 0 1 9 8 X 4 [MIN 6 0 4 4 8 5 7 8 5 1 X 5 5 4 5 X 7 [MAX 5 6 3 X 0 1 [SM 0 3 [MED 1 6 5 8 6 7 3 X [MIN 1 9 1 6 0 5 X 7 5 X [SM 1 [SM 7 5 2 X [SM 1 6 4 X 1 9 9 6 0 X 1 X 5 4 8 1 8 4 X 4 0 0 8 2 [SM 7 1 4 X X 9 X [MAX 8 0 [MED 6 5 7 4 4 7 6 2 [SM 1 2 X X 9 6 0 3 X 8 X 8 X 5 1 8 [MIN 1 1 4 [MAX 8 [SM 5 5 [MAX 2 5 6 [SM 7 4 2 [MAX 2 1 5 6 6 9 X 1 [MAX 2 3 [MIN 5 7 3 7 9 [MAX 5 0 X 3 4 8 [MED 4 3 9 0 1 6 3 5 4 2 X X [SM 6 7 0 7 [SM 2 8 X 3 X 2 X 9 [MAX 4 6 [MAX 7 2 9 0 0 [SM 3 2 7 0 5 4 4 9 X X 6 4 5 9 9 1 6 X 2 4 X [MAX [MED [MAX 4 0 X 1 3 0 3 0 6 6 2 X [MED 3 3 [MAX 8 3 0 3 0 4 3 4 X X 2 0 X 7 6 X [SM 5 3 [MIN 4 1 [MAX [MAX 9 9 7 8 X 8 [MAX 2 9 X 3 5 X 3 3 5 [MAX 8 3 2 1 8 5 8 [MIN [MED 2 8 7 5 2 4 X 1 9 6 X 5 X [MED 9 6 2 5 8 6 7 4 X X X 6 3 1 4 X 0 X 1 8 9 0 X X\n",
      "[MED [MIN 8 1 7 4 [MAX 1 2 9 3 0 X 9 X [MIN 2 7 7 5 7 X 2 [SM 2 5 5 6 [MAX 6 9 X 6 4 0 X [MIN 0 2 6 9 9 3 5 [MIN [MAX 3 9 3 4 2 3 6 X 4 [MAX 8 9 X 2 [MED 6 1 [SM 0 [SM 9 [MAX 6 9 [SM 3 7 9 3 5 X 5 8 9 2 [MAX [MIN 3 5 5 9 4 8 7 4 8 2 X 5 X X 2 [SM 4 6 [MED 0 6 4 [MIN 5 6 9 7 0 1 3 X 1 X [MED 6 3 0 3 X 1 [MAX 4 1 3 3 [SM 2 2 2 5 8 6 6 7 7 6 X 5 7 4 3 X 4 0 4 4 X 5 0 [MIN 8 [SM 7 7 [MIN 6 3 3 6 0 X 5 0 [MED 7 6 X 8 [MIN 8 8 2 8 0 4 2 1 X 3 X 0 X 5 [MIN 0 3 [MAX [SM 7 0 2 X 1 [SM 0 7 6 7 1 6 7 X 8 [SM 4 6 1 4 X 0 1 0 X 0 X [MED 3 9 6 [MIN [MAX 7 2 X [SM 4 4 5 5 8 X 0 2 0 X 2 9 X X 5 X 1 1 [SM 7 8 0 3 [MIN 2 [SM 3 6 9 [SM 4 0 [MAX 6 9 5 7 7 1 X X [SM 1 6 4 7 0 [SM 5 6 X 7 [SM 1 0 5 X X 1 0 X 3 [MED 8 [MED 4 5 7 [MIN 1 3 7 0 1 4 3 9 3 X 0 0 6 X 1 5 5 3 X 8 3 5 X 9 7 [SM 5 8 [MED 6 9 7 0 9 [SM 6 [MAX 0 5 5 8 0 0 3 7 9 X 3 3 6 9 8 5 X 4 X 1 5 [MIN 0 1 3 8 [MED 0 [MED 9 5 4 X [SM 8 1 9 8 X [MIN 0 3 6 3 5 2 5 X 3 8 3 1 X 6 1 8 X X 9 7 X 5 1 8 0 X 2 9 1 X X [SM 6 9 8 3 [MAX 3 [SM [MED 9 [MAX [MIN 1 1 [SM 4 4 [MAX 4 5 4 3 2 7 4 8 7 4 X 7 7 8 7 1 X 6 3 1 8 X [MED 8 [MIN 8 [MAX 7 5 1 X 1 5 [SM 2 6 4 7 6 4 3 0 7 5 X 5 4 6 X X X 0 [MIN 8 3 7 2 8 2 [MIN 0 1 4 2 [MED 6 4 3 6 X 1 [MIN [MED 1 7 6 2 9 5 1 X 8 9 [MED 4 2 6 3 4 4 X [SM 3 3 2 8 6 3 X [MAX 9 5 4 8 6 8 X 3 3 [MIN 2 2 X X 0 [MED 6 5 X X [MED 3 1 2 1 X X [MAX 1 0 4 [MED 0 [MIN 7 9 0 1 8 [MAX 0 2 5 4 7 X 3 [SM 5 7 3 5 4 8 2 X X 2 5 5 6 7 X X [MED [MAX [MED 5 8 5 2 2 8 4 [SM 7 5 3 8 3 X 5 5 X [MIN [SM 0 2 5 6 X 0 X 3 3 9 0 X 9 X 7 [MAX 2 6 X 6 1 X 3 X 7 [MED 8 0 [MIN 7 0 0 4 [MED 1 [MIN [MIN 3 [MED 9 7 5 5 1 X 4 7 5 3 2 X 5 4 [MED 2 9 0 8 0 1 9 5 X 9 7 2 X 7 1 3 X X 6 X X 5 0 8 7 X 2 5 X\n",
      "[MAX [MIN [MED 1 4 8 6 7 4 X [MAX 8 5 7 6 [MED 0 2 1 X [MED [SM 6 9 [MIN 7 [SM 7 3 6 2 7 0 [MIN 3 0 [SM 4 0 8 8 X 7 4 [MED 7 8 9 6 4 1 6 4 3 X [MAX 9 8 6 9 6 8 0 7 X X 8 X 5 3 [MIN 5 0 3 0 8 [MED 9 7 8 3 X 3 [MAX 3 [MAX 3 3 1 3 7 5 0 9 1 X 1 X [MAX 1 [MED 0 4 8 4 0 8 X 2 3 6 5 7 [MAX 4 0 6 9 5 X X X 0 0 [MAX 7 6 9 X X [MAX [MED 1 7 [SM 6 [MED 4 4 2 7 4 7 8 X 7 9 [MED 9 8 9 3 6 7 1 X 0 2 2 X [MED 0 [MIN 6 5 7 7 5 4 1 1 2 X X 0 7 [SM 1 [MED 7 3 0 7 2 1 0 9 1 4 X 8 1 2 6 9 X X 5 9 [MED [MED 5 2 [MIN 1 5 2 8 2 2 5 2 X 9 1 7 X 3 7 X 1 X 8 7 X [MAX 6 5 [SM 1 4 2 6 1 X 8 8 [MED 7 9 2 X 3 8 [MIN 4 1 9 6 9 9 X 9 X [MED 4 2 X 5 8 8 X 8 [SM 7 6 6 0 5 X 2 X 6 4 5 3 3 [MIN 4 3 5 1 [MED 4 [MIN 1 2 [MIN 4 [MAX [MIN 4 [SM 6 6 6 X 3 X 2 9 1 [MIN 2 4 [MAX 6 7 X [MAX 6 2 4 8 7 0 3 6 5 4 X X 0 3 6 9 X 9 [MIN [MAX 1 4 4 4 9 [MIN 2 0 0 X [MIN 7 8 1 X 9 2 [MIN 0 5 X X [MED 7 1 X 3 4 7 [MIN 4 4 2 4 5 3 7 0 X X [MIN [SM 3 8 [MIN 9 4 X 4 0 6 6 X 6 [MIN 0 6 0 2 2 1 6 X 4 0 1 9 4 9 X [SM 7 0 9 [SM 3 7 [MIN 2 3 X [SM 9 9 4 4 4 9 6 4 1 2 X 9 1 [MIN 0 6 9 X 4 [MAX 9 3 5 6 9 1 8 0 8 5 X 1 X [SM 1 4 3 7 0 5 7 X 7 [MIN 0 [MAX 0 1 7 0 8 X 9 2 5 8 [SM 1 2 3 X X 0 X 8 X 5 8 6 X 9 8 6 3 X [MED 1 4 [MED 9 5 [SM 0 5 1 3 7 3 X X 6 1 X 7 0 9 X [MAX [MIN 9 1 [MIN 3 4 [MED 6 1 [SM 1 0 3 9 4 [MAX 5 [MAX 7 0 X 1 [MED 9 0 3 1 2 3 1 5 X 0 5 [SM 7 6 1 X 2 1 X X [MIN 6 3 [MIN [MAX 2 7 1 0 6 5 1 7 X 6 [MED 5 2 8 0 5 9 X 1 X [MIN [MAX 5 0 2 0 3 5 7 X [SM 6 5 0 5 6 2 6 9 X [MED 0 2 9 4 0 X 0 [MAX 2 4 7 8 2 4 3 X 9 0 3 3 X [MAX 6 7 4 4 2 7 8 X 8 [MAX 9 4 6 4 9 3 5 [MAX 8 6 3 3 9 4 8 6 5 4 X 7 [MIN 3 0 3 7 4 7 8 3 X X 7 X 9 X 5 2 6 1 X [MED 9 7 2 0 [MIN [MED 3 [SM 8 9 1 1 5 8 3 [MED 5 9 9 4 5 6 9 7 4 X 1 X 2 [SM [MED 0 2 X 8 0 X 2 X 0 X 4 [MAX 5 7 0 3 9 X 3 1 3 X 9 5 X 3 3 9 8 4 3 9 [SM [MIN 1 [MAX 4 9 7 [MIN 0 5 1 5 3 4 [SM [MED 0 7 7 6 7 8 5 4 6 X [MIN 3 7 0 5 9 7 X 2 0 2 X X 6 6 3 9 4 [SM 1 [MED 5 3 2 [SM 9 0 1 5 X 6 0 3 X 8 [MED 7 0 1 3 1 X 8 [MIN [MIN 4 7 X 8 3 X 9 9 6 8 X X [SM [MIN 3 9 1 [MED [MIN 2 3 2 1 6 X 4 3 1 X 3 5 5 [MAX 6 [MED 5 3 9 3 X 3 6 0 8 2 6 1 X X 4 0 1 [MAX 5 [MIN [MIN 8 9 X 3 X 9 X 3 X 7 X 4 3 7 7 0 7 X 0 X [MED 4 [SM 1 [MED 8 [SM 1 [SM 9 9 2 0 6 6 0 7 X 5 1 [MIN [MAX 0 4 X [MED [SM 6 6 9 3 5 8 5 1 7 2 X 9 7 8 4 X 3 [MED 0 0 [MED 7 5 4 7 8 3 2 6 X [MIN 6 9 8 X X 8 9 2 1 2 7 X 3 X 4 X 4 X 8 2 8 1 0 9 X X 8 7 8 3 [MIN 5 0 2 2 [MAX [MIN 0 6 3 8 5 5 8 [MAX [MIN 9 5 [MIN 8 1 0 2 [MED 0 [MAX 3 5 X [MAX 3 2 X 3 9 [MIN 9 4 2 3 0 1 9 X 3 2 0 X X 5 2 9 7 X 1 5 4 5 X 4 0 X 5 3 7 8 X 2 X X\n",
      "[MAX 0 2 7 [SM [MAX 4 6 5 [MIN [SM 3 4 2 6 9 X 5 5 X 9 0 X 4 8 1 8 0 6 7 X [MIN [MED [MED 9 5 [MAX 4 [MIN [MIN 8 9 [MAX 1 0 8 5 2 [MED 1 1 1 0 1 7 4 1 0 X 8 5 [MAX 4 1 9 4 3 8 X X 5 [MAX 5 9 [SM 5 3 0 0 4 8 5 1 X [MIN 2 8 4 0 9 0 0 X 5 5 [SM 8 5 6 4 X X X 3 8 [MAX 1 [MAX 3 7 [SM 0 7 5 1 9 2 8 6 X 8 [MED 6 3 6 4 9 7 7 X 4 5 X [MAX 8 7 8 1 6 X 3 2 4 [MIN [MIN 8 1 5 1 6 9 6 0 X 2 7 4 X 7 1 X [MED 2 9 9 2 [SM 4 [SM 3 4 0 3 8 7 1 9 0 6 X [MIN 7 3 2 0 X 6 5 [SM 5 7 2 3 X 1 [MED 8 4 6 3 9 2 7 X X X 1 6 X X 1 0 [MED 5 5 3 X [MIN 7 [MED [MIN 5 8 X 8 2 X 5 3 4 [SM [MIN 4 [MED 8 0 2 8 5 [MED 6 2 9 3 X 5 X X [SM 6 [MAX 2 5 [MAX 3 4 3 9 6 X 3 [SM 6 1 6 X 7 3 X [MIN [MIN 5 0 X 8 5 5 X 9 8 4 X 1 [MAX 6 [MAX [MIN 6 2 9 1 3 6 1 7 7 X 7 X [SM [MAX 0 9 9 X [MAX 3 8 3 9 8 7 1 8 X 4 X 0 [SM 7 [SM 7 2 2 0 7 5 9 8 X [SM 0 7 5 2 2 3 8 7 X X [SM 8 [MIN 4 2 7 0 7 9 7 X 4 [MIN 5 0 3 1 5 3 2 0 8 2 X 2 4 3 [MIN 4 5 X [MAX 1 1 3 3 0 6 6 8 2 X 7 X 2 9 X 7 X 3 3 4 3 X 9 X 9 X 0 [MED 5 9 4 [SM [MAX 9 4 [MED 0 [MIN [MED 8 8 [MED 4 2 X 5 2 9 X 0 2 X X 5 X [SM 2 3 9 [MIN 7 5 7 [MAX [MIN 1 5 X 0 X 0 4 [MAX [MIN 0 8 7 [MIN 1 5 6 5 1 6 X 8 [MAX 3 2 9 2 4 X X 1 [SM 4 5 5 [MED 2 2 6 X X X [MAX [SM 6 3 [SM 6 3 6 0 2 1 X X [MAX 4 9 [MIN 2 1 5 5 3 6 2 2 5 9 X [SM 4 7 9 8 0 2 8 7 X X X X 5 6 6 8 6 X 7 [MIN 6 0 6 3 6 2 4 4 X 3 5 [SM [MED 1 9 [MAX 6 3 8 [MAX [SM 0 3 9 5 7 1 X 0 5 6 4 0 5 [SM 5 1 7 2 7 3 2 4 X 6 X [MAX [MED 7 0 X [MED 1 9 X 9 X 7 3 0 [MIN 6 6 4 5 [MED 4 1 8 5 0 5 8 4 5 X [MAX 9 9 2 7 2 2 0 X X X [MIN [MAX 8 [MIN 6 5 2 0 6 3 7 3 3 X [SM 9 7 9 5 2 4 X [SM 0 9 4 9 6 2 X X 9 [MAX 4 4 3 8 X 5 9 X 4 [SM 0 [SM 4 6 3 7 X [MED [MED 2 9 0 1 7 X 9 9 0 8 7 6 2 [MAX 5 2 X X 9 [MED 3 [SM 1 9 4 7 1 X [SM 9 0 8 2 4 X [MIN 6 1 0 5 X [MAX 7 4 X X 5 8 2 X 9 [MIN 5 9 1 4 X X 7 [SM [MED 1 0 [MED [MIN 6 2 5 3 3 1 8 4 4 X [SM 4 0 2 9 9 4 9 2 3 8 X X [MAX [MIN 6 8 8 0 7 2 X 5 X [MIN 2 8 2 2 0 2 5 9 [MED 4 0 0 9 9 6 X 0 X 6 7 1 [MAX 6 4 [MED 9 1 7 1 3 9 X 5 X 4 X 6 4 0 5 [SM 3 0 1 [MAX 6 2 3 9 3 6 5 2 X [MIN 8 0 0 X [MED 7 5 [SM 0 6 X [SM 6 8 5 7 5 9 6 2 2 X 3 X X [MED 4 0 [MAX 0 7 8 5 7 1 [MIN 3 1 0 4 0 1 1 2 3 8 X 8 1 3 X 4 6 4 0 X X 6 0 9 3 1 X 3 6 7 X 8 3 3 [MIN [MED 4 3 9 0 5 3 [MIN [MIN 1 [MAX 1 8 [MED 3 8 3 0 3 8 X 3 0 0 5 X [SM [MIN 5 1 2 5 8 X 6 9 3 7 5 3 [MED 5 3 X 7 X 3 X [MED 9 2 [MIN 3 1 X 1 0 3 1 [MIN 0 4 1 X [MED 9 0 1 8 X [MED 2 4 [MAX 6 8 1 2 X [SM 7 6 6 X [MED 2 1 0 6 8 1 8 0 9 0 X 1 1 8 X X [SM 3 5 1 X 8 0 7 [MIN [MIN 2 [MIN 3 0 4 X 1 [MIN 7 1 7 X 6 5 X 6 4 8 9 [MED 6 8 [SM 8 3 1 6 3 9 X 8 1 2 X X 2 7 6 X [MIN 5 0 [MIN 7 5 3 7 3 0 X 5 4 X 3 X 1 6 0 1 6 2 6 4 X 1 X 5 X 4 [MAX [MED 4 7 [MED 3 6 9 4 4 X [MED 4 2 X 5 [MIN [MIN 8 [MAX 5 9 0 [MED [MIN 3 [MIN 8 0 7 5 7 8 6 2 5 3 X 7 1 [MED 2 5 3 8 1 9 X 1 5 [MAX 6 7 3 0 6 7 1 6 X X [MED [SM 4 0 7 9 4 3 8 X 4 X [MIN 2 1 3 0 5 9 [SM 8 2 X X 8 X 7 3 [MED [MAX 0 5 [SM 5 4 9 0 2 5 X 3 X 3 0 X X 9 4 X 8 [MIN 5 8 2 6 8 7 X 1 5 8 6 6 [MIN [MED 8 [SM [SM 5 2 0 4 [MIN 7 9 4 2 5 1 9 X X 6 7 [MED 5 8 X [MIN 1 6 1 8 [MAX 3 2 2 X [MIN 9 6 3 2 1 0 4 0 8 X [MAX 4 3 1 3 7 2 X 1 [MED 0 2 2 3 4 1 X 0 X 5 [MED 7 1 9 7 2 3 X 3 0 9 X 5 6 [MAX 6 2 X 5 2 0 X 1 3 7 1 7 [MED 6 [MED [MED 3 6 1 X [MED 6 [SM 8 6 0 5 8 8 7 5 7 X 3 9 [SM 8 7 6 3 1 8 5 5 3 X 7 0 [MED 2 5 7 8 9 8 5 4 8 X X 9 [MAX 0 [MED 2 2 X 3 [MIN 3 4 X [MAX 8 2 1 7 2 4 1 0 X X 0 [MED 2 6 5 [MED 6 1 0 9 5 X 5 X 5 8 1 X [SM 7 5 7 X [MAX 3 3 9 7 7 X 4 0 9 X [SM [MAX 3 5 [SM 1 6 0 7 [MIN 7 9 7 8 9 7 5 4 X 6 2 0 X 3 X 4 1 0 8 8 8 1 [SM 2 2 7 X 9 X 7 X X X 2 [MIN [MED [MAX 3 9 0 [MAX 4 [MIN 4 1 6 9 [SM [MED 8 8 0 0 0 2 1 X 9 [MED 2 7 8 4 3 1 7 X [MIN 2 0 8 X 2 3 [MIN 7 5 X X [MED [MED 5 5 6 4 4 2 5 3 5 X [SM 1 6 4 8 X 8 7 4 [MED 7 9 5 3 7 4 2 X 0 4 X [MED 9 6 [SM 3 2 X [MIN 3 5 9 2 6 9 9 0 9 X 0 [MIN 4 1 4 8 2 4 X 6 X [SM 6 3 [MIN 0 7 7 X 4 9 [MAX 5 9 5 8 0 X [MED 3 3 1 9 4 9 3 2 4 0 X 2 X [MED 1 6 [MIN 7 6 0 0 5 X [MAX 8 0 9 4 6 X X X 1 7 8 [MAX 1 3 3 3 8 6 7 5 0 X 4 X 0 3 5 X [MAX 8 7 8 8 [MIN 1 5 5 2 6 [MAX 3 [SM [MAX 2 6 1 9 X 6 2 [MIN 9 8 3 4 3 7 X 5 0 5 X 0 [MIN [MAX 2 9 3 7 7 3 6 X 4 0 3 6 6 X 7 3 9 8 X 0 X 8 2 X [MED [MAX [MAX 9 3 [MAX 0 7 1 7 3 4 6 7 X X 0 1 3 9 [SM 6 2 3 X X 4 2 [MED 6 2 3 3 3 0 X 7 [MED 5 1 X 1 4 4 6 X [SM 0 0 0 [MAX 6 3 7 2 1 0 [MED 4 4 X 7 [MED 1 6 9 X 6 X 7 0 5 [MAX 4 [MIN 0 0 4 [MED 7 4 0 [MIN 6 5 7 8 4 6 X [MAX 2 4 5 1 8 8 9 2 5 X 0 5 2 [SM 1 1 8 1 2 2 6 1 7 X [SM 7 1 1 1 2 8 6 5 X X 9 0 2 X [MAX 1 0 8 [MAX 5 7 8 [MIN 5 3 2 6 8 8 1 0 8 1 X 7 8 8 7 X [MED [MED 5 0 3 0 9 9 8 1 3 4 X [MIN 6 6 8 8 1 4 4 1 X 9 X 8 [MAX 2 [MED 6 7 6 2 5 2 3 X X 2 5 X X 1 X 2 1 8 X 8 [MAX [MAX 1 1 X 3 1 3 X 8 0 X 2 5 4 [MAX 6 3 X [MED [MIN 1 2 4 2 X 3 [SM 7 7 4 7 6 1 8 2 2 3 X 1 9 X 6 X 2 3 X\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_dataset))['inputs']\n",
    "\n",
    "for i in range(min(4, batch_size)):\n",
    "  print(encoder.decode(sample[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4ZHtbE300dO"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TEmbedding(nn.Module):\n",
    "  def __init__(self, num_embeddings, hidden_dim, seq_length=1024, padding_idx=0):\n",
    "    super(TEmbedding, self).__init__()\n",
    "    \n",
    "    self.num_embeddings = num_embeddings\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.seq_length = seq_length\n",
    "    self.padding_idx = padding_idx\n",
    "\n",
    "    self.embedding = nn.Embedding(num_embeddings, hidden_dim, padding_idx)\n",
    "    self.pos_embeds  = nn.Parameter(torch.zeros(1, self.seq_length, self.hidden_dim))\n",
    "\n",
    "    self.cls = nn.Parameter(torch.zeros(1, 1, self.hidden_dim)) #!!!!!!! INIT WITH ANOTHER VALUE IF REQUIRED\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, seq_len = input.shape\n",
    "    \n",
    "    embed = self.embedding(input)\n",
    "    embed = embed + self.pos_embeds\n",
    "    embed = torch.cat([ self.cls.expand(batch_size, 1, -1), embed ], axis=1)\n",
    "\n",
    "    return embed\n",
    "    \n",
    "class TAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(TAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   =qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "    \n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "\n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "    q = torch.mul(q, 1. / torch.sqrt(torch.tensor(self.qkv_dim)))\n",
    "\n",
    "    qk = torch.matmul(q, k.transpose(-1, -2))\n",
    "    qk = nn.Softmax(dim=-1)(qk)\n",
    "\n",
    "    def assertion_function(tsr):\n",
    "      tsr = torch.sum(tsr, axis=-1)\n",
    "      tsr = tsr - torch.ones_like(tsr)\n",
    "      return torch.max(torch.abs(tsr)) < 1e-5\n",
    "\n",
    "    assert assertion_function(qk)\n",
    "\n",
    "    qk = self.dropout(qk) #Like in TF implementation; could be done before Softmax by random -inf addition\n",
    "\n",
    "    out = torch.matmul(qk, v)\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "\n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class HWLinear(nn.Module):\n",
    "  def __init__(self, num_heads, input_dim, output_dim, use_bias):\n",
    "    super(HWLinear, self).__init__()\n",
    "    \n",
    "    self.use_bias = use_bias\n",
    "    if use_bias:\n",
    "      self.bias   = nn.Parameter(torch.zeros( (1, num_heads, 1, output_dim)))\n",
    "\n",
    "    self.weight = nn.Parameter(torch.empty( (num_heads, input_dim, output_dim)))\n",
    "\n",
    "    def he_init(m):\n",
    "      s =  np.sqrt( 2. / input_dim )\n",
    "      m.data.normal_(0, s)\n",
    "\n",
    "    he_init(self.weight)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.matmul(x, self.weight)\n",
    "    if self.use_bias:\n",
    "      x += self.bias\n",
    "    return x\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "  def __init__(self, lambda_, objects=None):\n",
    "      super(Lambda, self).__init__()\n",
    "      self.lambda_ = lambda_\n",
    "      self.objects = objects\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.objects is not None:\n",
    "      return self.lambda_(self.objects, x)\n",
    "    return self.lambda_(x)\n",
    "\n",
    "class LKAAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(LKAAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   = qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "\n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "    \n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    #self.lka = nn.Sequential(\n",
    "    #  nn.Linear(self.head_dim, self.head_dim), nn.GELU(),\n",
    "    #  nn.Linear(self.head_dim, self.head_dim), nn.Softplus(beta=2.5),\n",
    "    #)\n",
    "\n",
    "    #256, 4, 16, 1024\n",
    "    #256, 64, 1, 1024\n",
    "    class AMGOLU(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, gate_rank, dropout_rate, gate_nonlinearity, kernel_nonlinearity, use_bias=False):\n",
    "        super(AMGOLU, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads= num_heads\n",
    "        \n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "\n",
    "        self.gate_weight_a = HWLinear(num_heads, self.head_dim, gate_rank, use_bias)\n",
    "        self.gate_weight_b = HWLinear(num_heads, gate_rank, self.head_dim, use_bias)\n",
    "\n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        self.gate_nonlinearity   = gate_nonlinearity\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        forward_info = self.orth_weight(x)\n",
    "        forward_info = self.kernel_nonlinearity(forward_info)\n",
    "\n",
    "        gate_info = self.gate_weight_a(x)\n",
    "        gate_info = self.gate_weight_b(gate_info)\n",
    "        gate_info = self.gate_nonlinearity(gate_info)\n",
    "\n",
    "        x = forward_info * gate_info\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "    class GatedOrthoKernel(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate=0.1, gate_nonlinearity=nn.Sigmoid(), kernel_nonlinearity=nn.Identity(), use_bias=False):\n",
    "        super(GatedOrthoKernel, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "        self.gate_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "\n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        self.gate_nonlinearity   = gate_nonlinearity\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.kernel_nonlinearity(self.orth_weight(x)) * self.gate_nonlinearity(self.gate_weight(x))\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "    class LinearKernel(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate=0.1, kernel_nonlinearity=nn.Identity(), use_bias=False):\n",
    "        super(LinearKernel, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "        \n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.kernel_nonlinearity(self.orth_weight(x))\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "\n",
    "    class HeadWiseFF(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate, nonlinearity=nn.Identity(), use_bias=False, residual=False):\n",
    "        super(HeadWiseFF, self).__init__()\n",
    "        \n",
    "        head_dim = qkv_dim // num_heads\n",
    "\n",
    "        self.bias   = nn.Parameter(torch.empty( (1, num_heads, 1, head_dim)))\n",
    "        self.dropout= nn.Dropout(dropout_rate)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty( (num_heads, head_dim, head_dim)))\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "        #Orthogonal initialization\n",
    "        #Workaround with torch.stack, since Torch initializes a tensor as orthgonal by flattening its trailing dims and QR-factorizing the resulting 2d\n",
    "        \n",
    "        #self.weight = torch.stack([ nn.init.orthogonal_(torch.empty((head_dim, head_dim))) for _ in range(num_heads) ], dim=0)\n",
    "        #self.weight = nn.Parameter(self.weight)\n",
    "\n",
    "        bound = 1 / math.sqrt(head_dim)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.residual= residual\n",
    "\n",
    "      def forward(self, x):\n",
    "        \n",
    "        x, losses = x\n",
    "\n",
    "        bs, hd, seq, hdim = x.shape\n",
    "        y = self.dropout(x)\n",
    "        y = torch.matmul(y, self.weight) #BS, HD, SEQ, HDIM\n",
    "        if self.use_bias:\n",
    "          y += self.bias\n",
    "        y = self.nonlinearity(y)\n",
    "\n",
    "        #loss = torch.eye(hdim, device=self.weight.device).unsqueeze(0).expand(* self.weight.shape)\n",
    "        #loss = nn.MSELoss()(torch.matmul(self.weight, self.weight.transpose(-1, -2)), loss)\n",
    "        #loss *= LAMBDA\n",
    "\n",
    "        #losses.append(loss)\n",
    "\n",
    "        if self.residual:\n",
    "          return x + y, losses\n",
    "        return y, losses\n",
    "\n",
    "    self.lka = nn.Sequential(\n",
    "        \n",
    "        #AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        #AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        #AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Softplus(), False),\n",
    "        \n",
    "        #HeadWiseFF(self.num_heads, self.qkv_dim, dropout_rate, nn.Softplus(), use_bias=False),\n",
    "        \n",
    "        #GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Softplus(), False)\n",
    "\n",
    "        #LinearKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Softplus(), False),\n",
    "\n",
    "        #Lambda(lambda o, x: (o['act'](x[0]), x[1]), { 'act' : nn.Identity() })\n",
    "        \n",
    "    )\n",
    "\n",
    "    self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "    #BS x HEADS x SEQ x HEAD_DIM\n",
    "    \n",
    "    q, _ = self.lka((q, losses))\n",
    "    k, _ = self.lka((k, losses)) #Use this for var kernel\n",
    "\n",
    "    q = q / math.sqrt(self.head_dim)\n",
    "    k = k / math.sqrt(self.head_dim)\n",
    "\n",
    "    numerator = torch.matmul(k.unsqueeze(-1), v.unsqueeze(-2))\n",
    "    numerator = numerator.sum(axis=2)\n",
    "    numerator = torch.matmul(q, numerator)\n",
    "    \n",
    "    denominator = k.sum(axis=2).unsqueeze(-1)\n",
    "    denominator = q.matmul(denominator)\n",
    "\n",
    "    out = numerator / denominator\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "    \n",
    "    #TODO: INSERT DROPOUT\n",
    "    \n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(SimpleAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   =qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "\n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "    \n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "    #self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v) #BS x HEADS x SEQ x HEAD_DIM\n",
    "\n",
    "    _, _, seq_len, _ = q.shape\n",
    "\n",
    "    kv = torch.matmul(k.transpose(-1, -2), v)\n",
    "    kv *= 1 / math.sqrt(seq_len)\n",
    "    kv = self.dropout(kv)\n",
    "\n",
    "    out = torch.matmul(q, kv)\n",
    "    #out *= 1 / math.sqrt(self.head_dim)\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "    \n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    #out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class FtAttention(nn.Module):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(FtAttention, self).__init__()\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    return torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n",
    "\n",
    "class TBlock(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, mlp_dim, num_heads, dropout_rate):\n",
    "    super(TBlock, self).__init__()\n",
    "\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.qkv_dim  = qkv_dim\n",
    "    self.mlp_dim  = mlp_dim\n",
    "\n",
    "    self.layernorm_input = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "    self.layernorm_inter = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "\n",
    "    self.attention = TAttention(hidden_dim, qkv_dim, num_heads, dropout_rate)\n",
    "\n",
    "    self.ffn       = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, mlp_dim), nn.GELU(), nn.Dropout(dropout_rate),\n",
    "        nn.Linear(mlp_dim, hidden_dim), nn.Dropout(dropout_rate),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, input, losses=[]):\n",
    "    x = self.layernorm_input(input)\n",
    "    x = self.attention(x, losses)\n",
    "\n",
    "    x = input + x\n",
    "\n",
    "    y = self.layernorm_inter(x)\n",
    "    x = self.ffn(y) + x\n",
    "\n",
    "    return x\n",
    "\n",
    "class TClassifier(nn.Module):\n",
    "  def __init__(self, classes, hidden_dim, inter_dim, dropout_rate):\n",
    "    super(TClassifier, self).__init__()\n",
    "\n",
    "    self.layernorm = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "    self.dropout   = nn.Dropout(dropout_rate)\n",
    "\n",
    "    self.ffn       = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, inter_dim), nn.GELU(),\n",
    "    )\n",
    "    self.output    = nn.Linear(inter_dim, classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layernorm(x)\n",
    "    x = x[:, 0, :]\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    x = self.ffn(x)\n",
    "    logits = self.output(x)\n",
    "\n",
    "    return logits\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, classes, num_embeddings, seq_len, hidden_dim, qkv_dim, mlp_dim, num_heads, num_blocks, output_mlp_units, internal_dropout_rate=0.1, output_dropout_rate=0.0):\n",
    "    super(Transformer, self).__init__()\n",
    "    \n",
    "    self.embed_layer = TEmbedding(num_embeddings, hidden_dim, seq_len)\n",
    "    self.blocks      = nn.ModuleList([ TBlock(hidden_dim, qkv_dim, mlp_dim, num_heads, internal_dropout_rate) for _ in range(num_blocks) ])\n",
    "    self.classifier  = TClassifier(classes, hidden_dim, output_mlp_units, output_dropout_rate)\n",
    "\n",
    "  def forward(self, pixel_values):\n",
    "    additional_losses = []\n",
    "\n",
    "    x = self.embed_layer(pixel_values)\n",
    "    \n",
    "    for block in self.blocks:\n",
    "      x = block(x, additional_losses)\n",
    "    \n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return x, additional_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKl9SMDoCJeK"
   },
   "outputs": [],
   "source": [
    "def num_parameters(model):\n",
    "  return sum(list(map(\n",
    "      lambda x: np.prod(x[1].shape), model.named_parameters()\n",
    "  )))\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "def model_factory():\n",
    "  model = Transformer(\n",
    "    classes   =n_classes,\n",
    "    num_embeddings=encoder.vocab_size,\n",
    "    seq_len=max_length,\n",
    "    hidden_dim=512,\n",
    "    qkv_dim=512,\n",
    "    num_heads =8,\n",
    "    num_blocks=6,\n",
    "    mlp_dim=2048,\n",
    "    output_mlp_units=2048,\n",
    "    internal_dropout_rate=0.1,\n",
    "    output_dropout_rate=0.0\n",
    "  ).cuda()\n",
    "  \n",
    "  orig_count = num_parameters(model)\n",
    "\n",
    "  for block in model.blocks:\n",
    "    #block.attention = FtAttention()\n",
    "    block.attention = LKAAttention(512, 512, 8, 0.1).cuda()\n",
    "    \n",
    "  \n",
    "  new_count = num_parameters(model)\n",
    "  print(f'Original model {orig_count} params, new model {new_count} params, ratio {new_count / orig_count:.3}')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "l0P77jbqTEqK",
    "outputId": "09510b93-02cb-4ae9-8b13-ebe46a51d4a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model 21019658 params, new model 21806090 params, ratio 1.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9002915d0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def get_schedule(warmup_steps):\n",
    "  def lr_schedule(step):\n",
    "    return 1.0 * np.minimum(1.0, step / warmup_steps) / np.sqrt(np.maximum(step, warmup_steps))\n",
    "\n",
    "  return lr_schedule\n",
    "\n",
    "lr=0.005\n",
    "weight_decay=0.1\n",
    "warmup=1000\n",
    "\n",
    "\n",
    "def const_schedule(lr):\n",
    "  def lr_schedule(step):\n",
    "    return lr\n",
    "  return lr_schedule\n",
    "\n",
    "def training_setup():\n",
    "  model = model_factory()\n",
    "  criterion = nn.CrossEntropyLoss().cuda()\n",
    "  optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  schedule_func = get_schedule(warmup)\n",
    "  #schedule_func = const_schedule(1.0) #<--------- TEMPORARY\n",
    "  scheduler = LambdaLR(optimizer, schedule_func)\n",
    "\n",
    "  return model, criterion, optimizer, schedule_func, scheduler\n",
    "\n",
    "_, _, _, schedule_func, _ = training_setup()\n",
    "\n",
    "plt.plot([ lr * schedule_func(i) for i in range(15000) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_B6ohTx7wgH"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def save_model(model, optimizer, name='/content/drive/MyDrive/Work/Misc/lka-mini-base.tar'):\n",
    "  torch.save({\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              }, name)\n",
    "\n",
    "def progress_bar(len, total, current):\n",
    "  current_scaled = int(round(len * current / total))\n",
    "\n",
    "  s = '[' + '=' * (current_scaled - 1)\n",
    "  s += '>' if current != total else '='\n",
    "  s += '-' * (len - current_scaled) + ']'\n",
    "\n",
    "  return s\n",
    "\n",
    "def accuracy(model_output, labels):\n",
    "  model_output = model_output.argmax(dim=-1)\n",
    "\n",
    "  return (labels == model_output).float().mean().cpu().numpy()\n",
    "\n",
    "def train_model(model, name, train_dataset, valid_dataset, optimizer, criterion, scheduler, accumulation_steps, epochs, epoch_len=64, eps = 1e-5, skip_eval=0):\n",
    "  \n",
    "  best_acc = 0.0\n",
    "  train_datagen = iter(train_dataset)\n",
    "      \n",
    "  for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "      \n",
    "      #epoch start timestamp\n",
    "      t = time.time()\n",
    "\n",
    "      running_loss = 0.0\n",
    "      running_reg  = 0.0\n",
    "      running_acc  = 0.0\n",
    "\n",
    "      running_momentum = 0.99\n",
    "\n",
    "      epoch_loss = [  ]\n",
    "      epoch_reg  = [  ]\n",
    "      epoch_acc  = [  ]\n",
    "\n",
    "      model.train()\n",
    "\n",
    "      print(f'Epoch {epoch}')\n",
    "\n",
    "      process_inputs = lambda x: torch.Tensor(x.numpy()).to(torch.int64)\n",
    "\n",
    "      for i in range(epoch_len):\n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          #accumulate gradients for a certain amount of steps\n",
    "          for k in range(accumulation_steps):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            try:\n",
    "              data = next(train_datagen)\n",
    "            except StopIteration:\n",
    "              train_datagen = iter(train_dataset)\n",
    "              data = next(train_datagen)\n",
    "            except:\n",
    "              break\n",
    "            inputs, labels = data['inputs'], data['targets']\n",
    "            inputs, labels = process_inputs(inputs), process_inputs(labels)\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs, additional_losses = model(inputs)\n",
    "            loss = criterion(outputs + eps, labels)\n",
    "\n",
    "            if torch.any(torch.isnan(loss)):\n",
    "              print(loss)\n",
    "              return None\n",
    "\n",
    "            additional_losses = sum(additional_losses) if additional_losses else torch.Tensor([ 0.0 ]).cuda()\n",
    "            ((loss + additional_losses) / accumulation_steps).backward()\n",
    "\n",
    "            acc = accuracy(outputs, labels)\n",
    "\n",
    "            running_loss = running_loss * running_momentum + (1 - running_momentum) * loss.item()\n",
    "            running_loss_unb = running_loss / (1 - running_momentum ** (i * accumulation_steps + k + 1))\n",
    "\n",
    "            running_acc  = running_acc  * running_momentum + (1 - running_momentum) * acc\n",
    "            running_acc_unb = running_acc / (1 - running_momentum ** (i * accumulation_steps + k + 1))\n",
    "\n",
    "            running_reg  = running_reg  * running_momentum + (1 - running_momentum) * additional_losses.item()\n",
    "            running_reg_unb = running_reg / (1 - running_momentum ** (i * accumulation_steps + k + 1))\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc)\n",
    "            epoch_reg.append(additional_losses.item())\n",
    "\n",
    "          optimizer.step()\n",
    "\n",
    "          pbar = progress_bar(20, epoch_len, i + 1)\n",
    "\n",
    "          print(f'\\r{pbar} {i + 1}/{epoch_len}:', end='')\n",
    "          print(f' - running_loss: {running_loss_unb:.4f} - running_reg: {running_reg_unb:.6f} - running_acc: {running_acc_unb:.4f} - lr: {scheduler.get_last_lr()[0]:.5f}', end='')\n",
    "\n",
    "          scheduler.step()\n",
    "      \n",
    "      epoch_loss = np.mean(epoch_loss)\n",
    "      epoch_acc  = np.mean(epoch_acc)\n",
    "      epoch_reg  = np.mean(epoch_reg)\n",
    "      \n",
    "      print(f' - epoch_loss: {epoch_loss:.4f} - epoch_reg: {epoch_reg:.6f} - epoch_acc: {epoch_acc:.4f}', end='')\n",
    "\n",
    "      epoch_loss, epoch_acc, epoch_reg = [], [], []\n",
    "\n",
    "      \n",
    "      if epoch >= skip_eval:\n",
    "        model.eval()\n",
    "        valid_dataset.repeat()\n",
    "        valid_datagen = iter(valid_dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "          for i, data in enumerate(valid_datagen):\n",
    "\n",
    "            inputs, labels = data['inputs'], data['targets']\n",
    "            inputs, labels = process_inputs(inputs), process_inputs(labels)\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            outputs, aux_losses = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = accuracy(outputs, labels)\n",
    "            aux_losses = sum(aux_losses) if aux_losses else torch.Tensor([ 0.0 ]).cuda()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc)\n",
    "            epoch_reg.append(aux_losses.item())\n",
    "\n",
    "        epoch_loss, epoch_acc, epoch_reg = np.mean(epoch_loss), np.mean(epoch_acc), np.mean(epoch_reg)\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "          best_acc = epoch_acc\n",
    "          save_model(model, optimizer, name)\n",
    "      \n",
    "      else:\n",
    "        epoch_loss, epoch_acc, epoch_reg = 0.0, 0.0, 0.0\n",
    "\n",
    "      #epoch computing time\n",
    "      t = time.time() - t\n",
    "\n",
    "      print(f' - valid_loss: {epoch_loss:.4f} - valid_reg: {epoch_reg:.6f} - valid_acc: {epoch_acc:.4f} - epoch_time: {t:.4f} s')\n",
    " \n",
    "  checkpoint = torch.load(name)\n",
    "  return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egh-IbW76AN4"
   },
   "outputs": [],
   "source": [
    "def test(model, criterion, test_dataset):\n",
    "  epoch_loss, epoch_acc, epoch_reg = [], [], []\n",
    "\n",
    "  model.eval()\n",
    "  test_dataset.repeat()\n",
    "\n",
    "  process_inputs = lambda x: torch.Tensor(x.numpy()).to(torch.int64)\n",
    "\n",
    "  t = time.time()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, data in enumerate(iter(test_dataset)):\n",
    "      inputs, labels = data['inputs'], data['targets']\n",
    "      inputs, labels = process_inputs(inputs), process_inputs(labels)\n",
    "      inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "      outputs, aux_losses = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      acc = accuracy(outputs, labels)\n",
    "      aux_losses = sum(aux_losses) if aux_losses else torch.Tensor([ 0.0 ]).cuda()\n",
    "\n",
    "      epoch_loss.append(loss.item())\n",
    "      epoch_acc.append(acc)\n",
    "      epoch_reg.append(aux_losses.item())\n",
    "\n",
    "  t = time.time() - t\n",
    "\n",
    "  epoch_loss, epoch_acc, epoch_reg = np.mean(epoch_loss), np.mean(epoch_acc), np.mean(epoch_reg)\n",
    "\n",
    "  print(f' - test_loss: {epoch_loss:.4f} - test_reg: {epoch_reg:.6f} - test_acc: {epoch_acc:.4f} - test_time: {t:.4f} s')\n",
    "  return epoch_loss, epoch_reg, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4d0DhgWDbm3",
    "outputId": "2c88a373-c3ae-412f-db44-258464d4d9f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model 21019658 params, new model 21806090 params, ratio 1.04\n",
      "Epoch 0\n",
      "[====================] 50/50: - running_loss: 2.2804 - running_reg: 0.000000 - running_acc: 0.1550 - lr: 0.00001 - epoch_loss: 2.2841 - epoch_reg: 0.000000 - epoch_acc: 0.1506 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 70.2115 s\n",
      "Epoch 1\n",
      "[====================] 50/50: - running_loss: 2.2541 - running_reg: 0.000000 - running_acc: 0.1945 - lr: 0.00002 - epoch_loss: 2.2577 - epoch_reg: 0.000000 - epoch_acc: 0.1881 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7987 s\n",
      "Epoch 2\n",
      "[====================] 50/50: - running_loss: 2.2859 - running_reg: 0.000000 - running_acc: 0.1382 - lr: 0.00002 - epoch_loss: 2.2827 - epoch_reg: 0.000000 - epoch_acc: 0.1488 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.9393 s\n",
      "Epoch 3\n",
      "[====================] 50/50: - running_loss: 2.2928 - running_reg: 0.000000 - running_acc: 0.1485 - lr: 0.00003 - epoch_loss: 2.2739 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7805 s\n",
      "Epoch 4\n",
      "[====================] 50/50: - running_loss: 2.2679 - running_reg: 0.000000 - running_acc: 0.1670 - lr: 0.00004 - epoch_loss: 2.2694 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 68.4609 s\n",
      "Epoch 5\n",
      "[====================] 50/50: - running_loss: 2.2644 - running_reg: 0.000000 - running_acc: 0.1372 - lr: 0.00005 - epoch_loss: 2.2793 - epoch_reg: 0.000000 - epoch_acc: 0.1363 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 68.1915 s\n",
      "Epoch 6\n",
      "[====================] 50/50: - running_loss: 2.2499 - running_reg: 0.000000 - running_acc: 0.1991 - lr: 0.00006 - epoch_loss: 2.2592 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8523 s\n",
      "Epoch 7\n",
      "[====================] 50/50: - running_loss: 2.2729 - running_reg: 0.000000 - running_acc: 0.1791 - lr: 0.00006 - epoch_loss: 2.2760 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7068 s\n",
      "Epoch 8\n",
      "[====================] 50/50: - running_loss: 2.2792 - running_reg: 0.000000 - running_acc: 0.1667 - lr: 0.00007 - epoch_loss: 2.2773 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7960 s\n",
      "Epoch 9\n",
      "[====================] 50/50: - running_loss: 2.2748 - running_reg: 0.000000 - running_acc: 0.1726 - lr: 0.00008 - epoch_loss: 2.2744 - epoch_reg: 0.000000 - epoch_acc: 0.1725 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7935 s\n",
      "Epoch 10\n",
      "[====================] 50/50: - running_loss: 2.2676 - running_reg: 0.000000 - running_acc: 0.1816 - lr: 0.00009 - epoch_loss: 2.2574 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8566 s\n",
      "Epoch 11\n",
      "[====================] 50/50: - running_loss: 2.2478 - running_reg: 0.000000 - running_acc: 0.1983 - lr: 0.00009 - epoch_loss: 2.2711 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8309 s\n",
      "Epoch 12\n",
      "[====================] 50/50: - running_loss: 2.2855 - running_reg: 0.000000 - running_acc: 0.1520 - lr: 0.00010 - epoch_loss: 2.2825 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8136 s\n",
      "Epoch 13\n",
      "[====================] 50/50: - running_loss: 2.2577 - running_reg: 0.000000 - running_acc: 0.1570 - lr: 0.00011 - epoch_loss: 2.2545 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8514 s\n",
      "Epoch 14\n",
      "[====================] 50/50: - running_loss: 2.2457 - running_reg: 0.000000 - running_acc: 0.1739 - lr: 0.00012 - epoch_loss: 2.2460 - epoch_reg: 0.000000 - epoch_acc: 0.1844 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8248 s\n",
      "Epoch 15\n",
      "[====================] 50/50: - running_loss: 2.2849 - running_reg: 0.000000 - running_acc: 0.1375 - lr: 0.00013 - epoch_loss: 2.2844 - epoch_reg: 0.000000 - epoch_acc: 0.1500 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7836 s\n",
      "Epoch 16\n",
      "[====================] 50/50: - running_loss: 2.2559 - running_reg: 0.000000 - running_acc: 0.1618 - lr: 0.00013 - epoch_loss: 2.2562 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8484 s\n",
      "Epoch 17\n",
      "[====================] 50/50: - running_loss: 2.2920 - running_reg: 0.000000 - running_acc: 0.1442 - lr: 0.00014 - epoch_loss: 2.2790 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8597 s\n",
      "Epoch 18\n",
      "[====================] 50/50: - running_loss: 2.2693 - running_reg: 0.000000 - running_acc: 0.1690 - lr: 0.00015 - epoch_loss: 2.2619 - epoch_reg: 0.000000 - epoch_acc: 0.1875 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8398 s\n",
      "Epoch 19\n",
      "[====================] 50/50: - running_loss: 2.2396 - running_reg: 0.000000 - running_acc: 0.1852 - lr: 0.00016 - epoch_loss: 2.2499 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8058 s\n",
      "Epoch 20\n",
      "[====================] 50/50: - running_loss: 2.2337 - running_reg: 0.000000 - running_acc: 0.1895 - lr: 0.00015 - epoch_loss: 2.2396 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7663 s\n",
      "Epoch 21\n",
      "[====================] 50/50: - running_loss: 2.2555 - running_reg: 0.000000 - running_acc: 0.1856 - lr: 0.00015 - epoch_loss: 2.2598 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8061 s\n",
      "Epoch 22\n",
      "[====================] 50/50: - running_loss: 2.2477 - running_reg: 0.000000 - running_acc: 0.1662 - lr: 0.00015 - epoch_loss: 2.2513 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8125 s\n",
      "Epoch 23\n",
      "[====================] 50/50: - running_loss: 2.2735 - running_reg: 0.000000 - running_acc: 0.1419 - lr: 0.00014 - epoch_loss: 2.2752 - epoch_reg: 0.000000 - epoch_acc: 0.1544 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7971 s\n",
      "Epoch 24\n",
      "[====================] 50/50: - running_loss: 2.2398 - running_reg: 0.000000 - running_acc: 0.1749 - lr: 0.00014 - epoch_loss: 2.2476 - epoch_reg: 0.000000 - epoch_acc: 0.1838 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7805 s\n",
      "Epoch 25\n",
      "[====================] 50/50: - running_loss: 2.2521 - running_reg: 0.000000 - running_acc: 0.1734 - lr: 0.00014 - epoch_loss: 2.2578 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8745 s\n",
      "Epoch 26\n",
      "[====================] 50/50: - running_loss: 2.2598 - running_reg: 0.000000 - running_acc: 0.1652 - lr: 0.00014 - epoch_loss: 2.2540 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8579 s\n",
      "Epoch 27\n",
      "[====================] 50/50: - running_loss: 2.2536 - running_reg: 0.000000 - running_acc: 0.1630 - lr: 0.00013 - epoch_loss: 2.2559 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8574 s\n",
      "Epoch 28\n",
      "[====================] 50/50: - running_loss: 2.2494 - running_reg: 0.000000 - running_acc: 0.1727 - lr: 0.00013 - epoch_loss: 2.2461 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.6908 s\n",
      "Epoch 29\n",
      "[====================] 50/50: - running_loss: 2.2623 - running_reg: 0.000000 - running_acc: 0.1681 - lr: 0.00013 - epoch_loss: 2.2586 - epoch_reg: 0.000000 - epoch_acc: 0.1619 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7299 s\n",
      "Epoch 30\n",
      "[====================] 50/50: - running_loss: 2.2611 - running_reg: 0.000000 - running_acc: 0.1540 - lr: 0.00013 - epoch_loss: 2.2506 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8025 s\n",
      "Epoch 31\n",
      "[====================] 50/50: - running_loss: 2.2642 - running_reg: 0.000000 - running_acc: 0.1677 - lr: 0.00013 - epoch_loss: 2.2633 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7755 s\n",
      "Epoch 32\n",
      "[====================] 50/50: - running_loss: 2.2614 - running_reg: 0.000000 - running_acc: 0.1696 - lr: 0.00012 - epoch_loss: 2.2657 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.6630 s\n",
      "Epoch 33\n",
      "[====================] 50/50: - running_loss: 2.2680 - running_reg: 0.000000 - running_acc: 0.1610 - lr: 0.00012 - epoch_loss: 2.2608 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7790 s\n",
      "Epoch 34\n",
      "[====================] 50/50: - running_loss: 2.2448 - running_reg: 0.000000 - running_acc: 0.1890 - lr: 0.00012 - epoch_loss: 2.2401 - epoch_reg: 0.000000 - epoch_acc: 0.1863 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8001 s\n",
      "Epoch 35\n",
      "[====================] 50/50: - running_loss: 2.2393 - running_reg: 0.000000 - running_acc: 0.1862 - lr: 0.00012 - epoch_loss: 2.2563 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8142 s\n",
      "Epoch 36\n",
      "[====================] 50/50: - running_loss: 2.2583 - running_reg: 0.000000 - running_acc: 0.1689 - lr: 0.00012 - epoch_loss: 2.2576 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8020 s\n",
      "Epoch 37\n",
      "[====================] 50/50: - running_loss: 2.2668 - running_reg: 0.000000 - running_acc: 0.1575 - lr: 0.00011 - epoch_loss: 2.2520 - epoch_reg: 0.000000 - epoch_acc: 0.1556 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8510 s\n",
      "Epoch 38\n",
      "[====================] 50/50: - running_loss: 2.2596 - running_reg: 0.000000 - running_acc: 0.1500 - lr: 0.00011 - epoch_loss: 2.2555 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7650 s\n",
      "Epoch 39\n",
      "[====================] 50/50: - running_loss: 2.2624 - running_reg: 0.000000 - running_acc: 0.1585 - lr: 0.00011 - epoch_loss: 2.2540 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7925 s\n",
      "Epoch 40\n",
      "[====================] 50/50: - running_loss: 2.2544 - running_reg: 0.000000 - running_acc: 0.1752 - lr: 0.00011 - epoch_loss: 2.2442 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8084 s\n",
      "Epoch 41\n",
      "[====================] 50/50: - running_loss: 2.2718 - running_reg: 0.000000 - running_acc: 0.1661 - lr: 0.00011 - epoch_loss: 2.2562 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8156 s\n",
      "Epoch 42\n",
      "[====================] 50/50: - running_loss: 2.2485 - running_reg: 0.000000 - running_acc: 0.1655 - lr: 0.00011 - epoch_loss: 2.2476 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.9031 s\n",
      "Epoch 43\n",
      "[====================] 50/50: - running_loss: 2.2688 - running_reg: 0.000000 - running_acc: 0.1483 - lr: 0.00011 - epoch_loss: 2.2673 - epoch_reg: 0.000000 - epoch_acc: 0.1581 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.9589 s\n",
      "Epoch 44\n",
      "[====================] 50/50: - running_loss: 2.2412 - running_reg: 0.000000 - running_acc: 0.1848 - lr: 0.00011 - epoch_loss: 2.2554 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8707 s\n",
      "Epoch 45\n",
      "[====================] 50/50: - running_loss: 2.2673 - running_reg: 0.000000 - running_acc: 0.1562 - lr: 0.00010 - epoch_loss: 2.2477 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8940 s\n",
      "Epoch 46\n",
      "[====================] 50/50: - running_loss: 2.2311 - running_reg: 0.000000 - running_acc: 0.1883 - lr: 0.00010 - epoch_loss: 2.2437 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8593 s\n",
      "Epoch 47\n",
      "[====================] 50/50: - running_loss: 2.2652 - running_reg: 0.000000 - running_acc: 0.1617 - lr: 0.00010 - epoch_loss: 2.2520 - epoch_reg: 0.000000 - epoch_acc: 0.1725 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8198 s\n",
      "Epoch 48\n",
      "[====================] 50/50: - running_loss: 2.2495 - running_reg: 0.000000 - running_acc: 0.1600 - lr: 0.00010 - epoch_loss: 2.2509 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8705 s\n",
      "Epoch 49\n",
      "[====================] 50/50: - running_loss: 2.2410 - running_reg: 0.000000 - running_acc: 0.1905 - lr: 0.00010 - epoch_loss: 2.2490 - epoch_reg: 0.000000 - epoch_acc: 0.1875 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8102 s\n",
      "Epoch 50\n",
      "[====================] 50/50: - running_loss: 2.2380 - running_reg: 0.000000 - running_acc: 0.1815 - lr: 0.00010 - epoch_loss: 2.2403 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8632 s\n",
      "Epoch 51\n",
      "[====================] 50/50: - running_loss: 2.2567 - running_reg: 0.000000 - running_acc: 0.1425 - lr: 0.00010 - epoch_loss: 2.2554 - epoch_reg: 0.000000 - epoch_acc: 0.1475 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.9110 s\n",
      "Epoch 52\n",
      "[====================] 50/50: - running_loss: 2.2499 - running_reg: 0.000000 - running_acc: 0.1790 - lr: 0.00010 - epoch_loss: 2.2503 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8509 s\n",
      "Epoch 53\n",
      "[====================] 50/50: - running_loss: 2.2485 - running_reg: 0.000000 - running_acc: 0.1861 - lr: 0.00010 - epoch_loss: 2.2572 - epoch_reg: 0.000000 - epoch_acc: 0.1737 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.8184 s\n",
      "Epoch 54\n",
      "[====================] 50/50: - running_loss: 2.2641 - running_reg: 0.000000 - running_acc: 0.1747 - lr: 0.00010 - epoch_loss: 2.2568 - epoch_reg: 0.000000 - epoch_acc: 0.1737 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7492 s\n",
      "Epoch 55\n",
      "[====================] 50/50: - running_loss: 2.2659 - running_reg: 0.000000 - running_acc: 0.1546 - lr: 0.00009 - epoch_loss: 2.2566 - epoch_reg: 0.000000 - epoch_acc: 0.1525 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7918 s\n",
      "Epoch 56\n",
      "[====================] 50/50: - running_loss: 2.2785 - running_reg: 0.000000 - running_acc: 0.1526 - lr: 0.00009 - epoch_loss: 2.2672 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7676 s\n",
      "Epoch 57\n",
      "[====================] 50/50: - running_loss: 2.2728 - running_reg: 0.000000 - running_acc: 0.1425 - lr: 0.00009 - epoch_loss: 2.2609 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7873 s\n",
      "Epoch 58\n",
      "[====================] 50/50: - running_loss: 2.2455 - running_reg: 0.000000 - running_acc: 0.1600 - lr: 0.00009 - epoch_loss: 2.2475 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 67.7499 s\n",
      "Epoch 59\n",
      "[====================] 50/50: - running_loss: 2.2590 - running_reg: 0.000000 - running_acc: 0.1599 - lr: 0.00009 - epoch_loss: 2.2596 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 66.3582 s\n",
      "Epoch 60\n",
      "[====================] 50/50: - running_loss: 2.2662 - running_reg: 0.000000 - running_acc: 0.1563 - lr: 0.00009 - epoch_loss: 2.2494 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2610 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 99.8973 s\n",
      "Epoch 61\n",
      "[====================] 50/50: - running_loss: 2.2589 - running_reg: 0.000000 - running_acc: 0.1624 - lr: 0.00009 - epoch_loss: 2.2565 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2582 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 99.1792 s\n",
      "Epoch 62\n",
      "[====================] 50/50: - running_loss: 2.2654 - running_reg: 0.000000 - running_acc: 0.1642 - lr: 0.00009 - epoch_loss: 2.2653 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2555 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.9403 s\n",
      "Epoch 63\n",
      "[====================] 50/50: - running_loss: 2.2576 - running_reg: 0.000000 - running_acc: 0.1569 - lr: 0.00009 - epoch_loss: 2.2513 - epoch_reg: 0.000000 - epoch_acc: 0.1587 - valid_loss: 2.2587 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2036 s\n",
      "Epoch 64\n",
      "[====================] 50/50: - running_loss: 2.2367 - running_reg: 0.000000 - running_acc: 0.1996 - lr: 0.00009 - epoch_loss: 2.2417 - epoch_reg: 0.000000 - epoch_acc: 0.1906 - valid_loss: 2.2625 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.4578 s\n",
      "Epoch 65\n",
      "[====================] 50/50: - running_loss: 2.2638 - running_reg: 0.000000 - running_acc: 0.1630 - lr: 0.00009 - epoch_loss: 2.2619 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2617 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2868 s\n",
      "Epoch 66\n",
      "[====================] 50/50: - running_loss: 2.2594 - running_reg: 0.000000 - running_acc: 0.1642 - lr: 0.00009 - epoch_loss: 2.2553 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2574 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2771 s\n",
      "Epoch 67\n",
      "[====================] 50/50: - running_loss: 2.2589 - running_reg: 0.000000 - running_acc: 0.1610 - lr: 0.00009 - epoch_loss: 2.2510 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 2.2596 - valid_reg: 0.000000 - valid_acc: 0.1625 - epoch_time: 97.2391 s\n",
      "Epoch 68\n",
      "[====================] 50/50: - running_loss: 2.2488 - running_reg: 0.000000 - running_acc: 0.1882 - lr: 0.00009 - epoch_loss: 2.2603 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 2.2604 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2430 s\n",
      "Epoch 69\n",
      "[====================] 50/50: - running_loss: 2.2749 - running_reg: 0.000000 - running_acc: 0.1500 - lr: 0.00008 - epoch_loss: 2.2567 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 2.2635 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.4514 s\n",
      "Epoch 70\n",
      "[====================] 50/50: - running_loss: 2.2615 - running_reg: 0.000000 - running_acc: 0.1649 - lr: 0.00008 - epoch_loss: 2.2641 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2584 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2190 s\n",
      "Epoch 71\n",
      "[====================] 50/50: - running_loss: 2.2421 - running_reg: 0.000000 - running_acc: 0.1779 - lr: 0.00008 - epoch_loss: 2.2609 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2595 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2331 s\n",
      "Epoch 72\n",
      "[====================] 50/50: - running_loss: 2.2680 - running_reg: 0.000000 - running_acc: 0.1586 - lr: 0.00008 - epoch_loss: 2.2559 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 2.2569 - valid_reg: 0.000000 - valid_acc: 0.1710 - epoch_time: 97.2351 s\n",
      "Epoch 73\n",
      "[====================] 50/50: - running_loss: 2.2613 - running_reg: 0.000000 - running_acc: 0.1524 - lr: 0.00008 - epoch_loss: 2.2654 - epoch_reg: 0.000000 - epoch_acc: 0.1519 - valid_loss: 2.2576 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.3007 s\n",
      "Epoch 74\n",
      "[====================] 50/50: - running_loss: 2.2588 - running_reg: 0.000000 - running_acc: 0.1475 - lr: 0.00008 - epoch_loss: 2.2581 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 2.2561 - valid_reg: 0.000000 - valid_acc: 0.1780 - epoch_time: 97.8428 s\n",
      "Epoch 75\n",
      "[====================] 50/50: - running_loss: 2.2364 - running_reg: 0.000000 - running_acc: 0.1824 - lr: 0.00008 - epoch_loss: 2.2377 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 2.2628 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.3083 s\n",
      "Epoch 76\n",
      "[====================] 50/50: - running_loss: 2.2428 - running_reg: 0.000000 - running_acc: 0.1858 - lr: 0.00008 - epoch_loss: 2.2400 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 2.2589 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2349 s\n",
      "Epoch 77\n",
      "[====================] 50/50: - running_loss: 2.2401 - running_reg: 0.000000 - running_acc: 0.1664 - lr: 0.00008 - epoch_loss: 2.2533 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 2.2645 - valid_reg: 0.000000 - valid_acc: 0.1705 - epoch_time: 97.1461 s\n",
      "Epoch 78\n",
      "[====================] 50/50: - running_loss: 2.2622 - running_reg: 0.000000 - running_acc: 0.1648 - lr: 0.00008 - epoch_loss: 2.2528 - epoch_reg: 0.000000 - epoch_acc: 0.1725 - valid_loss: 2.2580 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2010 s\n",
      "Epoch 79\n",
      "[====================] 50/50: - running_loss: 2.2517 - running_reg: 0.000000 - running_acc: 0.1572 - lr: 0.00008 - epoch_loss: 2.2571 - epoch_reg: 0.000000 - epoch_acc: 0.1575 - valid_loss: 2.2565 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2037 s\n",
      "Epoch 80\n",
      "[====================] 50/50: - running_loss: 2.2441 - running_reg: 0.000000 - running_acc: 0.1701 - lr: 0.00008 - epoch_loss: 2.2480 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2621 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2087 s\n",
      "Epoch 81\n",
      "[====================] 50/50: - running_loss: 2.2452 - running_reg: 0.000000 - running_acc: 0.1800 - lr: 0.00008 - epoch_loss: 2.2365 - epoch_reg: 0.000000 - epoch_acc: 0.1894 - valid_loss: 2.2589 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.1811 s\n",
      "Epoch 82\n",
      "[====================] 50/50: - running_loss: 2.2442 - running_reg: 0.000000 - running_acc: 0.1627 - lr: 0.00008 - epoch_loss: 2.2454 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2598 - valid_reg: 0.000000 - valid_acc: 0.1745 - epoch_time: 97.2297 s\n",
      "Epoch 83\n",
      "[====================] 50/50: - running_loss: 2.2424 - running_reg: 0.000000 - running_acc: 0.1579 - lr: 0.00008 - epoch_loss: 2.2490 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 2.2598 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.1671 s\n",
      "Epoch 84\n",
      "[====================] 50/50: - running_loss: 2.2625 - running_reg: 0.000000 - running_acc: 0.1544 - lr: 0.00008 - epoch_loss: 2.2531 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2629 - valid_reg: 0.000000 - valid_acc: 0.1565 - epoch_time: 97.1956 s\n",
      "Epoch 85\n",
      "[====================] 50/50: - running_loss: 2.2531 - running_reg: 0.000000 - running_acc: 0.1771 - lr: 0.00008 - epoch_loss: 2.2568 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 2.2569 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.3155 s\n",
      "Epoch 86\n",
      "[====================] 50/50: - running_loss: 2.2364 - running_reg: 0.000000 - running_acc: 0.1719 - lr: 0.00008 - epoch_loss: 2.2503 - epoch_reg: 0.000000 - epoch_acc: 0.1525 - valid_loss: 2.2622 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.3952 s\n",
      "Epoch 87\n",
      "[====================] 50/50: - running_loss: 2.2502 - running_reg: 0.000000 - running_acc: 0.1749 - lr: 0.00008 - epoch_loss: 2.2537 - epoch_reg: 0.000000 - epoch_acc: 0.1775 - valid_loss: 2.2558 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.3975 s\n",
      "Epoch 88\n",
      "[====================] 50/50: - running_loss: 2.2513 - running_reg: 0.000000 - running_acc: 0.1815 - lr: 0.00007 - epoch_loss: 2.2510 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2576 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.3953 s\n",
      "Epoch 89\n",
      "[====================] 50/50: - running_loss: 2.2447 - running_reg: 0.000000 - running_acc: 0.1794 - lr: 0.00007 - epoch_loss: 2.2476 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 2.2561 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.4447 s\n",
      "Epoch 90\n",
      "[====================] 50/50: - running_loss: 2.2580 - running_reg: 0.000000 - running_acc: 0.1500 - lr: 0.00007 - epoch_loss: 2.2559 - epoch_reg: 0.000000 - epoch_acc: 0.1575 - valid_loss: 2.2582 - valid_reg: 0.000000 - valid_acc: 0.1610 - epoch_time: 97.5909 s\n",
      "Epoch 91\n",
      "[====================] 50/50: - running_loss: 2.2470 - running_reg: 0.000000 - running_acc: 0.1663 - lr: 0.00007 - epoch_loss: 2.2465 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 2.2559 - valid_reg: 0.000000 - valid_acc: 0.1695 - epoch_time: 97.1977 s\n",
      "Epoch 92\n",
      "[====================] 50/50: - running_loss: 2.2632 - running_reg: 0.000000 - running_acc: 0.1762 - lr: 0.00007 - epoch_loss: 2.2472 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 2.2557 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.1709 s\n",
      "Epoch 93\n",
      "[====================] 50/50: - running_loss: 2.2721 - running_reg: 0.000000 - running_acc: 0.1663 - lr: 0.00007 - epoch_loss: 2.2646 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 2.2601 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.1624 s\n",
      "Epoch 94\n",
      "[====================] 50/50: - running_loss: 2.2535 - running_reg: 0.000000 - running_acc: 0.1723 - lr: 0.00007 - epoch_loss: 2.2518 - epoch_reg: 0.000000 - epoch_acc: 0.1775 - valid_loss: 2.2607 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.0894 s\n",
      "Epoch 95\n",
      "[====================] 50/50: - running_loss: 2.2331 - running_reg: 0.000000 - running_acc: 0.1811 - lr: 0.00007 - epoch_loss: 2.2493 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2564 - valid_reg: 0.000000 - valid_acc: 0.1680 - epoch_time: 97.1516 s\n",
      "Epoch 96\n",
      "[====================] 50/50: - running_loss: 2.2415 - running_reg: 0.000000 - running_acc: 0.1895 - lr: 0.00007 - epoch_loss: 2.2446 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 2.2602 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.1231 s\n",
      "Epoch 97\n",
      "[====================] 50/50: - running_loss: 2.2672 - running_reg: 0.000000 - running_acc: 0.1689 - lr: 0.00007 - epoch_loss: 2.2633 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2608 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2644 s\n",
      "Epoch 98\n",
      "[====================] 50/50: - running_loss: 2.2667 - running_reg: 0.000000 - running_acc: 0.1582 - lr: 0.00007 - epoch_loss: 2.2574 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2570 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.1598 s\n",
      "Epoch 99\n",
      "[====================] 50/50: - running_loss: 2.2439 - running_reg: 0.000000 - running_acc: 0.1808 - lr: 0.00007 - epoch_loss: 2.2481 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2642 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2244 s\n",
      "Epoch 100\n",
      "[====================] 50/50: - running_loss: 2.2615 - running_reg: 0.000000 - running_acc: 0.1663 - lr: 0.00007 - epoch_loss: 2.2422 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2575 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.1876 s\n",
      "Epoch 101\n",
      "[====================] 50/50: - running_loss: 2.2645 - running_reg: 0.000000 - running_acc: 0.1510 - lr: 0.00007 - epoch_loss: 2.2511 - epoch_reg: 0.000000 - epoch_acc: 0.1575 - valid_loss: 2.2575 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2279 s\n",
      "Epoch 102\n",
      "[====================] 50/50: - running_loss: 2.2072 - running_reg: 0.000000 - running_acc: 0.1764 - lr: 0.00007 - epoch_loss: 2.2262 - epoch_reg: 0.000000 - epoch_acc: 0.1825 - valid_loss: 2.2639 - valid_reg: 0.000000 - valid_acc: 0.1785 - epoch_time: 97.9324 s\n",
      "Epoch 103\n",
      "[====================] 50/50: - running_loss: 2.2649 - running_reg: 0.000000 - running_acc: 0.1595 - lr: 0.00007 - epoch_loss: 2.2566 - epoch_reg: 0.000000 - epoch_acc: 0.1562 - valid_loss: 2.2604 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.1916 s\n",
      "Epoch 104\n",
      "[====================] 50/50: - running_loss: 2.2557 - running_reg: 0.000000 - running_acc: 0.1672 - lr: 0.00007 - epoch_loss: 2.2536 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2588 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2304 s\n",
      "Epoch 105\n",
      "[====================] 50/50: - running_loss: 2.2522 - running_reg: 0.000000 - running_acc: 0.1607 - lr: 0.00007 - epoch_loss: 2.2485 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2580 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.4048 s\n",
      "Epoch 106\n",
      "[====================] 50/50: - running_loss: 2.2552 - running_reg: 0.000000 - running_acc: 0.1653 - lr: 0.00007 - epoch_loss: 2.2603 - epoch_reg: 0.000000 - epoch_acc: 0.1581 - valid_loss: 2.2559 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.3004 s\n",
      "Epoch 107\n",
      "[====================] 50/50: - running_loss: 2.2478 - running_reg: 0.000000 - running_acc: 0.1735 - lr: 0.00007 - epoch_loss: 2.2523 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2570 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2093 s\n",
      "Epoch 108\n",
      "[====================] 50/50: - running_loss: 2.2549 - running_reg: 0.000000 - running_acc: 0.1719 - lr: 0.00007 - epoch_loss: 2.2482 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2621 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2145 s\n",
      "Epoch 109\n",
      "[====================] 50/50: - running_loss: 2.2711 - running_reg: 0.000000 - running_acc: 0.1367 - lr: 0.00007 - epoch_loss: 2.2642 - epoch_reg: 0.000000 - epoch_acc: 0.1450 - valid_loss: 2.2555 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.1751 s\n",
      "Epoch 110\n",
      "[====================] 50/50: - running_loss: 2.2548 - running_reg: 0.000000 - running_acc: 0.1697 - lr: 0.00007 - epoch_loss: 2.2519 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 2.2548 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2791 s\n",
      "Epoch 111\n",
      "[====================] 50/50: - running_loss: 2.2565 - running_reg: 0.000000 - running_acc: 0.1751 - lr: 0.00007 - epoch_loss: 2.2485 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 2.2578 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.3106 s\n",
      "Epoch 112\n",
      "[====================] 50/50: - running_loss: 2.2465 - running_reg: 0.000000 - running_acc: 0.1748 - lr: 0.00007 - epoch_loss: 2.2446 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2576 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2284 s\n",
      "Epoch 113\n",
      "[====================] 50/50: - running_loss: 2.2484 - running_reg: 0.000000 - running_acc: 0.1738 - lr: 0.00007 - epoch_loss: 2.2461 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2571 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.1202 s\n",
      "Epoch 114\n",
      "[====================] 50/50: - running_loss: 2.2129 - running_reg: 0.000000 - running_acc: 0.1748 - lr: 0.00007 - epoch_loss: 2.2430 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 2.2635 - valid_reg: 0.000000 - valid_acc: 0.1785 - epoch_time: 97.1770 s\n",
      "Epoch 115\n",
      "[====================] 50/50: - running_loss: 2.2396 - running_reg: 0.000000 - running_acc: 0.1797 - lr: 0.00007 - epoch_loss: 2.2482 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2559 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2591 s\n",
      "Epoch 116\n",
      "[====================] 50/50: - running_loss: 2.2712 - running_reg: 0.000000 - running_acc: 0.1489 - lr: 0.00007 - epoch_loss: 2.2747 - epoch_reg: 0.000000 - epoch_acc: 0.1494 - valid_loss: 2.2553 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.1821 s\n",
      "Epoch 117\n",
      "[====================] 50/50: - running_loss: 2.2777 - running_reg: 0.000000 - running_acc: 0.1643 - lr: 0.00007 - epoch_loss: 2.2749 - epoch_reg: 0.000000 - epoch_acc: 0.1519 - valid_loss: 2.2579 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2532 s\n",
      "Epoch 118\n",
      "[====================] 50/50: - running_loss: 2.2669 - running_reg: 0.000000 - running_acc: 0.1685 - lr: 0.00006 - epoch_loss: 2.2472 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2607 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.1934 s\n",
      "Epoch 119\n",
      "[====================] 50/50: - running_loss: 2.2499 - running_reg: 0.000000 - running_acc: 0.1766 - lr: 0.00006 - epoch_loss: 2.2548 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2558 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 95.8737 s\n",
      "Epoch 120\n",
      "[====================] 50/50: - running_loss: 2.2466 - running_reg: 0.000000 - running_acc: 0.1743 - lr: 0.00006 - epoch_loss: 2.2555 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 2.2611 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 99.1325 s\n",
      "Epoch 121\n",
      "[====================] 50/50: - running_loss: 2.2611 - running_reg: 0.000000 - running_acc: 0.1823 - lr: 0.00006 - epoch_loss: 2.2558 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 2.2610 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2079 s\n",
      "Epoch 122\n",
      "[====================] 50/50: - running_loss: 2.2673 - running_reg: 0.000000 - running_acc: 0.1585 - lr: 0.00006 - epoch_loss: 2.2635 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 2.2573 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2195 s\n",
      "Epoch 123\n",
      "[====================] 50/50: - running_loss: 2.2528 - running_reg: 0.000000 - running_acc: 0.1739 - lr: 0.00006 - epoch_loss: 2.2497 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2579 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.1924 s\n",
      "Epoch 124\n",
      "[====================] 50/50: - running_loss: 2.2689 - running_reg: 0.000000 - running_acc: 0.1541 - lr: 0.00006 - epoch_loss: 2.2517 - epoch_reg: 0.000000 - epoch_acc: 0.1725 - valid_loss: 2.2579 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2600 s\n",
      "Epoch 125\n",
      "[====================] 50/50: - running_loss: 2.2621 - running_reg: 0.000000 - running_acc: 0.1614 - lr: 0.00006 - epoch_loss: 2.2573 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 2.2557 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.3624 s\n",
      "Epoch 126\n",
      "[====================] 50/50: - running_loss: 2.2453 - running_reg: 0.000000 - running_acc: 0.1760 - lr: 0.00006 - epoch_loss: 2.2444 - epoch_reg: 0.000000 - epoch_acc: 0.1737 - valid_loss: 2.2579 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2047 s\n",
      "Epoch 127\n",
      "[====================] 50/50: - running_loss: 2.2443 - running_reg: 0.000000 - running_acc: 0.1549 - lr: 0.00006 - epoch_loss: 2.2467 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 2.2602 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.1544 s\n",
      "Epoch 128\n",
      "[====================] 50/50: - running_loss: 2.2634 - running_reg: 0.000000 - running_acc: 0.1632 - lr: 0.00006 - epoch_loss: 2.2550 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2580 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2460 s\n",
      "Epoch 129\n",
      "[====================] 50/50: - running_loss: 2.2521 - running_reg: 0.000000 - running_acc: 0.1796 - lr: 0.00006 - epoch_loss: 2.2510 - epoch_reg: 0.000000 - epoch_acc: 0.1737 - valid_loss: 2.2581 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2635 s\n",
      "Epoch 130\n",
      "[====================] 50/50: - running_loss: 2.2515 - running_reg: 0.000000 - running_acc: 0.1740 - lr: 0.00006 - epoch_loss: 2.2585 - epoch_reg: 0.000000 - epoch_acc: 0.1688 - valid_loss: 2.2600 - valid_reg: 0.000000 - valid_acc: 0.1620 - epoch_time: 97.2871 s\n",
      "Epoch 131\n",
      "[====================] 50/50: - running_loss: 2.2692 - running_reg: 0.000000 - running_acc: 0.1551 - lr: 0.00006 - epoch_loss: 2.2501 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 2.2568 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2361 s\n",
      "Epoch 132\n",
      "[====================] 50/50: - running_loss: 2.2739 - running_reg: 0.000000 - running_acc: 0.1503 - lr: 0.00006 - epoch_loss: 2.2570 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 2.2615 - valid_reg: 0.000000 - valid_acc: 0.1520 - epoch_time: 97.1571 s\n",
      "Epoch 133\n",
      "[====================] 50/50: - running_loss: 2.2562 - running_reg: 0.000000 - running_acc: 0.1723 - lr: 0.00006 - epoch_loss: 2.2598 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 2.2578 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.5494 s\n",
      "Epoch 134\n",
      "[====================] 50/50: - running_loss: 2.2520 - running_reg: 0.000000 - running_acc: 0.1538 - lr: 0.00006 - epoch_loss: 2.2463 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 2.2584 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2526 s\n",
      "Epoch 135\n",
      "[====================] 50/50: - running_loss: 2.2589 - running_reg: 0.000000 - running_acc: 0.1718 - lr: 0.00006 - epoch_loss: 2.2522 - epoch_reg: 0.000000 - epoch_acc: 0.1619 - valid_loss: 2.2596 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.3016 s\n",
      "Epoch 136\n",
      "[====================] 50/50: - running_loss: 2.2593 - running_reg: 0.000000 - running_acc: 0.1641 - lr: 0.00006 - epoch_loss: 2.2539 - epoch_reg: 0.000000 - epoch_acc: 0.1688 - valid_loss: 2.2632 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2541 s\n",
      "Epoch 137\n",
      "[====================] 50/50: - running_loss: 2.2356 - running_reg: 0.000000 - running_acc: 0.1916 - lr: 0.00006 - epoch_loss: 2.2415 - epoch_reg: 0.000000 - epoch_acc: 0.1775 - valid_loss: 2.2564 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.1540 s\n",
      "Epoch 138\n",
      "[====================] 50/50: - running_loss: 2.2695 - running_reg: 0.000000 - running_acc: 0.1577 - lr: 0.00006 - epoch_loss: 2.2623 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2621 - valid_reg: 0.000000 - valid_acc: 0.1745 - epoch_time: 97.2411 s\n",
      "Epoch 139\n",
      "[====================] 50/50: - running_loss: 2.2405 - running_reg: 0.000000 - running_acc: 0.1772 - lr: 0.00006 - epoch_loss: 2.2520 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 2.2593 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2424 s\n",
      "Epoch 140\n",
      "[====================] 50/50: - running_loss: 2.2787 - running_reg: 0.000000 - running_acc: 0.1421 - lr: 0.00006 - epoch_loss: 2.2648 - epoch_reg: 0.000000 - epoch_acc: 0.1519 - valid_loss: 2.2604 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2532 s\n",
      "Epoch 141\n",
      "[====================] 50/50: - running_loss: 2.2482 - running_reg: 0.000000 - running_acc: 0.1816 - lr: 0.00006 - epoch_loss: 2.2577 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2553 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.1946 s\n",
      "Epoch 142\n",
      "[====================] 50/50: - running_loss: 2.2569 - running_reg: 0.000000 - running_acc: 0.1621 - lr: 0.00006 - epoch_loss: 2.2571 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2577 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2414 s\n",
      "Epoch 143\n",
      "[====================] 50/50: - running_loss: 2.2284 - running_reg: 0.000000 - running_acc: 0.1987 - lr: 0.00006 - epoch_loss: 2.2474 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2609 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.3554 s\n",
      "Epoch 144\n",
      "[====================] 50/50: - running_loss: 2.2410 - running_reg: 0.000000 - running_acc: 0.2021 - lr: 0.00006 - epoch_loss: 2.2515 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2582 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2567 s\n",
      "Epoch 145\n",
      "[====================] 50/50: - running_loss: 2.2594 - running_reg: 0.000000 - running_acc: 0.1427 - lr: 0.00006 - epoch_loss: 2.2607 - epoch_reg: 0.000000 - epoch_acc: 0.1519 - valid_loss: 2.2571 - valid_reg: 0.000000 - valid_acc: 0.1750 - epoch_time: 97.2207 s\n",
      "Epoch 146\n",
      "[====================] 50/50: - running_loss: 2.2360 - running_reg: 0.000000 - running_acc: 0.1941 - lr: 0.00006 - epoch_loss: 2.2465 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2573 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.2234 s\n",
      "Epoch 147\n",
      "[====================] 50/50: - running_loss: 2.2320 - running_reg: 0.000000 - running_acc: 0.1759 - lr: 0.00006 - epoch_loss: 2.2476 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2578 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 97.3931 s\n",
      "Epoch 148\n",
      "[====================] 50/50: - running_loss: 2.2326 - running_reg: 0.000000 - running_acc: 0.1832 - lr: 0.00006 - epoch_loss: 2.2289 - epoch_reg: 0.000000 - epoch_acc: 0.1912 - valid_loss: 2.2625 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.4630 s\n",
      "Epoch 149\n",
      "[====================] 50/50: - running_loss: 2.2432 - running_reg: 0.000000 - running_acc: 0.1822 - lr: 0.00006 - epoch_loss: 2.2454 - epoch_reg: 0.000000 - epoch_acc: 0.1856 - valid_loss: 2.2615 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 97.2717 s\n",
      " - test_loss: 2.2445 - test_reg: 0.000000 - test_acc: 0.1845 - test_time: 29.6311 s\n",
      "\n",
      "Total accuracy: 0.1845\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = [  ]\n",
    "\n",
    "for i in range(1): ####!!!!!!!!!!!!!!\n",
    "  path = 'model_to_test_' + str(i) + '.b'\n",
    "\n",
    "  model, criterion, optimizer, schedule_func, scheduler = training_setup()\n",
    "\n",
    "  checkpoint = train_model(model, path, train_dataset, valid_dataset, optimizer, criterion, scheduler, accumulation_steps, 150, 50, skip_eval=60)\n",
    "  \n",
    "  if checkpoint is None:\n",
    "    break\n",
    "  \n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  \n",
    "  _, _, acc = test(model, criterion, test_dataset)\n",
    "  test_accuracy.append(acc)\n",
    "\n",
    "test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "print(f'\\nTotal accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLAnGphxnEf7"
   },
   "outputs": [],
   "source": [
    "2xGLU:  0.1845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oJI-xf2xaLqc"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TEmbedding(nn.Module):\n",
    "  def __init__(self, num_embeddings, hidden_dim, seq_length=1024, padding_idx=0):\n",
    "    super(TEmbedding, self).__init__()\n",
    "    \n",
    "    self.num_embeddings = num_embeddings\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.seq_length = seq_length\n",
    "    self.padding_idx = padding_idx\n",
    "\n",
    "    self.embedding = nn.Embedding(num_embeddings, hidden_dim, padding_idx)\n",
    "    self.pos_embeds  = nn.Parameter(torch.zeros(1, self.seq_length, self.hidden_dim))\n",
    "\n",
    "    self.cls = nn.Parameter(torch.zeros(1, 1, self.hidden_dim)) #!!!!!!! INIT WITH ANOTHER VALUE IF REQUIRED\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, seq_len = input.shape\n",
    "    \n",
    "    embed = self.embedding(input)\n",
    "    embed = embed + self.pos_embeds\n",
    "    embed = torch.cat([ self.cls.expand(batch_size, 1, -1), embed ], axis=1)\n",
    "\n",
    "    return embed\n",
    "    \n",
    "class TAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(TAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   =qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "    \n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "\n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "    q = torch.mul(q, 1. / torch.sqrt(torch.tensor(self.qkv_dim)))\n",
    "\n",
    "    qk = torch.matmul(q, k.transpose(-1, -2))\n",
    "    qk = nn.Softmax(dim=-1)(qk)\n",
    "\n",
    "    def assertion_function(tsr):\n",
    "      tsr = torch.sum(tsr, axis=-1)\n",
    "      tsr = tsr - torch.ones_like(tsr)\n",
    "      return torch.max(torch.abs(tsr)) < 1e-5\n",
    "\n",
    "    assert assertion_function(qk)\n",
    "\n",
    "    qk = self.dropout(qk) #Like in TF implementation; could be done before Softmax by random -inf addition\n",
    "\n",
    "    out = torch.matmul(qk, v)\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "\n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class HWLinear(nn.Module):\n",
    "  def __init__(self, num_heads, input_dim, output_dim, use_bias):\n",
    "    super(HWLinear, self).__init__()\n",
    "    \n",
    "    self.use_bias = use_bias\n",
    "    if use_bias:\n",
    "      self.bias   = nn.Parameter(torch.zeros( (1, num_heads, 1, output_dim)))\n",
    "\n",
    "    self.weight = nn.Parameter(torch.empty( (num_heads, input_dim, output_dim)))\n",
    "\n",
    "    def he_init(m):\n",
    "      s =  np.sqrt( 2. / input_dim )\n",
    "      m.data.normal_(0, s)\n",
    "\n",
    "    he_init(self.weight)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.matmul(x, self.weight)\n",
    "    if self.use_bias:\n",
    "      x += self.bias\n",
    "    return x\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "  def __init__(self, lambda_, objects=None):\n",
    "      super(Lambda, self).__init__()\n",
    "      self.lambda_ = lambda_\n",
    "      self.objects = objects\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.objects is not None:\n",
    "      return self.lambda_(self.objects, x)\n",
    "    return self.lambda_(x)\n",
    "\n",
    "class LKAAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(LKAAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   = qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "\n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "    \n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    #self.lka = nn.Sequential(\n",
    "    #  nn.Linear(self.head_dim, self.head_dim), nn.GELU(),\n",
    "    #  nn.Linear(self.head_dim, self.head_dim), nn.Softplus(beta=2.5),\n",
    "    #)\n",
    "\n",
    "    #256, 4, 16, 1024\n",
    "    #256, 64, 1, 1024\n",
    "    class AMGOLU(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, gate_rank, dropout_rate, gate_nonlinearity, kernel_nonlinearity, use_bias=False):\n",
    "        super(AMGOLU, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads= num_heads\n",
    "        \n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "\n",
    "        self.gate_weight_a = HWLinear(num_heads, self.head_dim, gate_rank, use_bias)\n",
    "        self.gate_weight_b = HWLinear(num_heads, gate_rank, self.head_dim, use_bias)\n",
    "\n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        self.gate_nonlinearity   = gate_nonlinearity\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        forward_info = self.orth_weight(x)\n",
    "        forward_info = self.kernel_nonlinearity(forward_info)\n",
    "\n",
    "        gate_info = self.gate_weight_a(x)\n",
    "        gate_info = self.gate_weight_b(gate_info)\n",
    "        gate_info = self.gate_nonlinearity(gate_info)\n",
    "\n",
    "        x = forward_info * gate_info\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "    class GatedOrthoKernel(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate=0.1, gate_nonlinearity=nn.Sigmoid(), kernel_nonlinearity=nn.Identity(), use_bias=False):\n",
    "        super(GatedOrthoKernel, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "        self.gate_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "\n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        self.gate_nonlinearity   = gate_nonlinearity\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.kernel_nonlinearity(self.orth_weight(x)) * self.gate_nonlinearity(self.gate_weight(x))\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "    class LinearKernel(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate=0.1, kernel_nonlinearity=nn.Identity(), use_bias=False):\n",
    "        super(LinearKernel, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "        \n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.kernel_nonlinearity(self.orth_weight(x))\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "\n",
    "    class HeadWiseFF(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate, nonlinearity=nn.Identity(), use_bias=False, residual=False):\n",
    "        super(HeadWiseFF, self).__init__()\n",
    "        \n",
    "        head_dim = qkv_dim // num_heads\n",
    "\n",
    "        self.bias   = nn.Parameter(torch.empty( (1, num_heads, 1, head_dim)))\n",
    "        self.dropout= nn.Dropout(dropout_rate)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty( (num_heads, head_dim, head_dim)))\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "        #Orthogonal initialization\n",
    "        #Workaround with torch.stack, since Torch initializes a tensor as orthgonal by flattening its trailing dims and QR-factorizing the resulting 2d\n",
    "        \n",
    "        #self.weight = torch.stack([ nn.init.orthogonal_(torch.empty((head_dim, head_dim))) for _ in range(num_heads) ], dim=0)\n",
    "        #self.weight = nn.Parameter(self.weight)\n",
    "\n",
    "        bound = 1 / math.sqrt(head_dim)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.residual= residual\n",
    "\n",
    "      def forward(self, x):\n",
    "        \n",
    "        x, losses = x\n",
    "\n",
    "        bs, hd, seq, hdim = x.shape\n",
    "        y = self.dropout(x)\n",
    "        y = torch.matmul(y, self.weight) #BS, HD, SEQ, HDIM\n",
    "        if self.use_bias:\n",
    "          y += self.bias\n",
    "        y = self.nonlinearity(y)\n",
    "\n",
    "        #loss = torch.eye(hdim, device=self.weight.device).unsqueeze(0).expand(* self.weight.shape)\n",
    "        #loss = nn.MSELoss()(torch.matmul(self.weight, self.weight.transpose(-1, -2)), loss)\n",
    "        #loss *= LAMBDA\n",
    "\n",
    "        #losses.append(loss)\n",
    "\n",
    "        if self.residual:\n",
    "          return x + y, losses\n",
    "        return y, losses\n",
    "\n",
    "    self.lka = nn.Sequential(\n",
    "        \n",
    "        #AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        #AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        #AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Softplus(), False),\n",
    "        \n",
    "        #HeadWiseFF(self.num_heads, self.qkv_dim, dropout_rate, nn.Softplus(), use_bias=False),\n",
    "        \n",
    "        #GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        #GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Softplus(), False)\n",
    "\n",
    "        #LinearKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Softplus(), False),\n",
    "\n",
    "        #Lambda(lambda o, x: (o['act'](x[0]), x[1]), { 'act' : nn.Identity() })\n",
    "        \n",
    "    )\n",
    "\n",
    "    self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "    #BS x HEADS x SEQ x HEAD_DIM\n",
    "    \n",
    "    q, _ = self.lka((q, losses))\n",
    "    k, _ = self.lka((k, losses)) #Use this for var kernel\n",
    "\n",
    "    q = q / math.sqrt(self.head_dim)\n",
    "    k = k / math.sqrt(self.head_dim)\n",
    "\n",
    "    numerator = torch.matmul(k.unsqueeze(-1), v.unsqueeze(-2))\n",
    "    numerator = numerator.sum(axis=2)\n",
    "    numerator = torch.matmul(q, numerator)\n",
    "    \n",
    "    denominator = k.sum(axis=2).unsqueeze(-1)\n",
    "    denominator = q.matmul(denominator)\n",
    "\n",
    "    out = numerator / denominator\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "    \n",
    "    #TODO: INSERT DROPOUT\n",
    "    \n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(SimpleAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   =qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "\n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "    \n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "    #self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v) #BS x HEADS x SEQ x HEAD_DIM\n",
    "\n",
    "    _, _, seq_len, _ = q.shape\n",
    "\n",
    "    kv = torch.matmul(k.transpose(-1, -2), v)\n",
    "    kv *= 1 / math.sqrt(seq_len)\n",
    "    kv = self.dropout(kv)\n",
    "\n",
    "    out = torch.matmul(q, kv)\n",
    "    #out *= 1 / math.sqrt(self.head_dim)\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "    \n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    #out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class FtAttention(nn.Module):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(FtAttention, self).__init__()\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    return torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n",
    "\n",
    "class TBlock(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, mlp_dim, num_heads, dropout_rate):\n",
    "    super(TBlock, self).__init__()\n",
    "\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.qkv_dim  = qkv_dim\n",
    "    self.mlp_dim  = mlp_dim\n",
    "\n",
    "    self.layernorm_input = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "    self.layernorm_inter = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "\n",
    "    self.attention = TAttention(hidden_dim, qkv_dim, num_heads, dropout_rate)\n",
    "\n",
    "    self.ffn       = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, mlp_dim), nn.GELU(), nn.Dropout(dropout_rate),\n",
    "        nn.Linear(mlp_dim, hidden_dim), nn.Dropout(dropout_rate),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, input, losses=[]):\n",
    "    x = self.layernorm_input(input)\n",
    "    x = self.attention(x, losses)\n",
    "\n",
    "    x = input + x\n",
    "\n",
    "    y = self.layernorm_inter(x)\n",
    "    x = self.ffn(y) + x\n",
    "\n",
    "    return x\n",
    "\n",
    "class TClassifier(nn.Module):\n",
    "  def __init__(self, classes, hidden_dim, inter_dim, dropout_rate):\n",
    "    super(TClassifier, self).__init__()\n",
    "\n",
    "    self.layernorm = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "    self.dropout   = nn.Dropout(dropout_rate)\n",
    "\n",
    "    self.ffn       = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, inter_dim), nn.GELU(),\n",
    "    )\n",
    "    self.output    = nn.Linear(inter_dim, classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layernorm(x)\n",
    "    x = x[:, 0, :]\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    x = self.ffn(x)\n",
    "    logits = self.output(x)\n",
    "\n",
    "    return logits\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, classes, num_embeddings, seq_len, hidden_dim, qkv_dim, mlp_dim, num_heads, num_blocks, output_mlp_units, internal_dropout_rate=0.1, output_dropout_rate=0.0):\n",
    "    super(Transformer, self).__init__()\n",
    "    \n",
    "    self.embed_layer = TEmbedding(num_embeddings, hidden_dim, seq_len)\n",
    "    self.blocks      = nn.ModuleList([ TBlock(hidden_dim, qkv_dim, mlp_dim, num_heads, internal_dropout_rate) for _ in range(num_blocks) ])\n",
    "    self.classifier  = TClassifier(classes, hidden_dim, output_mlp_units, output_dropout_rate)\n",
    "\n",
    "  def forward(self, pixel_values):\n",
    "    additional_losses = []\n",
    "\n",
    "    x = self.embed_layer(pixel_values)\n",
    "    \n",
    "    for block in self.blocks:\n",
    "      x = block(x, additional_losses)\n",
    "    \n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return x, additional_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NUYKMp3zpofW",
    "outputId": "a3589216-6111-468f-a1fd-f55149f57011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model 21019658 params, new model 21412874 params, ratio 1.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9000e1f50>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def get_schedule(warmup_steps):\n",
    "  def lr_schedule(step):\n",
    "    return 1.0 * np.minimum(1.0, step / warmup_steps) / np.sqrt(np.maximum(step, warmup_steps))\n",
    "\n",
    "  return lr_schedule\n",
    "\n",
    "lr=0.005\n",
    "weight_decay=0.1\n",
    "warmup=1000\n",
    "\n",
    "\n",
    "def const_schedule(lr):\n",
    "  def lr_schedule(step):\n",
    "    return lr\n",
    "  return lr_schedule\n",
    "\n",
    "def training_setup():\n",
    "  model = model_factory()\n",
    "  criterion = nn.CrossEntropyLoss().cuda()\n",
    "  optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  schedule_func = get_schedule(warmup)\n",
    "  #schedule_func = const_schedule(1.0) #<--------- TEMPORARY\n",
    "  scheduler = LambdaLR(optimizer, schedule_func)\n",
    "\n",
    "  return model, criterion, optimizer, schedule_func, scheduler\n",
    "\n",
    "_, _, _, schedule_func, _ = training_setup()\n",
    "\n",
    "plt.plot([ lr * schedule_func(i) for i in range(15000) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fyaGMm_WprZR",
    "outputId": "41f13e16-4b0b-4939-b5bd-8f27155bad7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model 21019658 params, new model 21412874 params, ratio 1.02\n",
      "Epoch 0\n",
      "[====================] 50/50: - running_loss: 2.2384 - running_reg: 0.000000 - running_acc: 0.1818 - lr: 0.00001 - epoch_loss: 2.2740 - epoch_reg: 0.000000 - epoch_acc: 0.1525 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 64.1898 s\n",
      "Epoch 1\n",
      "[====================] 50/50: - running_loss: 2.2709 - running_reg: 0.000000 - running_acc: 0.1663 - lr: 0.00002 - epoch_loss: 2.2650 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2776 s\n",
      "Epoch 2\n",
      "[====================] 50/50: - running_loss: 2.2673 - running_reg: 0.000000 - running_acc: 0.1805 - lr: 0.00002 - epoch_loss: 2.2700 - epoch_reg: 0.000000 - epoch_acc: 0.1819 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.3101 s\n",
      "Epoch 3\n",
      "[====================] 50/50: - running_loss: 2.2640 - running_reg: 0.000000 - running_acc: 0.1586 - lr: 0.00003 - epoch_loss: 2.2732 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2012 s\n",
      "Epoch 4\n",
      "[====================] 50/50: - running_loss: 2.2713 - running_reg: 0.000000 - running_acc: 0.1752 - lr: 0.00004 - epoch_loss: 2.2687 - epoch_reg: 0.000000 - epoch_acc: 0.1806 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1802 s\n",
      "Epoch 5\n",
      "[====================] 50/50: - running_loss: 2.2748 - running_reg: 0.000000 - running_acc: 0.1700 - lr: 0.00005 - epoch_loss: 2.2663 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2525 s\n",
      "Epoch 6\n",
      "[====================] 50/50: - running_loss: 2.2989 - running_reg: 0.000000 - running_acc: 0.1491 - lr: 0.00006 - epoch_loss: 2.2863 - epoch_reg: 0.000000 - epoch_acc: 0.1556 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1843 s\n",
      "Epoch 7\n",
      "[====================] 50/50: - running_loss: 2.2770 - running_reg: 0.000000 - running_acc: 0.1673 - lr: 0.00006 - epoch_loss: 2.2834 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1985 s\n",
      "Epoch 8\n",
      "[====================] 50/50: - running_loss: 2.2647 - running_reg: 0.000000 - running_acc: 0.1790 - lr: 0.00007 - epoch_loss: 2.2663 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2548 s\n",
      "Epoch 9\n",
      "[====================] 50/50: - running_loss: 2.2613 - running_reg: 0.000000 - running_acc: 0.1729 - lr: 0.00008 - epoch_loss: 2.2687 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2248 s\n",
      "Epoch 10\n",
      "[====================] 50/50: - running_loss: 2.2492 - running_reg: 0.000000 - running_acc: 0.1733 - lr: 0.00009 - epoch_loss: 2.2661 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1953 s\n",
      "Epoch 11\n",
      "[====================] 50/50: - running_loss: 2.2979 - running_reg: 0.000000 - running_acc: 0.1665 - lr: 0.00009 - epoch_loss: 2.2773 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1345 s\n",
      "Epoch 12\n",
      "[====================] 50/50: - running_loss: 2.2563 - running_reg: 0.000000 - running_acc: 0.1719 - lr: 0.00010 - epoch_loss: 2.2555 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1795 s\n",
      "Epoch 13\n",
      "[====================] 50/50: - running_loss: 2.2444 - running_reg: 0.000000 - running_acc: 0.1598 - lr: 0.00011 - epoch_loss: 2.2777 - epoch_reg: 0.000000 - epoch_acc: 0.1538 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1951 s\n",
      "Epoch 14\n",
      "[====================] 50/50: - running_loss: 2.2633 - running_reg: 0.000000 - running_acc: 0.1681 - lr: 0.00012 - epoch_loss: 2.2602 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1585 s\n",
      "Epoch 15\n",
      "[====================] 50/50: - running_loss: 2.2683 - running_reg: 0.000000 - running_acc: 0.1843 - lr: 0.00013 - epoch_loss: 2.2576 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2122 s\n",
      "Epoch 16\n",
      "[====================] 50/50: - running_loss: 2.2831 - running_reg: 0.000000 - running_acc: 0.1495 - lr: 0.00013 - epoch_loss: 2.2650 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2159 s\n",
      "Epoch 17\n",
      "[====================] 50/50: - running_loss: 2.2712 - running_reg: 0.000000 - running_acc: 0.1323 - lr: 0.00014 - epoch_loss: 2.2673 - epoch_reg: 0.000000 - epoch_acc: 0.1575 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1841 s\n",
      "Epoch 18\n",
      "[====================] 50/50: - running_loss: 2.2462 - running_reg: 0.000000 - running_acc: 0.1752 - lr: 0.00015 - epoch_loss: 2.2554 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2115 s\n",
      "Epoch 19\n",
      "[====================] 50/50: - running_loss: 2.2839 - running_reg: 0.000000 - running_acc: 0.1528 - lr: 0.00016 - epoch_loss: 2.2628 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.3569 s\n",
      "Epoch 20\n",
      "[====================] 50/50: - running_loss: 2.2589 - running_reg: 0.000000 - running_acc: 0.1714 - lr: 0.00015 - epoch_loss: 2.2639 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2277 s\n",
      "Epoch 21\n",
      "[====================] 50/50: - running_loss: 2.2803 - running_reg: 0.000000 - running_acc: 0.1544 - lr: 0.00015 - epoch_loss: 2.2683 - epoch_reg: 0.000000 - epoch_acc: 0.1688 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2031 s\n",
      "Epoch 22\n",
      "[====================] 50/50: - running_loss: 2.2590 - running_reg: 0.000000 - running_acc: 0.1695 - lr: 0.00015 - epoch_loss: 2.2570 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1434 s\n",
      "Epoch 23\n",
      "[====================] 50/50: - running_loss: 2.2629 - running_reg: 0.000000 - running_acc: 0.1632 - lr: 0.00014 - epoch_loss: 2.2578 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1145 s\n",
      "Epoch 24\n",
      "[====================] 50/50: - running_loss: 2.2327 - running_reg: 0.000000 - running_acc: 0.1937 - lr: 0.00014 - epoch_loss: 2.2487 - epoch_reg: 0.000000 - epoch_acc: 0.1775 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1010 s\n",
      "Epoch 25\n",
      "[====================] 50/50: - running_loss: 2.2359 - running_reg: 0.000000 - running_acc: 0.1785 - lr: 0.00014 - epoch_loss: 2.2468 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1330 s\n",
      "Epoch 26\n",
      "[====================] 50/50: - running_loss: 2.2533 - running_reg: 0.000000 - running_acc: 0.1629 - lr: 0.00014 - epoch_loss: 2.2503 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1185 s\n",
      "Epoch 27\n",
      "[====================] 50/50: - running_loss: 2.2498 - running_reg: 0.000000 - running_acc: 0.1669 - lr: 0.00013 - epoch_loss: 2.2561 - epoch_reg: 0.000000 - epoch_acc: 0.1556 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.0799 s\n",
      "Epoch 28\n",
      "[====================] 50/50: - running_loss: 2.2784 - running_reg: 0.000000 - running_acc: 0.1637 - lr: 0.00013 - epoch_loss: 2.2679 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1979 s\n",
      "Epoch 29\n",
      "[====================] 50/50: - running_loss: 2.2723 - running_reg: 0.000000 - running_acc: 0.1561 - lr: 0.00013 - epoch_loss: 2.2676 - epoch_reg: 0.000000 - epoch_acc: 0.1575 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1534 s\n",
      "Epoch 30\n",
      "[====================] 50/50: - running_loss: 2.2587 - running_reg: 0.000000 - running_acc: 0.1627 - lr: 0.00013 - epoch_loss: 2.2620 - epoch_reg: 0.000000 - epoch_acc: 0.1637 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1064 s\n",
      "Epoch 31\n",
      "[====================] 50/50: - running_loss: 2.2656 - running_reg: 0.000000 - running_acc: 0.1666 - lr: 0.00013 - epoch_loss: 2.2556 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1292 s\n",
      "Epoch 32\n",
      "[====================] 50/50: - running_loss: 2.2744 - running_reg: 0.000000 - running_acc: 0.1576 - lr: 0.00012 - epoch_loss: 2.2688 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1430 s\n",
      "Epoch 33\n",
      "[====================] 50/50: - running_loss: 2.2656 - running_reg: 0.000000 - running_acc: 0.1576 - lr: 0.00012 - epoch_loss: 2.2641 - epoch_reg: 0.000000 - epoch_acc: 0.1519 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1464 s\n",
      "Epoch 34\n",
      "[====================] 50/50: - running_loss: 2.2382 - running_reg: 0.000000 - running_acc: 0.1544 - lr: 0.00012 - epoch_loss: 2.2412 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.0813 s\n",
      "Epoch 35\n",
      "[====================] 50/50: - running_loss: 2.2280 - running_reg: 0.000000 - running_acc: 0.2099 - lr: 0.00012 - epoch_loss: 2.2389 - epoch_reg: 0.000000 - epoch_acc: 0.1875 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.0701 s\n",
      "Epoch 36\n",
      "[====================] 50/50: - running_loss: 2.2281 - running_reg: 0.000000 - running_acc: 0.1838 - lr: 0.00012 - epoch_loss: 2.2430 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1193 s\n",
      "Epoch 37\n",
      "[====================] 50/50: - running_loss: 2.2476 - running_reg: 0.000000 - running_acc: 0.1723 - lr: 0.00011 - epoch_loss: 2.2550 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1233 s\n",
      "Epoch 38\n",
      "[====================] 50/50: - running_loss: 2.2636 - running_reg: 0.000000 - running_acc: 0.1593 - lr: 0.00011 - epoch_loss: 2.2541 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1211 s\n",
      "Epoch 39\n",
      "[====================] 50/50: - running_loss: 2.2643 - running_reg: 0.000000 - running_acc: 0.1563 - lr: 0.00011 - epoch_loss: 2.2517 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1369 s\n",
      "Epoch 40\n",
      "[====================] 50/50: - running_loss: 2.2551 - running_reg: 0.000000 - running_acc: 0.1670 - lr: 0.00011 - epoch_loss: 2.2595 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1448 s\n",
      "Epoch 41\n",
      "[====================] 50/50: - running_loss: 2.2554 - running_reg: 0.000000 - running_acc: 0.1757 - lr: 0.00011 - epoch_loss: 2.2604 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1173 s\n",
      "Epoch 42\n",
      "[====================] 50/50: - running_loss: 2.2686 - running_reg: 0.000000 - running_acc: 0.1712 - lr: 0.00011 - epoch_loss: 2.2639 - epoch_reg: 0.000000 - epoch_acc: 0.1619 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1822 s\n",
      "Epoch 43\n",
      "[====================] 50/50: - running_loss: 2.2551 - running_reg: 0.000000 - running_acc: 0.1728 - lr: 0.00011 - epoch_loss: 2.2552 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1534 s\n",
      "Epoch 44\n",
      "[====================] 50/50: - running_loss: 2.2612 - running_reg: 0.000000 - running_acc: 0.1628 - lr: 0.00011 - epoch_loss: 2.2526 - epoch_reg: 0.000000 - epoch_acc: 0.1806 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1409 s\n",
      "Epoch 45\n",
      "[====================] 50/50: - running_loss: 2.2444 - running_reg: 0.000000 - running_acc: 0.1702 - lr: 0.00010 - epoch_loss: 2.2474 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1581 s\n",
      "Epoch 46\n",
      "[====================] 50/50: - running_loss: 2.2665 - running_reg: 0.000000 - running_acc: 0.1614 - lr: 0.00010 - epoch_loss: 2.2530 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1632 s\n",
      "Epoch 47\n",
      "[====================] 50/50: - running_loss: 2.2483 - running_reg: 0.000000 - running_acc: 0.1797 - lr: 0.00010 - epoch_loss: 2.2556 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1998 s\n",
      "Epoch 48\n",
      "[====================] 50/50: - running_loss: 2.2629 - running_reg: 0.000000 - running_acc: 0.1575 - lr: 0.00010 - epoch_loss: 2.2425 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.2324 s\n",
      "Epoch 49\n",
      "[====================] 50/50: - running_loss: 2.2516 - running_reg: 0.000000 - running_acc: 0.1756 - lr: 0.00010 - epoch_loss: 2.2474 - epoch_reg: 0.000000 - epoch_acc: 0.1906 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1866 s\n",
      "Epoch 50\n",
      "[====================] 50/50: - running_loss: 2.2566 - running_reg: 0.000000 - running_acc: 0.1685 - lr: 0.00010 - epoch_loss: 2.2535 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1641 s\n",
      "Epoch 51\n",
      "[====================] 50/50: - running_loss: 2.2615 - running_reg: 0.000000 - running_acc: 0.1663 - lr: 0.00010 - epoch_loss: 2.2499 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1423 s\n",
      "Epoch 52\n",
      "[====================] 50/50: - running_loss: 2.2492 - running_reg: 0.000000 - running_acc: 0.1631 - lr: 0.00010 - epoch_loss: 2.2458 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1678 s\n",
      "Epoch 53\n",
      "[====================] 50/50: - running_loss: 2.2605 - running_reg: 0.000000 - running_acc: 0.1607 - lr: 0.00010 - epoch_loss: 2.2621 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1335 s\n",
      "Epoch 54\n",
      "[====================] 50/50: - running_loss: 2.2347 - running_reg: 0.000000 - running_acc: 0.2040 - lr: 0.00010 - epoch_loss: 2.2486 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1422 s\n",
      "Epoch 55\n",
      "[====================] 50/50: - running_loss: 2.2615 - running_reg: 0.000000 - running_acc: 0.1526 - lr: 0.00009 - epoch_loss: 2.2557 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1251 s\n",
      "Epoch 56\n",
      "[====================] 50/50: - running_loss: 2.2586 - running_reg: 0.000000 - running_acc: 0.1551 - lr: 0.00009 - epoch_loss: 2.2560 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1522 s\n",
      "Epoch 57\n",
      "[====================] 50/50: - running_loss: 2.2526 - running_reg: 0.000000 - running_acc: 0.1661 - lr: 0.00009 - epoch_loss: 2.2605 - epoch_reg: 0.000000 - epoch_acc: 0.1519 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1778 s\n",
      "Epoch 58\n",
      "[====================] 50/50: - running_loss: 2.2552 - running_reg: 0.000000 - running_acc: 0.1596 - lr: 0.00009 - epoch_loss: 2.2571 - epoch_reg: 0.000000 - epoch_acc: 0.1600 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 62.1595 s\n",
      "Epoch 59\n",
      "[====================] 50/50: - running_loss: 2.2588 - running_reg: 0.000000 - running_acc: 0.1581 - lr: 0.00009 - epoch_loss: 2.2575 - epoch_reg: 0.000000 - epoch_acc: 0.1475 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 60.8136 s\n",
      "Epoch 60\n",
      "[====================] 50/50: - running_loss: 2.2640 - running_reg: 0.000000 - running_acc: 0.1643 - lr: 0.00009 - epoch_loss: 2.2550 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2573 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 91.8368 s\n",
      "Epoch 61\n",
      "[====================] 50/50: - running_loss: 2.2522 - running_reg: 0.000000 - running_acc: 0.1829 - lr: 0.00009 - epoch_loss: 2.2457 - epoch_reg: 0.000000 - epoch_acc: 0.1963 - valid_loss: 2.2569 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.2309 s\n",
      "Epoch 62\n",
      "[====================] 50/50: - running_loss: 2.2567 - running_reg: 0.000000 - running_acc: 0.1714 - lr: 0.00009 - epoch_loss: 2.2513 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2567 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 90.1633 s\n",
      "Epoch 63\n",
      "[====================] 50/50: - running_loss: 2.2455 - running_reg: 0.000000 - running_acc: 0.1717 - lr: 0.00009 - epoch_loss: 2.2585 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 2.2615 - valid_reg: 0.000000 - valid_acc: 0.1705 - epoch_time: 89.3843 s\n",
      "Epoch 64\n",
      "[====================] 50/50: - running_loss: 2.2515 - running_reg: 0.000000 - running_acc: 0.1820 - lr: 0.00009 - epoch_loss: 2.2609 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 2.2600 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.1296 s\n",
      "Epoch 65\n",
      "[====================] 50/50: - running_loss: 2.2512 - running_reg: 0.000000 - running_acc: 0.1617 - lr: 0.00009 - epoch_loss: 2.2520 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 2.2590 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.1269 s\n",
      "Epoch 66\n",
      "[====================] 50/50: - running_loss: 2.2534 - running_reg: 0.000000 - running_acc: 0.1740 - lr: 0.00009 - epoch_loss: 2.2554 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2608 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3193 s\n",
      "Epoch 67\n",
      "[====================] 50/50: - running_loss: 2.2618 - running_reg: 0.000000 - running_acc: 0.1542 - lr: 0.00009 - epoch_loss: 2.2447 - epoch_reg: 0.000000 - epoch_acc: 0.1688 - valid_loss: 2.2578 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.2513 s\n",
      "Epoch 68\n",
      "[====================] 50/50: - running_loss: 2.2573 - running_reg: 0.000000 - running_acc: 0.1452 - lr: 0.00009 - epoch_loss: 2.2544 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2567 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.3149 s\n",
      "Epoch 69\n",
      "[====================] 50/50: - running_loss: 2.2649 - running_reg: 0.000000 - running_acc: 0.1432 - lr: 0.00008 - epoch_loss: 2.2571 - epoch_reg: 0.000000 - epoch_acc: 0.1581 - valid_loss: 2.2591 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.2130 s\n",
      "Epoch 70\n",
      "[====================] 50/50: - running_loss: 2.2641 - running_reg: 0.000000 - running_acc: 0.1613 - lr: 0.00008 - epoch_loss: 2.2551 - epoch_reg: 0.000000 - epoch_acc: 0.1600 - valid_loss: 2.2589 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.1130 s\n",
      "Epoch 71\n",
      "[====================] 50/50: - running_loss: 2.2786 - running_reg: 0.000000 - running_acc: 0.1613 - lr: 0.00008 - epoch_loss: 2.2658 - epoch_reg: 0.000000 - epoch_acc: 0.1581 - valid_loss: 2.2568 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.1368 s\n",
      "Epoch 72\n",
      "[====================] 50/50: - running_loss: 2.2492 - running_reg: 0.000000 - running_acc: 0.1829 - lr: 0.00008 - epoch_loss: 2.2554 - epoch_reg: 0.000000 - epoch_acc: 0.1819 - valid_loss: 2.2585 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3949 s\n",
      "Epoch 73\n",
      "[====================] 50/50: - running_loss: 2.2536 - running_reg: 0.000000 - running_acc: 0.1593 - lr: 0.00008 - epoch_loss: 2.2534 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 2.2590 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.1242 s\n",
      "Epoch 74\n",
      "[====================] 50/50: - running_loss: 2.2351 - running_reg: 0.000000 - running_acc: 0.1717 - lr: 0.00008 - epoch_loss: 2.2519 - epoch_reg: 0.000000 - epoch_acc: 0.1619 - valid_loss: 2.2604 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.2176 s\n",
      "Epoch 75\n",
      "[====================] 50/50: - running_loss: 2.2387 - running_reg: 0.000000 - running_acc: 0.1876 - lr: 0.00008 - epoch_loss: 2.2510 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2564 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.1710 s\n",
      "Epoch 76\n",
      "[====================] 50/50: - running_loss: 2.2409 - running_reg: 0.000000 - running_acc: 0.1943 - lr: 0.00008 - epoch_loss: 2.2533 - epoch_reg: 0.000000 - epoch_acc: 0.1800 - valid_loss: 2.2583 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.3859 s\n",
      "Epoch 77\n",
      "[====================] 50/50: - running_loss: 2.2447 - running_reg: 0.000000 - running_acc: 0.1917 - lr: 0.00008 - epoch_loss: 2.2571 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2591 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.4938 s\n",
      "Epoch 78\n",
      "[====================] 50/50: - running_loss: 2.2706 - running_reg: 0.000000 - running_acc: 0.1563 - lr: 0.00008 - epoch_loss: 2.2592 - epoch_reg: 0.000000 - epoch_acc: 0.1619 - valid_loss: 2.2605 - valid_reg: 0.000000 - valid_acc: 0.1705 - epoch_time: 89.5585 s\n",
      "Epoch 79\n",
      "[====================] 50/50: - running_loss: 2.2318 - running_reg: 0.000000 - running_acc: 0.1858 - lr: 0.00008 - epoch_loss: 2.2491 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 2.2657 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.7165 s\n",
      "Epoch 80\n",
      "[====================] 50/50: - running_loss: 2.2605 - running_reg: 0.000000 - running_acc: 0.1590 - lr: 0.00008 - epoch_loss: 2.2539 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 2.2581 - valid_reg: 0.000000 - valid_acc: 0.1705 - epoch_time: 89.6081 s\n",
      "Epoch 81\n",
      "[====================] 50/50: - running_loss: 2.2304 - running_reg: 0.000000 - running_acc: 0.1920 - lr: 0.00008 - epoch_loss: 2.2533 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2599 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3202 s\n",
      "Epoch 82\n",
      "[====================] 50/50: - running_loss: 2.2494 - running_reg: 0.000000 - running_acc: 0.1653 - lr: 0.00008 - epoch_loss: 2.2517 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 2.2623 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.3481 s\n",
      "Epoch 83\n",
      "[====================] 50/50: - running_loss: 2.2430 - running_reg: 0.000000 - running_acc: 0.1888 - lr: 0.00008 - epoch_loss: 2.2512 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2566 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.2810 s\n",
      "Epoch 84\n",
      "[====================] 50/50: - running_loss: 2.2413 - running_reg: 0.000000 - running_acc: 0.1796 - lr: 0.00008 - epoch_loss: 2.2418 - epoch_reg: 0.000000 - epoch_acc: 0.1850 - valid_loss: 2.2635 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.3188 s\n",
      "Epoch 85\n",
      "[====================] 50/50: - running_loss: 2.2376 - running_reg: 0.000000 - running_acc: 0.1597 - lr: 0.00008 - epoch_loss: 2.2507 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 2.2598 - valid_reg: 0.000000 - valid_acc: 0.1765 - epoch_time: 89.9925 s\n",
      "Epoch 86\n",
      "[====================] 50/50: - running_loss: 2.2489 - running_reg: 0.000000 - running_acc: 0.1644 - lr: 0.00008 - epoch_loss: 2.2573 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 2.2556 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3228 s\n",
      "Epoch 87\n",
      "[====================] 50/50: - running_loss: 2.2591 - running_reg: 0.000000 - running_acc: 0.1534 - lr: 0.00008 - epoch_loss: 2.2586 - epoch_reg: 0.000000 - epoch_acc: 0.1581 - valid_loss: 2.2594 - valid_reg: 0.000000 - valid_acc: 0.1660 - epoch_time: 141.9732 s\n",
      "Epoch 88\n",
      "[====================] 50/50: - running_loss: 2.2559 - running_reg: 0.000000 - running_acc: 0.1686 - lr: 0.00007 - epoch_loss: 2.2608 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2616 - valid_reg: 0.000000 - valid_acc: 0.1815 - epoch_time: 90.0085 s\n",
      "Epoch 89\n",
      "[====================] 50/50: - running_loss: 2.2664 - running_reg: 0.000000 - running_acc: 0.1472 - lr: 0.00007 - epoch_loss: 2.2588 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 2.2584 - valid_reg: 0.000000 - valid_acc: 0.1685 - epoch_time: 89.3726 s\n",
      "Epoch 90\n",
      "[====================] 50/50: - running_loss: 2.2597 - running_reg: 0.000000 - running_acc: 0.1862 - lr: 0.00007 - epoch_loss: 2.2630 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2580 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3128 s\n",
      "Epoch 91\n",
      "[====================] 50/50: - running_loss: 2.2411 - running_reg: 0.000000 - running_acc: 0.1741 - lr: 0.00007 - epoch_loss: 2.2483 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.2818 s\n",
      "Epoch 92\n",
      "[====================] 50/50: - running_loss: 2.2366 - running_reg: 0.000000 - running_acc: 0.1798 - lr: 0.00007 - epoch_loss: 2.2378 - epoch_reg: 0.000000 - epoch_acc: 0.1919 - valid_loss: 2.2593 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.5391 s\n",
      "Epoch 93\n",
      "[====================] 50/50: - running_loss: 2.2434 - running_reg: 0.000000 - running_acc: 0.1833 - lr: 0.00007 - epoch_loss: 2.2540 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 2.2563 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.4843 s\n",
      "Epoch 94\n",
      "[====================] 50/50: - running_loss: 2.2615 - running_reg: 0.000000 - running_acc: 0.1588 - lr: 0.00007 - epoch_loss: 2.2619 - epoch_reg: 0.000000 - epoch_acc: 0.1581 - valid_loss: 2.2585 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3135 s\n",
      "Epoch 95\n",
      "[====================] 50/50: - running_loss: 2.2766 - running_reg: 0.000000 - running_acc: 0.1380 - lr: 0.00007 - epoch_loss: 2.2649 - epoch_reg: 0.000000 - epoch_acc: 0.1544 - valid_loss: 2.2584 - valid_reg: 0.000000 - valid_acc: 0.1635 - epoch_time: 89.2761 s\n",
      "Epoch 96\n",
      "[====================] 50/50: - running_loss: 2.2353 - running_reg: 0.000000 - running_acc: 0.2071 - lr: 0.00007 - epoch_loss: 2.2402 - epoch_reg: 0.000000 - epoch_acc: 0.1925 - valid_loss: 2.2586 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.3070 s\n",
      "Epoch 97\n",
      "[====================] 50/50: - running_loss: 2.2383 - running_reg: 0.000000 - running_acc: 0.1653 - lr: 0.00007 - epoch_loss: 2.2546 - epoch_reg: 0.000000 - epoch_acc: 0.1562 - valid_loss: 2.2682 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3310 s\n",
      "Epoch 98\n",
      "[====================] 50/50: - running_loss: 2.2524 - running_reg: 0.000000 - running_acc: 0.1772 - lr: 0.00007 - epoch_loss: 2.2588 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2574 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.2563 s\n",
      "Epoch 99\n",
      "[====================] 50/50: - running_loss: 2.2369 - running_reg: 0.000000 - running_acc: 0.1822 - lr: 0.00007 - epoch_loss: 2.2409 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2562 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3632 s\n",
      "Epoch 100\n",
      "[====================] 50/50: - running_loss: 2.2398 - running_reg: 0.000000 - running_acc: 0.1744 - lr: 0.00007 - epoch_loss: 2.2424 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 2.2606 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.2550 s\n",
      "Epoch 101\n",
      "[====================] 50/50: - running_loss: 2.2261 - running_reg: 0.000000 - running_acc: 0.1917 - lr: 0.00007 - epoch_loss: 2.2405 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 2.2632 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3391 s\n",
      "Epoch 102\n",
      "[====================] 50/50: - running_loss: 2.2692 - running_reg: 0.000000 - running_acc: 0.1708 - lr: 0.00007 - epoch_loss: 2.2626 - epoch_reg: 0.000000 - epoch_acc: 0.1737 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3416 s\n",
      "Epoch 103\n",
      "[====================] 50/50: - running_loss: 2.2528 - running_reg: 0.000000 - running_acc: 0.1592 - lr: 0.00007 - epoch_loss: 2.2576 - epoch_reg: 0.000000 - epoch_acc: 0.1587 - valid_loss: 2.2552 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3719 s\n",
      "Epoch 104\n",
      "[====================] 50/50: - running_loss: 2.2462 - running_reg: 0.000000 - running_acc: 0.1750 - lr: 0.00007 - epoch_loss: 2.2355 - epoch_reg: 0.000000 - epoch_acc: 0.1856 - valid_loss: 2.2624 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.3624 s\n",
      "Epoch 105\n",
      "[====================] 50/50: - running_loss: 2.2483 - running_reg: 0.000000 - running_acc: 0.1791 - lr: 0.00007 - epoch_loss: 2.2382 - epoch_reg: 0.000000 - epoch_acc: 0.1825 - valid_loss: 2.2581 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3082 s\n",
      "Epoch 106\n",
      "[====================] 50/50: - running_loss: 2.2579 - running_reg: 0.000000 - running_acc: 0.1610 - lr: 0.00007 - epoch_loss: 2.2502 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2555 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3093 s\n",
      "Epoch 107\n",
      "[====================] 50/50: - running_loss: 2.2503 - running_reg: 0.000000 - running_acc: 0.1767 - lr: 0.00007 - epoch_loss: 2.2523 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3733 s\n",
      "Epoch 108\n",
      "[====================] 50/50: - running_loss: 2.2275 - running_reg: 0.000000 - running_acc: 0.1870 - lr: 0.00007 - epoch_loss: 2.2436 - epoch_reg: 0.000000 - epoch_acc: 0.1806 - valid_loss: 2.2595 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.7297 s\n",
      "Epoch 109\n",
      "[====================] 50/50: - running_loss: 2.2646 - running_reg: 0.000000 - running_acc: 0.1619 - lr: 0.00007 - epoch_loss: 2.2562 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 2.2594 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.4876 s\n",
      "Epoch 110\n",
      "[====================] 50/50: - running_loss: 2.2732 - running_reg: 0.000000 - running_acc: 0.1659 - lr: 0.00007 - epoch_loss: 2.2597 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2602 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.2972 s\n",
      "Epoch 111\n",
      "[====================] 50/50: - running_loss: 2.2556 - running_reg: 0.000000 - running_acc: 0.1638 - lr: 0.00007 - epoch_loss: 2.2523 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2594 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.2443 s\n",
      "Epoch 112\n",
      "[====================] 50/50: - running_loss: 2.2652 - running_reg: 0.000000 - running_acc: 0.1683 - lr: 0.00007 - epoch_loss: 2.2548 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2568 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.2272 s\n",
      "Epoch 113\n",
      "[====================] 50/50: - running_loss: 2.2467 - running_reg: 0.000000 - running_acc: 0.1548 - lr: 0.00007 - epoch_loss: 2.2482 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2654 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3754 s\n",
      "Epoch 114\n",
      "[====================] 50/50: - running_loss: 2.2490 - running_reg: 0.000000 - running_acc: 0.1719 - lr: 0.00007 - epoch_loss: 2.2521 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2598 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3066 s\n",
      "Epoch 115\n",
      "[====================] 50/50: - running_loss: 2.2673 - running_reg: 0.000000 - running_acc: 0.1602 - lr: 0.00007 - epoch_loss: 2.2628 - epoch_reg: 0.000000 - epoch_acc: 0.1619 - valid_loss: 2.2611 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.3539 s\n",
      "Epoch 116\n",
      "[====================] 50/50: - running_loss: 2.2479 - running_reg: 0.000000 - running_acc: 0.1660 - lr: 0.00007 - epoch_loss: 2.2480 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2587 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.3286 s\n",
      "Epoch 117\n",
      "[====================] 50/50: - running_loss: 2.2681 - running_reg: 0.000000 - running_acc: 0.1533 - lr: 0.00007 - epoch_loss: 2.2583 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 2.2567 - valid_reg: 0.000000 - valid_acc: 0.1805 - epoch_time: 89.3111 s\n",
      "Epoch 118\n",
      "[====================] 50/50: - running_loss: 2.2607 - running_reg: 0.000000 - running_acc: 0.1708 - lr: 0.00006 - epoch_loss: 2.2467 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2596 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.3485 s\n",
      "Epoch 119\n",
      "[====================] 50/50: - running_loss: 2.2453 - running_reg: 0.000000 - running_acc: 0.1633 - lr: 0.00006 - epoch_loss: 2.2521 - epoch_reg: 0.000000 - epoch_acc: 0.1587 - valid_loss: 2.2577 - valid_reg: 0.000000 - valid_acc: 0.1625 - epoch_time: 87.8880 s\n",
      "Epoch 120\n",
      "[====================] 50/50: - running_loss: 2.2650 - running_reg: 0.000000 - running_acc: 0.1691 - lr: 0.00006 - epoch_loss: 2.2601 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 2.2606 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 91.1784 s\n",
      "Epoch 121\n",
      "[====================] 50/50: - running_loss: 2.2509 - running_reg: 0.000000 - running_acc: 0.1497 - lr: 0.00006 - epoch_loss: 2.2510 - epoch_reg: 0.000000 - epoch_acc: 0.1544 - valid_loss: 2.2590 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.3080 s\n",
      "Epoch 122\n",
      "[====================] 50/50: - running_loss: 2.2427 - running_reg: 0.000000 - running_acc: 0.1893 - lr: 0.00006 - epoch_loss: 2.2465 - epoch_reg: 0.000000 - epoch_acc: 0.1825 - valid_loss: 2.2621 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.4606 s\n",
      "Epoch 123\n",
      "[====================] 50/50: - running_loss: 2.2458 - running_reg: 0.000000 - running_acc: 0.1629 - lr: 0.00006 - epoch_loss: 2.2475 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 2.2561 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.3847 s\n",
      "Epoch 124\n",
      "[====================] 50/50: - running_loss: 2.2770 - running_reg: 0.000000 - running_acc: 0.1473 - lr: 0.00006 - epoch_loss: 2.2511 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2594 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.7540 s\n",
      "Epoch 125\n",
      "[====================] 50/50: - running_loss: 2.2525 - running_reg: 0.000000 - running_acc: 0.1738 - lr: 0.00006 - epoch_loss: 2.2568 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2589 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.4318 s\n",
      "Epoch 126\n",
      "[====================] 50/50: - running_loss: 2.2470 - running_reg: 0.000000 - running_acc: 0.1767 - lr: 0.00006 - epoch_loss: 2.2596 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2564 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.5009 s\n",
      "Epoch 127\n",
      "[====================] 50/50: - running_loss: 2.2467 - running_reg: 0.000000 - running_acc: 0.1890 - lr: 0.00006 - epoch_loss: 2.2379 - epoch_reg: 0.000000 - epoch_acc: 0.1937 - valid_loss: 2.2632 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.6734 s\n",
      "Epoch 128\n",
      "[====================] 50/50: - running_loss: 2.2711 - running_reg: 0.000000 - running_acc: 0.1441 - lr: 0.00006 - epoch_loss: 2.2559 - epoch_reg: 0.000000 - epoch_acc: 0.1600 - valid_loss: 2.2595 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.6231 s\n",
      "Epoch 129\n",
      "[====================] 50/50: - running_loss: 2.2519 - running_reg: 0.000000 - running_acc: 0.1640 - lr: 0.00006 - epoch_loss: 2.2522 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.6410 s\n",
      "Epoch 130\n",
      "[====================] 50/50: - running_loss: 2.2700 - running_reg: 0.000000 - running_acc: 0.1555 - lr: 0.00006 - epoch_loss: 2.2610 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2555 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.6661 s\n",
      "Epoch 131\n",
      "[====================] 50/50: - running_loss: 2.2362 - running_reg: 0.000000 - running_acc: 0.1843 - lr: 0.00006 - epoch_loss: 2.2605 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 2.2559 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.6985 s\n",
      "Epoch 132\n",
      "[====================] 50/50: - running_loss: 2.2504 - running_reg: 0.000000 - running_acc: 0.1778 - lr: 0.00006 - epoch_loss: 2.2523 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2610 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.8019 s\n",
      "Epoch 133\n",
      "[====================] 50/50: - running_loss: 2.2633 - running_reg: 0.000000 - running_acc: 0.1638 - lr: 0.00006 - epoch_loss: 2.2627 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.7152 s\n",
      "Epoch 134\n",
      "[====================] 50/50: - running_loss: 2.2350 - running_reg: 0.000000 - running_acc: 0.1843 - lr: 0.00006 - epoch_loss: 2.2377 - epoch_reg: 0.000000 - epoch_acc: 0.1737 - valid_loss: 2.2578 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.5722 s\n",
      "Epoch 135\n",
      "[====================] 50/50: - running_loss: 2.2405 - running_reg: 0.000000 - running_acc: 0.1730 - lr: 0.00006 - epoch_loss: 2.2471 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2580 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.5833 s\n",
      "Epoch 136\n",
      "[====================] 50/50: - running_loss: 2.2400 - running_reg: 0.000000 - running_acc: 0.1748 - lr: 0.00006 - epoch_loss: 2.2515 - epoch_reg: 0.000000 - epoch_acc: 0.1688 - valid_loss: 2.2558 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 141.9874 s\n",
      "Epoch 137\n",
      "[====================] 50/50: - running_loss: 2.2545 - running_reg: 0.000000 - running_acc: 0.1593 - lr: 0.00006 - epoch_loss: 2.2518 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 2.2550 - valid_reg: 0.000000 - valid_acc: 0.1695 - epoch_time: 89.6783 s\n",
      "Epoch 138\n",
      "[====================] 50/50: - running_loss: 2.2678 - running_reg: 0.000000 - running_acc: 0.1618 - lr: 0.00006 - epoch_loss: 2.2618 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 2.2612 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.7146 s\n",
      "Epoch 139\n",
      "[====================] 50/50: - running_loss: 2.2636 - running_reg: 0.000000 - running_acc: 0.1495 - lr: 0.00006 - epoch_loss: 2.2580 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 2.2575 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.8125 s\n",
      "Epoch 140\n",
      "[====================] 50/50: - running_loss: 2.2413 - running_reg: 0.000000 - running_acc: 0.1681 - lr: 0.00006 - epoch_loss: 2.2486 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 2.2596 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.6372 s\n",
      "Epoch 141\n",
      "[====================] 50/50: - running_loss: 2.2648 - running_reg: 0.000000 - running_acc: 0.1689 - lr: 0.00006 - epoch_loss: 2.2649 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 2.2566 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.6474 s\n",
      "Epoch 142\n",
      "[====================] 50/50: - running_loss: 2.2423 - running_reg: 0.000000 - running_acc: 0.1736 - lr: 0.00006 - epoch_loss: 2.2418 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2602 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.6312 s\n",
      "Epoch 143\n",
      "[====================] 50/50: - running_loss: 2.2626 - running_reg: 0.000000 - running_acc: 0.1365 - lr: 0.00006 - epoch_loss: 2.2696 - epoch_reg: 0.000000 - epoch_acc: 0.1488 - valid_loss: 2.2587 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.5914 s\n",
      "Epoch 144\n",
      "[====================] 50/50: - running_loss: 2.2491 - running_reg: 0.000000 - running_acc: 0.1648 - lr: 0.00006 - epoch_loss: 2.2470 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 2.2566 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.6427 s\n",
      "Epoch 145\n",
      "[====================] 50/50: - running_loss: 2.2418 - running_reg: 0.000000 - running_acc: 0.1739 - lr: 0.00006 - epoch_loss: 2.2403 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2588 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.5740 s\n",
      "Epoch 146\n",
      "[====================] 50/50: - running_loss: 2.2473 - running_reg: 0.000000 - running_acc: 0.1765 - lr: 0.00006 - epoch_loss: 2.2569 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2586 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.4777 s\n",
      "Epoch 147\n",
      "[====================] 50/50: - running_loss: 2.2595 - running_reg: 0.000000 - running_acc: 0.1666 - lr: 0.00006 - epoch_loss: 2.2534 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2547 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.6115 s\n",
      "Epoch 148\n",
      "[====================] 50/50: - running_loss: 2.2454 - running_reg: 0.000000 - running_acc: 0.1740 - lr: 0.00006 - epoch_loss: 2.2527 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 2.2562 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 89.5804 s\n",
      "Epoch 149\n",
      "[====================] 50/50: - running_loss: 2.2421 - running_reg: 0.000000 - running_acc: 0.1868 - lr: 0.00006 - epoch_loss: 2.2382 - epoch_reg: 0.000000 - epoch_acc: 0.1850 - valid_loss: 2.2588 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 89.6810 s\n",
      " - test_loss: 2.2442 - test_reg: 0.000000 - test_acc: 0.1870 - test_time: 27.3522 s\n",
      "\n",
      "Total accuracy: 0.1870\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = [  ]\n",
    "\n",
    "for i in range(1): ####!!!!!!!!!!!!!!\n",
    "  path = 'model_to_test_' + str(i) + '.b'\n",
    "\n",
    "  model, criterion, optimizer, schedule_func, scheduler = training_setup()\n",
    "\n",
    "  checkpoint = train_model(model, path, train_dataset, valid_dataset, optimizer, criterion, scheduler, accumulation_steps, 150, 50, skip_eval=60)\n",
    "  \n",
    "  if checkpoint is None:\n",
    "    break\n",
    "  \n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  \n",
    "  _, _, acc = test(model, criterion, test_dataset)\n",
    "  test_accuracy.append(acc)\n",
    "\n",
    "test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "print(f'\\nTotal accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCL5UQ_JgVa6"
   },
   "outputs": [],
   "source": [
    "glu: 0.1870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qgnMknjwpuRt"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TEmbedding(nn.Module):\n",
    "  def __init__(self, num_embeddings, hidden_dim, seq_length=1024, padding_idx=0):\n",
    "    super(TEmbedding, self).__init__()\n",
    "    \n",
    "    self.num_embeddings = num_embeddings\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.seq_length = seq_length\n",
    "    self.padding_idx = padding_idx\n",
    "\n",
    "    self.embedding = nn.Embedding(num_embeddings, hidden_dim, padding_idx)\n",
    "    self.pos_embeds  = nn.Parameter(torch.zeros(1, self.seq_length, self.hidden_dim))\n",
    "\n",
    "    self.cls = nn.Parameter(torch.zeros(1, 1, self.hidden_dim)) #!!!!!!! INIT WITH ANOTHER VALUE IF REQUIRED\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, seq_len = input.shape\n",
    "    \n",
    "    embed = self.embedding(input)\n",
    "    embed = embed + self.pos_embeds\n",
    "    embed = torch.cat([ self.cls.expand(batch_size, 1, -1), embed ], axis=1)\n",
    "\n",
    "    return embed\n",
    "    \n",
    "class TAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(TAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   =qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "    \n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "\n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "    q = torch.mul(q, 1. / torch.sqrt(torch.tensor(self.qkv_dim)))\n",
    "\n",
    "    qk = torch.matmul(q, k.transpose(-1, -2))\n",
    "    qk = nn.Softmax(dim=-1)(qk)\n",
    "\n",
    "    def assertion_function(tsr):\n",
    "      tsr = torch.sum(tsr, axis=-1)\n",
    "      tsr = tsr - torch.ones_like(tsr)\n",
    "      return torch.max(torch.abs(tsr)) < 1e-5\n",
    "\n",
    "    assert assertion_function(qk)\n",
    "\n",
    "    qk = self.dropout(qk) #Like in TF implementation; could be done before Softmax by random -inf addition\n",
    "\n",
    "    out = torch.matmul(qk, v)\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "\n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class HWLinear(nn.Module):\n",
    "  def __init__(self, num_heads, input_dim, output_dim, use_bias):\n",
    "    super(HWLinear, self).__init__()\n",
    "    \n",
    "    self.use_bias = use_bias\n",
    "    if use_bias:\n",
    "      self.bias   = nn.Parameter(torch.zeros( (1, num_heads, 1, output_dim)))\n",
    "\n",
    "    self.weight = nn.Parameter(torch.empty( (num_heads, input_dim, output_dim)))\n",
    "\n",
    "    def he_init(m):\n",
    "      s =  np.sqrt( 2. / input_dim )\n",
    "      m.data.normal_(0, s)\n",
    "\n",
    "    he_init(self.weight)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.matmul(x, self.weight)\n",
    "    if self.use_bias:\n",
    "      x += self.bias\n",
    "    return x\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "  def __init__(self, lambda_, objects=None):\n",
    "      super(Lambda, self).__init__()\n",
    "      self.lambda_ = lambda_\n",
    "      self.objects = objects\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.objects is not None:\n",
    "      return self.lambda_(self.objects, x)\n",
    "    return self.lambda_(x)\n",
    "\n",
    "class LKAAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(LKAAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   = qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "\n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "    \n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    #self.lka = nn.Sequential(\n",
    "    #  nn.Linear(self.head_dim, self.head_dim), nn.GELU(),\n",
    "    #  nn.Linear(self.head_dim, self.head_dim), nn.Softplus(beta=2.5),\n",
    "    #)\n",
    "\n",
    "    #256, 4, 16, 1024\n",
    "    #256, 64, 1, 1024\n",
    "    class AMGOLU(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, gate_rank, dropout_rate, gate_nonlinearity, kernel_nonlinearity, use_bias=False):\n",
    "        super(AMGOLU, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads= num_heads\n",
    "        \n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "\n",
    "        self.gate_weight_a = HWLinear(num_heads, self.head_dim, gate_rank, use_bias)\n",
    "        self.gate_weight_b = HWLinear(num_heads, gate_rank, self.head_dim, use_bias)\n",
    "\n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        self.gate_nonlinearity   = gate_nonlinearity\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        forward_info = self.orth_weight(x)\n",
    "        forward_info = self.kernel_nonlinearity(forward_info)\n",
    "\n",
    "        gate_info = self.gate_weight_a(x)\n",
    "        gate_info = self.gate_weight_b(gate_info)\n",
    "        gate_info = self.gate_nonlinearity(gate_info)\n",
    "\n",
    "        x = forward_info * gate_info\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "    class GatedOrthoKernel(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate=0.1, gate_nonlinearity=nn.Sigmoid(), kernel_nonlinearity=nn.Identity(), use_bias=False):\n",
    "        super(GatedOrthoKernel, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "        self.gate_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "\n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        self.gate_nonlinearity   = gate_nonlinearity\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.kernel_nonlinearity(self.orth_weight(x)) * self.gate_nonlinearity(self.gate_weight(x))\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "    class LinearKernel(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate=0.1, kernel_nonlinearity=nn.Identity(), use_bias=False):\n",
    "        super(LinearKernel, self).__init__()\n",
    "\n",
    "        self.head_dim = qkv_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
    "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
    "        \n",
    "        self.kernel_nonlinearity = kernel_nonlinearity\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "      def forward(self, x):\n",
    "        x, losses = x\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.kernel_nonlinearity(self.orth_weight(x))\n",
    "        \n",
    "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
    "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
    "        loss *= LAMBDA\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        return x, losses\n",
    "\n",
    "\n",
    "    class HeadWiseFF(nn.Module):\n",
    "      def __init__(self, num_heads, qkv_dim, dropout_rate, nonlinearity=nn.Identity(), use_bias=False, residual=False):\n",
    "        super(HeadWiseFF, self).__init__()\n",
    "        \n",
    "        head_dim = qkv_dim // num_heads\n",
    "\n",
    "        self.bias   = nn.Parameter(torch.empty( (1, num_heads, 1, head_dim)))\n",
    "        self.dropout= nn.Dropout(dropout_rate)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty( (num_heads, head_dim, head_dim)))\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "        #Orthogonal initialization\n",
    "        #Workaround with torch.stack, since Torch initializes a tensor as orthgonal by flattening its trailing dims and QR-factorizing the resulting 2d\n",
    "        \n",
    "        #self.weight = torch.stack([ nn.init.orthogonal_(torch.empty((head_dim, head_dim))) for _ in range(num_heads) ], dim=0)\n",
    "        #self.weight = nn.Parameter(self.weight)\n",
    "\n",
    "        bound = 1 / math.sqrt(head_dim)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.residual= residual\n",
    "\n",
    "      def forward(self, x):\n",
    "        \n",
    "        x, losses = x\n",
    "\n",
    "        bs, hd, seq, hdim = x.shape\n",
    "        y = self.dropout(x)\n",
    "        y = torch.matmul(y, self.weight) #BS, HD, SEQ, HDIM\n",
    "        if self.use_bias:\n",
    "          y += self.bias\n",
    "        y = self.nonlinearity(y)\n",
    "\n",
    "        #loss = torch.eye(hdim, device=self.weight.device).unsqueeze(0).expand(* self.weight.shape)\n",
    "        #loss = nn.MSELoss()(torch.matmul(self.weight, self.weight.transpose(-1, -2)), loss)\n",
    "        #loss *= LAMBDA\n",
    "\n",
    "        #losses.append(loss)\n",
    "\n",
    "        if self.residual:\n",
    "          return x + y, losses\n",
    "        return y, losses\n",
    "\n",
    "    self.lka = nn.Sequential(\n",
    "        \n",
    "        AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        AMGOLU(self.num_heads, self.qkv_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Softplus(), False),\n",
    "        \n",
    "        #HeadWiseFF(self.num_heads, self.qkv_dim, dropout_rate, nn.Softplus(), use_bias=False),\n",
    "        \n",
    "        #GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        #GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
    "        #GatedOrthoKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Sigmoid(), nn.Softplus(), False)\n",
    "\n",
    "        #LinearKernel(self.num_heads, self.qkv_dim, dropout_rate, nn.Softplus(), False),\n",
    "\n",
    "        #Lambda(lambda o, x: (o['act'](x[0]), x[1]), { 'act' : nn.Identity() })\n",
    "        \n",
    "    )\n",
    "\n",
    "    self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "    #BS x HEADS x SEQ x HEAD_DIM\n",
    "    \n",
    "    q, _ = self.lka((q, losses))\n",
    "    k, _ = self.lka((k, losses)) #Use this for var kernel\n",
    "\n",
    "    q = q / math.sqrt(self.head_dim)\n",
    "    k = k / math.sqrt(self.head_dim)\n",
    "\n",
    "    numerator = torch.matmul(k.unsqueeze(-1), v.unsqueeze(-2))\n",
    "    numerator = numerator.sum(axis=2)\n",
    "    numerator = torch.matmul(q, numerator)\n",
    "    \n",
    "    denominator = k.sum(axis=2).unsqueeze(-1)\n",
    "    denominator = q.matmul(denominator)\n",
    "\n",
    "    out = numerator / denominator\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "    \n",
    "    #TODO: INSERT DROPOUT\n",
    "    \n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    super(SimpleAttention, self).__init__()\n",
    "    self.hidden_dim=hidden_dim\n",
    "    self.qkv_dim   =qkv_dim\n",
    "    self.num_heads =num_heads\n",
    "\n",
    "    assert not qkv_dim % num_heads\n",
    "    \n",
    "    self.head_dim = qkv_dim // num_heads\n",
    "    \n",
    "    self.q = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.k = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "    self.v = nn.Linear(self.hidden_dim, self.qkv_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "    #self.lin = nn.Linear(self.qkv_dim, self.hidden_dim)\n",
    "\n",
    "  def split_heads(self, x):\n",
    "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "    x = x.view(* new_shape)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    q = self.q(x)\n",
    "    k = self.k(x)\n",
    "    v = self.v(x)\n",
    "\n",
    "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v) #BS x HEADS x SEQ x HEAD_DIM\n",
    "\n",
    "    _, _, seq_len, _ = q.shape\n",
    "\n",
    "    kv = torch.matmul(k.transpose(-1, -2), v)\n",
    "    kv *= 1 / math.sqrt(seq_len)\n",
    "    kv = self.dropout(kv)\n",
    "\n",
    "    out = torch.matmul(q, kv)\n",
    "    #out *= 1 / math.sqrt(self.head_dim)\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "    \n",
    "    new_shape = out.shape[:-2] + (self.qkv_dim,)\n",
    "    out = out.reshape(* new_shape)\n",
    "\n",
    "    #out = self.lin(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class FtAttention(nn.Module):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(FtAttention, self).__init__()\n",
    "\n",
    "  def forward(self, x, losses=[]):\n",
    "    return torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n",
    "\n",
    "class TBlock(nn.Module):\n",
    "  def __init__(self, hidden_dim, qkv_dim, mlp_dim, num_heads, dropout_rate):\n",
    "    super(TBlock, self).__init__()\n",
    "\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.qkv_dim  = qkv_dim\n",
    "    self.mlp_dim  = mlp_dim\n",
    "\n",
    "    self.layernorm_input = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "    self.layernorm_inter = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "\n",
    "    self.attention = TAttention(hidden_dim, qkv_dim, num_heads, dropout_rate)\n",
    "\n",
    "    self.ffn       = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, mlp_dim), nn.GELU(), nn.Dropout(dropout_rate),\n",
    "        nn.Linear(mlp_dim, hidden_dim), nn.Dropout(dropout_rate),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, input, losses=[]):\n",
    "    x = self.layernorm_input(input)\n",
    "    x = self.attention(x, losses)\n",
    "\n",
    "    x = input + x\n",
    "\n",
    "    y = self.layernorm_inter(x)\n",
    "    x = self.ffn(y) + x\n",
    "\n",
    "    return x\n",
    "\n",
    "class TClassifier(nn.Module):\n",
    "  def __init__(self, classes, hidden_dim, inter_dim, dropout_rate):\n",
    "    super(TClassifier, self).__init__()\n",
    "\n",
    "    self.layernorm = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "    self.dropout   = nn.Dropout(dropout_rate)\n",
    "\n",
    "    self.ffn       = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, inter_dim), nn.GELU(),\n",
    "    )\n",
    "    self.output    = nn.Linear(inter_dim, classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layernorm(x)\n",
    "    x = x[:, 0, :]\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    x = self.ffn(x)\n",
    "    logits = self.output(x)\n",
    "\n",
    "    return logits\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, classes, num_embeddings, seq_len, hidden_dim, qkv_dim, mlp_dim, num_heads, num_blocks, output_mlp_units, internal_dropout_rate=0.1, output_dropout_rate=0.0):\n",
    "    super(Transformer, self).__init__()\n",
    "    \n",
    "    self.embed_layer = TEmbedding(num_embeddings, hidden_dim, seq_len)\n",
    "    self.blocks      = nn.ModuleList([ TBlock(hidden_dim, qkv_dim, mlp_dim, num_heads, internal_dropout_rate) for _ in range(num_blocks) ])\n",
    "    self.classifier  = TClassifier(classes, hidden_dim, output_mlp_units, output_dropout_rate)\n",
    "\n",
    "  def forward(self, pixel_values):\n",
    "    additional_losses = []\n",
    "\n",
    "    x = self.embed_layer(pixel_values)\n",
    "    \n",
    "    for block in self.blocks:\n",
    "      x = block(x, additional_losses)\n",
    "    \n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return x, additional_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Vd0_2v1I5wcL",
    "outputId": "0ae0a9ec-8940-4936-d832-563486b6ad8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model 21019658 params, new model 21904394 params, ratio 1.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc7c4f4bf50>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def get_schedule(warmup_steps):\n",
    "  def lr_schedule(step):\n",
    "    return 1.0 * np.minimum(1.0, step / warmup_steps) / np.sqrt(np.maximum(step, warmup_steps))\n",
    "\n",
    "  return lr_schedule\n",
    "\n",
    "lr=0.005\n",
    "weight_decay=0.1\n",
    "warmup=1000\n",
    "\n",
    "\n",
    "def const_schedule(lr):\n",
    "  def lr_schedule(step):\n",
    "    return lr\n",
    "  return lr_schedule\n",
    "\n",
    "def training_setup():\n",
    "  model = model_factory()\n",
    "  criterion = nn.CrossEntropyLoss().cuda()\n",
    "  optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  schedule_func = get_schedule(warmup)\n",
    "  #schedule_func = const_schedule(1.0) #<--------- TEMPORARY\n",
    "  scheduler = LambdaLR(optimizer, schedule_func)\n",
    "\n",
    "  return model, criterion, optimizer, schedule_func, scheduler\n",
    "\n",
    "_, _, _, schedule_func, _ = training_setup()\n",
    "\n",
    "plt.plot([ lr * schedule_func(i) for i in range(15000) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PcXKwNA35yP3",
    "outputId": "0219c449-059c-4dea-98ea-9052545cee89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model 21019658 params, new model 21904394 params, ratio 1.04\n",
      "Epoch 0\n",
      "[====================] 50/50: - running_loss: 2.2577 - running_reg: 0.000000 - running_acc: 0.1724 - lr: 0.00001 - epoch_loss: 2.2789 - epoch_reg: 0.000000 - epoch_acc: 0.1544 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 77.5891 s\n",
      "Epoch 1\n",
      "[====================] 50/50: - running_loss: 2.2599 - running_reg: 0.000000 - running_acc: 0.1778 - lr: 0.00002 - epoch_loss: 2.2653 - epoch_reg: 0.000000 - epoch_acc: 0.1806 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.7719 s\n",
      "Epoch 2\n",
      "[====================] 50/50: - running_loss: 2.2756 - running_reg: 0.000000 - running_acc: 0.1526 - lr: 0.00002 - epoch_loss: 2.2677 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.7000 s\n",
      "Epoch 3\n",
      "[====================] 50/50: - running_loss: 2.2793 - running_reg: 0.000000 - running_acc: 0.1681 - lr: 0.00003 - epoch_loss: 2.2646 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6596 s\n",
      "Epoch 4\n",
      "[====================] 50/50: - running_loss: 2.2885 - running_reg: 0.000000 - running_acc: 0.1687 - lr: 0.00004 - epoch_loss: 2.2809 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.7257 s\n",
      "Epoch 5\n",
      "[====================] 50/50: - running_loss: 2.2750 - running_reg: 0.000000 - running_acc: 0.1691 - lr: 0.00005 - epoch_loss: 2.2757 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.7650 s\n",
      "Epoch 6\n",
      "[====================] 50/50: - running_loss: 2.2670 - running_reg: 0.000000 - running_acc: 0.1776 - lr: 0.00006 - epoch_loss: 2.2671 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6263 s\n",
      "Epoch 7\n",
      "[====================] 50/50: - running_loss: 2.2851 - running_reg: 0.000000 - running_acc: 0.1468 - lr: 0.00006 - epoch_loss: 2.2735 - epoch_reg: 0.000000 - epoch_acc: 0.1562 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.8114 s\n",
      "Epoch 8\n",
      "[====================] 50/50: - running_loss: 2.2797 - running_reg: 0.000000 - running_acc: 0.1589 - lr: 0.00007 - epoch_loss: 2.2785 - epoch_reg: 0.000000 - epoch_acc: 0.1600 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6800 s\n",
      "Epoch 9\n",
      "[====================] 50/50: - running_loss: 2.2768 - running_reg: 0.000000 - running_acc: 0.1716 - lr: 0.00008 - epoch_loss: 2.2721 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6900 s\n",
      "Epoch 10\n",
      "[====================] 50/50: - running_loss: 2.2677 - running_reg: 0.000000 - running_acc: 0.1645 - lr: 0.00009 - epoch_loss: 2.2600 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6562 s\n",
      "Epoch 11\n",
      "[====================] 50/50: - running_loss: 2.2788 - running_reg: 0.000000 - running_acc: 0.1692 - lr: 0.00009 - epoch_loss: 2.2583 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6343 s\n",
      "Epoch 12\n",
      "[====================] 50/50: - running_loss: 2.2843 - running_reg: 0.000000 - running_acc: 0.1403 - lr: 0.00010 - epoch_loss: 2.2874 - epoch_reg: 0.000000 - epoch_acc: 0.1538 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6381 s\n",
      "Epoch 13\n",
      "[====================] 50/50: - running_loss: 2.2320 - running_reg: 0.000000 - running_acc: 0.1857 - lr: 0.00011 - epoch_loss: 2.2480 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6181 s\n",
      "Epoch 14\n",
      "[====================] 50/50: - running_loss: 2.2565 - running_reg: 0.000000 - running_acc: 0.1841 - lr: 0.00012 - epoch_loss: 2.2637 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6285 s\n",
      "Epoch 15\n",
      "[====================] 50/50: - running_loss: 2.2791 - running_reg: 0.000000 - running_acc: 0.1636 - lr: 0.00013 - epoch_loss: 2.2772 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6494 s\n",
      "Epoch 16\n",
      "[====================] 50/50: - running_loss: 2.2459 - running_reg: 0.000000 - running_acc: 0.1752 - lr: 0.00013 - epoch_loss: 2.2352 - epoch_reg: 0.000000 - epoch_acc: 0.1869 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6243 s\n",
      "Epoch 17\n",
      "[====================] 50/50: - running_loss: 2.2651 - running_reg: 0.000000 - running_acc: 0.1739 - lr: 0.00014 - epoch_loss: 2.2551 - epoch_reg: 0.000000 - epoch_acc: 0.1850 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6102 s\n",
      "Epoch 18\n",
      "[====================] 50/50: - running_loss: 2.2632 - running_reg: 0.000000 - running_acc: 0.1540 - lr: 0.00015 - epoch_loss: 2.2664 - epoch_reg: 0.000000 - epoch_acc: 0.1513 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5618 s\n",
      "Epoch 19\n",
      "[====================] 50/50: - running_loss: 2.2528 - running_reg: 0.000000 - running_acc: 0.1671 - lr: 0.00016 - epoch_loss: 2.2699 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6108 s\n",
      "Epoch 20\n",
      "[====================] 50/50: - running_loss: 2.2771 - running_reg: 0.000000 - running_acc: 0.1706 - lr: 0.00015 - epoch_loss: 2.2778 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6344 s\n",
      "Epoch 21\n",
      "[====================] 50/50: - running_loss: 2.2629 - running_reg: 0.000000 - running_acc: 0.1852 - lr: 0.00015 - epoch_loss: 2.2714 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6237 s\n",
      "Epoch 22\n",
      "[====================] 50/50: - running_loss: 2.2550 - running_reg: 0.000000 - running_acc: 0.1620 - lr: 0.00015 - epoch_loss: 2.2665 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.4497 s\n",
      "Epoch 23\n",
      "[====================] 50/50: - running_loss: 2.2375 - running_reg: 0.000000 - running_acc: 0.1857 - lr: 0.00014 - epoch_loss: 2.2430 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5601 s\n",
      "Epoch 24\n",
      "[====================] 50/50: - running_loss: 2.2771 - running_reg: 0.000000 - running_acc: 0.1705 - lr: 0.00014 - epoch_loss: 2.2640 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6073 s\n",
      "Epoch 25\n",
      "[====================] 50/50: - running_loss: 2.2550 - running_reg: 0.000000 - running_acc: 0.1775 - lr: 0.00014 - epoch_loss: 2.2492 - epoch_reg: 0.000000 - epoch_acc: 0.1863 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6339 s\n",
      "Epoch 26\n",
      "[====================] 50/50: - running_loss: 2.2465 - running_reg: 0.000000 - running_acc: 0.1811 - lr: 0.00014 - epoch_loss: 2.2575 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6367 s\n",
      "Epoch 27\n",
      "[====================] 50/50: - running_loss: 2.2469 - running_reg: 0.000000 - running_acc: 0.1785 - lr: 0.00013 - epoch_loss: 2.2550 - epoch_reg: 0.000000 - epoch_acc: 0.1844 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6577 s\n",
      "Epoch 28\n",
      "[====================] 50/50: - running_loss: 2.2585 - running_reg: 0.000000 - running_acc: 0.1719 - lr: 0.00013 - epoch_loss: 2.2573 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6491 s\n",
      "Epoch 29\n",
      "[====================] 50/50: - running_loss: 2.2248 - running_reg: 0.000000 - running_acc: 0.1869 - lr: 0.00013 - epoch_loss: 2.2387 - epoch_reg: 0.000000 - epoch_acc: 0.1844 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.7568 s\n",
      "Epoch 30\n",
      "[====================] 50/50: - running_loss: 2.2623 - running_reg: 0.000000 - running_acc: 0.1594 - lr: 0.00013 - epoch_loss: 2.2483 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5804 s\n",
      "Epoch 31\n",
      "[====================] 50/50: - running_loss: 2.2733 - running_reg: 0.000000 - running_acc: 0.1511 - lr: 0.00013 - epoch_loss: 2.2601 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5353 s\n",
      "Epoch 32\n",
      "[====================] 50/50: - running_loss: 2.2615 - running_reg: 0.000000 - running_acc: 0.1691 - lr: 0.00012 - epoch_loss: 2.2527 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.7711 s\n",
      "Epoch 33\n",
      "[====================] 50/50: - running_loss: 2.2675 - running_reg: 0.000000 - running_acc: 0.1445 - lr: 0.00012 - epoch_loss: 2.2636 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5969 s\n",
      "Epoch 34\n",
      "[====================] 50/50: - running_loss: 2.2632 - running_reg: 0.000000 - running_acc: 0.1609 - lr: 0.00012 - epoch_loss: 2.2668 - epoch_reg: 0.000000 - epoch_acc: 0.1619 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5735 s\n",
      "Epoch 35\n",
      "[====================] 50/50: - running_loss: 2.2520 - running_reg: 0.000000 - running_acc: 0.1724 - lr: 0.00012 - epoch_loss: 2.2576 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.3803 s\n",
      "Epoch 36\n",
      "[====================] 50/50: - running_loss: 2.2663 - running_reg: 0.000000 - running_acc: 0.1564 - lr: 0.00012 - epoch_loss: 2.2596 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.4980 s\n",
      "Epoch 37\n",
      "[====================] 50/50: - running_loss: 2.2698 - running_reg: 0.000000 - running_acc: 0.1596 - lr: 0.00011 - epoch_loss: 2.2547 - epoch_reg: 0.000000 - epoch_acc: 0.1806 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.4980 s\n",
      "Epoch 38\n",
      "[====================] 50/50: - running_loss: 2.2521 - running_reg: 0.000000 - running_acc: 0.1747 - lr: 0.00011 - epoch_loss: 2.2503 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.7931 s\n",
      "Epoch 39\n",
      "[====================] 50/50: - running_loss: 2.2539 - running_reg: 0.000000 - running_acc: 0.1820 - lr: 0.00011 - epoch_loss: 2.2510 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.7412 s\n",
      "Epoch 40\n",
      "[====================] 50/50: - running_loss: 2.2505 - running_reg: 0.000000 - running_acc: 0.1613 - lr: 0.00011 - epoch_loss: 2.2520 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6317 s\n",
      "Epoch 41\n",
      "[====================] 50/50: - running_loss: 2.2557 - running_reg: 0.000000 - running_acc: 0.1648 - lr: 0.00011 - epoch_loss: 2.2593 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5895 s\n",
      "Epoch 42\n",
      "[====================] 50/50: - running_loss: 2.2585 - running_reg: 0.000000 - running_acc: 0.1595 - lr: 0.00011 - epoch_loss: 2.2578 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6989 s\n",
      "Epoch 43\n",
      "[====================] 50/50: - running_loss: 2.2584 - running_reg: 0.000000 - running_acc: 0.1505 - lr: 0.00011 - epoch_loss: 2.2485 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5634 s\n",
      "Epoch 44\n",
      "[====================] 50/50: - running_loss: 2.2415 - running_reg: 0.000000 - running_acc: 0.1670 - lr: 0.00011 - epoch_loss: 2.2367 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5771 s\n",
      "Epoch 45\n",
      "[====================] 50/50: - running_loss: 2.2839 - running_reg: 0.000000 - running_acc: 0.1203 - lr: 0.00010 - epoch_loss: 2.2647 - epoch_reg: 0.000000 - epoch_acc: 0.1406 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.4572 s\n",
      "Epoch 46\n",
      "[====================] 50/50: - running_loss: 2.2424 - running_reg: 0.000000 - running_acc: 0.1734 - lr: 0.00010 - epoch_loss: 2.2550 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.3098 s\n",
      "Epoch 47\n",
      "[====================] 50/50: - running_loss: 2.2527 - running_reg: 0.000000 - running_acc: 0.1824 - lr: 0.00010 - epoch_loss: 2.2532 - epoch_reg: 0.000000 - epoch_acc: 0.1825 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.6652 s\n",
      "Epoch 48\n",
      "[====================] 50/50: - running_loss: 2.2395 - running_reg: 0.000000 - running_acc: 0.1828 - lr: 0.00010 - epoch_loss: 2.2543 - epoch_reg: 0.000000 - epoch_acc: 0.1737 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.4558 s\n",
      "Epoch 49\n",
      "[====================] 50/50: - running_loss: 2.2449 - running_reg: 0.000000 - running_acc: 0.1598 - lr: 0.00010 - epoch_loss: 2.2567 - epoch_reg: 0.000000 - epoch_acc: 0.1587 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.2815 s\n",
      "Epoch 50\n",
      "[====================] 50/50: - running_loss: 2.2733 - running_reg: 0.000000 - running_acc: 0.1622 - lr: 0.00010 - epoch_loss: 2.2533 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.2971 s\n",
      "Epoch 51\n",
      "[====================] 50/50: - running_loss: 2.2538 - running_reg: 0.000000 - running_acc: 0.1617 - lr: 0.00010 - epoch_loss: 2.2495 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.3146 s\n",
      "Epoch 52\n",
      "[====================] 50/50: - running_loss: 2.2517 - running_reg: 0.000000 - running_acc: 0.1781 - lr: 0.00010 - epoch_loss: 2.2489 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.2561 s\n",
      "Epoch 53\n",
      "[====================] 50/50: - running_loss: 2.2562 - running_reg: 0.000000 - running_acc: 0.1667 - lr: 0.00010 - epoch_loss: 2.2635 - epoch_reg: 0.000000 - epoch_acc: 0.1663 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.2977 s\n",
      "Epoch 54\n",
      "[====================] 50/50: - running_loss: 2.2689 - running_reg: 0.000000 - running_acc: 0.1625 - lr: 0.00010 - epoch_loss: 2.2586 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.4800 s\n",
      "Epoch 55\n",
      "[====================] 50/50: - running_loss: 2.2796 - running_reg: 0.000000 - running_acc: 0.1589 - lr: 0.00009 - epoch_loss: 2.2698 - epoch_reg: 0.000000 - epoch_acc: 0.1569 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.4487 s\n",
      "Epoch 56\n",
      "[====================] 50/50: - running_loss: 2.2564 - running_reg: 0.000000 - running_acc: 0.1655 - lr: 0.00009 - epoch_loss: 2.2577 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.4695 s\n",
      "Epoch 57\n",
      "[====================] 50/50: - running_loss: 2.2514 - running_reg: 0.000000 - running_acc: 0.1743 - lr: 0.00009 - epoch_loss: 2.2508 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5470 s\n",
      "Epoch 58\n",
      "[====================] 50/50: - running_loss: 2.2422 - running_reg: 0.000000 - running_acc: 0.1688 - lr: 0.00009 - epoch_loss: 2.2479 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 75.5637 s\n",
      "Epoch 59\n",
      "[====================] 50/50: - running_loss: 2.2513 - running_reg: 0.000000 - running_acc: 0.1650 - lr: 0.00009 - epoch_loss: 2.2298 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 74.1919 s\n",
      "Epoch 60\n",
      "[====================] 50/50: - running_loss: 2.2623 - running_reg: 0.000000 - running_acc: 0.1673 - lr: 0.00009 - epoch_loss: 2.2592 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 2.2620 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 109.8171 s\n",
      "Epoch 61\n",
      "[====================] 50/50: - running_loss: 2.2611 - running_reg: 0.000000 - running_acc: 0.1658 - lr: 0.00009 - epoch_loss: 2.2509 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2607 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.6208 s\n",
      "Epoch 62\n",
      "[====================] 50/50: - running_loss: 2.2612 - running_reg: 0.000000 - running_acc: 0.1653 - lr: 0.00009 - epoch_loss: 2.2477 - epoch_reg: 0.000000 - epoch_acc: 0.1612 - valid_loss: 2.2587 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.2712 s\n",
      "Epoch 63\n",
      "[====================] 50/50: - running_loss: 2.2650 - running_reg: 0.000000 - running_acc: 0.1609 - lr: 0.00009 - epoch_loss: 2.2569 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 2.2562 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.4960 s\n",
      "Epoch 64\n",
      "[====================] 50/50: - running_loss: 2.2707 - running_reg: 0.000000 - running_acc: 0.1498 - lr: 0.00009 - epoch_loss: 2.2650 - epoch_reg: 0.000000 - epoch_acc: 0.1531 - valid_loss: 2.2655 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.4940 s\n",
      "Epoch 65\n",
      "[====================] 50/50: - running_loss: 2.2376 - running_reg: 0.000000 - running_acc: 0.1820 - lr: 0.00009 - epoch_loss: 2.2423 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2581 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.5867 s\n",
      "Epoch 66\n",
      "[====================] 50/50: - running_loss: 2.2393 - running_reg: 0.000000 - running_acc: 0.1738 - lr: 0.00009 - epoch_loss: 2.2489 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2663 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.5812 s\n",
      "Epoch 67\n",
      "[====================] 50/50: - running_loss: 2.2442 - running_reg: 0.000000 - running_acc: 0.1667 - lr: 0.00009 - epoch_loss: 2.2560 - epoch_reg: 0.000000 - epoch_acc: 0.1600 - valid_loss: 2.2608 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.6143 s\n",
      "Epoch 68\n",
      "[====================] 50/50: - running_loss: 2.2565 - running_reg: 0.000000 - running_acc: 0.1695 - lr: 0.00009 - epoch_loss: 2.2639 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2564 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.5602 s\n",
      "Epoch 69\n",
      "[====================] 50/50: - running_loss: 2.2450 - running_reg: 0.000000 - running_acc: 0.1947 - lr: 0.00008 - epoch_loss: 2.2535 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 2.2570 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.5782 s\n",
      "Epoch 70\n",
      "[====================] 50/50: - running_loss: 2.2616 - running_reg: 0.000000 - running_acc: 0.1640 - lr: 0.00008 - epoch_loss: 2.2560 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2572 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.6376 s\n",
      "Epoch 71\n",
      "[====================] 50/50: - running_loss: 2.2531 - running_reg: 0.000000 - running_acc: 0.1681 - lr: 0.00008 - epoch_loss: 2.2538 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2569 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.5834 s\n",
      "Epoch 72\n",
      "[====================] 50/50: - running_loss: 2.2489 - running_reg: 0.000000 - running_acc: 0.1858 - lr: 0.00008 - epoch_loss: 2.2550 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 2.2633 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.6956 s\n",
      "Epoch 73\n",
      "[====================] 50/50: - running_loss: 2.2601 - running_reg: 0.000000 - running_acc: 0.1659 - lr: 0.00008 - epoch_loss: 2.2408 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2579 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.7622 s\n",
      "Epoch 74\n",
      "[====================] 50/50: - running_loss: 2.2176 - running_reg: 0.000000 - running_acc: 0.1952 - lr: 0.00008 - epoch_loss: 2.2418 - epoch_reg: 0.000000 - epoch_acc: 0.1825 - valid_loss: 2.2658 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.7135 s\n",
      "Epoch 75\n",
      "[====================] 50/50: - running_loss: 2.2428 - running_reg: 0.000000 - running_acc: 0.1698 - lr: 0.00008 - epoch_loss: 2.2443 - epoch_reg: 0.000000 - epoch_acc: 0.1844 - valid_loss: 2.2575 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.6070 s\n",
      "Epoch 76\n",
      "[====================] 50/50: - running_loss: 2.2656 - running_reg: 0.000000 - running_acc: 0.1694 - lr: 0.00008 - epoch_loss: 2.2669 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2604 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.7837 s\n",
      "Epoch 77\n",
      "[====================] 50/50: - running_loss: 2.2455 - running_reg: 0.000000 - running_acc: 0.1619 - lr: 0.00008 - epoch_loss: 2.2597 - epoch_reg: 0.000000 - epoch_acc: 0.1619 - valid_loss: 2.2640 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.0186 s\n",
      "Epoch 78\n",
      "[====================] 50/50: - running_loss: 2.2507 - running_reg: 0.000000 - running_acc: 0.1690 - lr: 0.00008 - epoch_loss: 2.2579 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2610 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.9808 s\n",
      "Epoch 79\n",
      "[====================] 50/50: - running_loss: 2.2698 - running_reg: 0.000000 - running_acc: 0.1407 - lr: 0.00008 - epoch_loss: 2.2678 - epoch_reg: 0.000000 - epoch_acc: 0.1475 - valid_loss: 2.2552 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.0097 s\n",
      "Epoch 80\n",
      "[====================] 50/50: - running_loss: 2.2382 - running_reg: 0.000000 - running_acc: 0.1741 - lr: 0.00008 - epoch_loss: 2.2478 - epoch_reg: 0.000000 - epoch_acc: 0.1844 - valid_loss: 2.2560 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.0242 s\n",
      "Epoch 81\n",
      "[====================] 50/50: - running_loss: 2.2429 - running_reg: 0.000000 - running_acc: 0.1626 - lr: 0.00008 - epoch_loss: 2.2556 - epoch_reg: 0.000000 - epoch_acc: 0.1581 - valid_loss: 2.2601 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.0020 s\n",
      "Epoch 82\n",
      "[====================] 50/50: - running_loss: 2.2536 - running_reg: 0.000000 - running_acc: 0.1815 - lr: 0.00008 - epoch_loss: 2.2499 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2628 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.5142 s\n",
      "Epoch 83\n",
      "[====================] 50/50: - running_loss: 2.2446 - running_reg: 0.000000 - running_acc: 0.1858 - lr: 0.00008 - epoch_loss: 2.2600 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2556 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.5691 s\n",
      "Epoch 84\n",
      "[====================] 50/50: - running_loss: 2.2324 - running_reg: 0.000000 - running_acc: 0.1933 - lr: 0.00008 - epoch_loss: 2.2466 - epoch_reg: 0.000000 - epoch_acc: 0.1813 - valid_loss: 2.2614 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.5684 s\n",
      "Epoch 85\n",
      "[====================] 50/50: - running_loss: 2.2577 - running_reg: 0.000000 - running_acc: 0.1838 - lr: 0.00008 - epoch_loss: 2.2544 - epoch_reg: 0.000000 - epoch_acc: 0.1731 - valid_loss: 2.2594 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.7793 s\n",
      "Epoch 86\n",
      "[====================] 50/50: - running_loss: 2.2536 - running_reg: 0.000000 - running_acc: 0.1791 - lr: 0.00008 - epoch_loss: 2.2508 - epoch_reg: 0.000000 - epoch_acc: 0.1725 - valid_loss: 2.2570 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.8295 s\n",
      "Epoch 87\n",
      "[====================] 50/50: - running_loss: 2.2370 - running_reg: 0.000000 - running_acc: 0.1914 - lr: 0.00008 - epoch_loss: 2.2490 - epoch_reg: 0.000000 - epoch_acc: 0.1781 - valid_loss: 2.2553 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.8119 s\n",
      "Epoch 88\n",
      "[====================] 50/50: - running_loss: 2.2622 - running_reg: 0.000000 - running_acc: 0.1633 - lr: 0.00007 - epoch_loss: 2.2585 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2582 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.8089 s\n",
      "Epoch 89\n",
      "[====================] 50/50: - running_loss: 2.2429 - running_reg: 0.000000 - running_acc: 0.1757 - lr: 0.00007 - epoch_loss: 2.2380 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2631 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.7557 s\n",
      "Epoch 90\n",
      "[====================] 50/50: - running_loss: 2.2600 - running_reg: 0.000000 - running_acc: 0.1537 - lr: 0.00007 - epoch_loss: 2.2616 - epoch_reg: 0.000000 - epoch_acc: 0.1488 - valid_loss: 2.2570 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.9037 s\n",
      "Epoch 91\n",
      "[====================] 50/50: - running_loss: 2.2610 - running_reg: 0.000000 - running_acc: 0.1474 - lr: 0.00007 - epoch_loss: 2.2517 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 2.2595 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 141.9534 s\n",
      "Epoch 92\n",
      "[====================] 50/50: - running_loss: 2.2488 - running_reg: 0.000000 - running_acc: 0.1697 - lr: 0.00007 - epoch_loss: 2.2434 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2585 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.6549 s\n",
      "Epoch 93\n",
      "[====================] 50/50: - running_loss: 2.2130 - running_reg: 0.000000 - running_acc: 0.1792 - lr: 0.00007 - epoch_loss: 2.2371 - epoch_reg: 0.000000 - epoch_acc: 0.1725 - valid_loss: 2.2607 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.8233 s\n",
      "Epoch 94\n",
      "[====================] 50/50: - running_loss: 2.2431 - running_reg: 0.000000 - running_acc: 0.1797 - lr: 0.00007 - epoch_loss: 2.2529 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 2.2611 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.5756 s\n",
      "Epoch 95\n",
      "[====================] 50/50: - running_loss: 2.2622 - running_reg: 0.000000 - running_acc: 0.1665 - lr: 0.00007 - epoch_loss: 2.2590 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 2.2550 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.5856 s\n",
      "Epoch 96\n",
      "[====================] 50/50: - running_loss: 2.2532 - running_reg: 0.000000 - running_acc: 0.1661 - lr: 0.00007 - epoch_loss: 2.2626 - epoch_reg: 0.000000 - epoch_acc: 0.1575 - valid_loss: 2.2578 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.5513 s\n",
      "Epoch 97\n",
      "[====================] 50/50: - running_loss: 2.2469 - running_reg: 0.000000 - running_acc: 0.1750 - lr: 0.00007 - epoch_loss: 2.2493 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2564 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.4496 s\n",
      "Epoch 98\n",
      "[====================] 50/50: - running_loss: 2.2620 - running_reg: 0.000000 - running_acc: 0.1512 - lr: 0.00007 - epoch_loss: 2.2497 - epoch_reg: 0.000000 - epoch_acc: 0.1538 - valid_loss: 2.2559 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.4464 s\n",
      "Epoch 99\n",
      "[====================] 50/50: - running_loss: 2.2573 - running_reg: 0.000000 - running_acc: 0.1722 - lr: 0.00007 - epoch_loss: 2.2451 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 2.2579 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.4212 s\n",
      "Epoch 100\n",
      "[====================] 50/50: - running_loss: 2.2767 - running_reg: 0.000000 - running_acc: 0.1556 - lr: 0.00007 - epoch_loss: 2.2657 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2562 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.7004 s\n",
      "Epoch 101\n",
      "[====================] 50/50: - running_loss: 2.2505 - running_reg: 0.000000 - running_acc: 0.1725 - lr: 0.00007 - epoch_loss: 2.2539 - epoch_reg: 0.000000 - epoch_acc: 0.1694 - valid_loss: 2.2552 - valid_reg: 0.000000 - valid_acc: 0.1695 - epoch_time: 107.0141 s\n",
      "Epoch 102\n",
      "[====================] 50/50: - running_loss: 2.2560 - running_reg: 0.000000 - running_acc: 0.1783 - lr: 0.00007 - epoch_loss: 2.2423 - epoch_reg: 0.000000 - epoch_acc: 0.1844 - valid_loss: 2.2575 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.9328 s\n",
      "Epoch 103\n",
      "[====================] 50/50: - running_loss: 2.2449 - running_reg: 0.000000 - running_acc: 0.1977 - lr: 0.00007 - epoch_loss: 2.2600 - epoch_reg: 0.000000 - epoch_acc: 0.1744 - valid_loss: 2.2660 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.2365 s\n",
      "Epoch 104\n",
      "[====================] 50/50: - running_loss: 2.2424 - running_reg: 0.000000 - running_acc: 0.1963 - lr: 0.00007 - epoch_loss: 2.2486 - epoch_reg: 0.000000 - epoch_acc: 0.1806 - valid_loss: 2.2649 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.9314 s\n",
      "Epoch 105\n",
      "[====================] 50/50: - running_loss: 2.2472 - running_reg: 0.000000 - running_acc: 0.1847 - lr: 0.00007 - epoch_loss: 2.2495 - epoch_reg: 0.000000 - epoch_acc: 0.1713 - valid_loss: 2.2627 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.9345 s\n",
      "Epoch 106\n",
      "[====================] 50/50: - running_loss: 2.2569 - running_reg: 0.000000 - running_acc: 0.1763 - lr: 0.00007 - epoch_loss: 2.2489 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2612 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.7717 s\n",
      "Epoch 107\n",
      "[====================] 50/50: - running_loss: 2.2394 - running_reg: 0.000000 - running_acc: 0.1982 - lr: 0.00007 - epoch_loss: 2.2498 - epoch_reg: 0.000000 - epoch_acc: 0.1869 - valid_loss: 2.2597 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.6934 s\n",
      "Epoch 108\n",
      "[====================] 50/50: - running_loss: 2.2547 - running_reg: 0.000000 - running_acc: 0.1582 - lr: 0.00007 - epoch_loss: 2.2424 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 2.2576 - valid_reg: 0.000000 - valid_acc: 0.1725 - epoch_time: 108.2089 s\n",
      "Epoch 109\n",
      "[====================] 50/50: - running_loss: 2.2506 - running_reg: 0.000000 - running_acc: 0.1823 - lr: 0.00007 - epoch_loss: 2.2448 - epoch_reg: 0.000000 - epoch_acc: 0.1700 - valid_loss: 2.2643 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.3969 s\n",
      "Epoch 110\n",
      "[====================] 50/50: - running_loss: 2.2840 - running_reg: 0.000000 - running_acc: 0.1372 - lr: 0.00007 - epoch_loss: 2.2696 - epoch_reg: 0.000000 - epoch_acc: 0.1506 - valid_loss: 2.2640 - valid_reg: 0.000000 - valid_acc: 0.1805 - epoch_time: 108.5117 s\n",
      "Epoch 111\n",
      "[====================] 50/50: - running_loss: 2.2334 - running_reg: 0.000000 - running_acc: 0.1936 - lr: 0.00007 - epoch_loss: 2.2424 - epoch_reg: 0.000000 - epoch_acc: 0.1762 - valid_loss: 2.2617 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.4590 s\n",
      "Epoch 112\n",
      "[====================] 50/50: - running_loss: 2.2391 - running_reg: 0.000000 - running_acc: 0.1855 - lr: 0.00007 - epoch_loss: 2.2521 - epoch_reg: 0.000000 - epoch_acc: 0.1656 - valid_loss: 2.2569 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.4239 s\n",
      "Epoch 113\n",
      "[====================] 50/50: - running_loss: 2.2614 - running_reg: 0.000000 - running_acc: 0.1532 - lr: 0.00007 - epoch_loss: 2.2644 - epoch_reg: 0.000000 - epoch_acc: 0.1594 - valid_loss: 2.2637 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.4193 s\n",
      "Epoch 114\n",
      "[====================] 50/50: - running_loss: 2.2638 - running_reg: 0.000000 - running_acc: 0.1649 - lr: 0.00007 - epoch_loss: 2.2659 - epoch_reg: 0.000000 - epoch_acc: 0.1619 - valid_loss: 2.2551 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.4208 s\n",
      "Epoch 115\n",
      "[====================] 50/50: - running_loss: 2.2351 - running_reg: 0.000000 - running_acc: 0.1604 - lr: 0.00007 - epoch_loss: 2.2585 - epoch_reg: 0.000000 - epoch_acc: 0.1631 - valid_loss: 2.2575 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.3287 s\n",
      "Epoch 116\n",
      "[====================] 50/50: - running_loss: 2.2476 - running_reg: 0.000000 - running_acc: 0.1691 - lr: 0.00007 - epoch_loss: 2.2465 - epoch_reg: 0.000000 - epoch_acc: 0.1644 - valid_loss: 2.2566 - valid_reg: 0.000000 - valid_acc: 0.1735 - epoch_time: 107.3192 s\n",
      "Epoch 117\n",
      "[====================] 50/50: - running_loss: 2.2531 - running_reg: 0.000000 - running_acc: 0.1654 - lr: 0.00007 - epoch_loss: 2.2477 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 2.2590 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.3994 s\n",
      "Epoch 118\n",
      "[====================] 50/50: - running_loss: 2.2788 - running_reg: 0.000000 - running_acc: 0.1519 - lr: 0.00006 - epoch_loss: 2.2599 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2568 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.3215 s\n",
      "Epoch 119\n",
      "[====================] 50/50: - running_loss: 2.2240 - running_reg: 0.000000 - running_acc: 0.1895 - lr: 0.00006 - epoch_loss: 2.2385 - epoch_reg: 0.000000 - epoch_acc: 0.1750 - valid_loss: 2.2576 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 105.9291 s\n",
      "Epoch 120\n",
      "[====================] 50/50: - running_loss: 2.2261 - running_reg: 0.000000 - running_acc: 0.1653 - lr: 0.00006 - epoch_loss: 2.2341 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2639 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 109.2098 s\n",
      "Epoch 121\n",
      "[====================] 50/50: - running_loss: 2.2454 - running_reg: 0.000000 - running_acc: 0.1649 - lr: 0.00006 - epoch_loss: 2.2576 - epoch_reg: 0.000000 - epoch_acc: 0.1600 - valid_loss: 2.2585 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.3458 s\n",
      "Epoch 122\n",
      "[====================] 50/50: - running_loss: 2.2689 - running_reg: 0.000000 - running_acc: 0.1596 - lr: 0.00006 - epoch_loss: 2.2604 - epoch_reg: 0.000000 - epoch_acc: 0.1787 - valid_loss: 2.2572 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.4502 s\n",
      "Epoch 123\n",
      "[====================] 50/50: - running_loss: 2.2385 - running_reg: 0.000000 - running_acc: 0.1765 - lr: 0.00006 - epoch_loss: 2.2502 - epoch_reg: 0.000000 - epoch_acc: 0.1675 - valid_loss: 2.2572 - valid_reg: 0.000000 - valid_acc: 0.1745 - epoch_time: 107.5540 s\n",
      "Epoch 124\n",
      "[====================] 50/50: - running_loss: 2.2775 - running_reg: 0.000000 - running_acc: 0.1447 - lr: 0.00006 - epoch_loss: 2.2714 - epoch_reg: 0.000000 - epoch_acc: 0.1569 - valid_loss: 2.2591 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.3874 s\n",
      "Epoch 125\n",
      "[====================] 50/50: - running_loss: 2.2486 - running_reg: 0.000000 - running_acc: 0.1550 - lr: 0.00006 - epoch_loss: 2.2532 - epoch_reg: 0.000000 - epoch_acc: 0.1550 - valid_loss: 2.2565 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.4621 s\n",
      "Epoch 126\n",
      "[====================] 50/50: - running_loss: 2.2648 - running_reg: 0.000000 - running_acc: 0.1484 - lr: 0.00006 - epoch_loss: 2.2573 - epoch_reg: 0.000000 - epoch_acc: 0.1569 - valid_loss: 2.2626 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.0066 s\n",
      "Epoch 127\n",
      "[====================] 50/50: - running_loss: 2.2388 - running_reg: 0.000000 - running_acc: 0.1822 - lr: 0.00006 - epoch_loss: 2.2417 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2599 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.7547 s\n",
      "Epoch 128\n",
      "[====================] 50/50: - running_loss: 2.2606 - running_reg: 0.000000 - running_acc: 0.1641 - lr: 0.00006 - epoch_loss: 2.2687 - epoch_reg: 0.000000 - epoch_acc: 0.1587 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.7832 s\n",
      "Epoch 129\n",
      "[====================] 50/50: - running_loss: 2.2508 - running_reg: 0.000000 - running_acc: 0.1868 - lr: 0.00006 - epoch_loss: 2.2523 - epoch_reg: 0.000000 - epoch_acc: 0.1775 - valid_loss: 2.2686 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 106.7504 s\n",
      "Epoch 130\n",
      "[====================] 50/50: - running_loss: 2.2650 - running_reg: 0.000000 - running_acc: 0.1680 - lr: 0.00006 - epoch_loss: 2.2678 - epoch_reg: 0.000000 - epoch_acc: 0.1600 - valid_loss: 2.2602 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 106.7127 s\n",
      "Epoch 131\n",
      "[====================] 50/50: - running_loss: 2.2588 - running_reg: 0.000000 - running_acc: 0.1668 - lr: 0.00006 - epoch_loss: 2.2535 - epoch_reg: 0.000000 - epoch_acc: 0.1844 - valid_loss: 2.2581 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.2490 s\n",
      "Epoch 132\n",
      "[====================] 50/50: - running_loss: 2.2475 - running_reg: 0.000000 - running_acc: 0.1674 - lr: 0.00006 - epoch_loss: 2.2412 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2599 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.4216 s\n",
      "Epoch 133\n",
      "[====================] 50/50: - running_loss: 2.2530 - running_reg: 0.000000 - running_acc: 0.1899 - lr: 0.00006 - epoch_loss: 2.2590 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2588 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.3005 s\n",
      "Epoch 134\n",
      "[====================] 50/50: - running_loss: 2.2332 - running_reg: 0.000000 - running_acc: 0.1917 - lr: 0.00006 - epoch_loss: 2.2422 - epoch_reg: 0.000000 - epoch_acc: 0.1919 - valid_loss: 2.2594 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.2987 s\n",
      "Epoch 135\n",
      "[====================] 50/50: - running_loss: 2.2620 - running_reg: 0.000000 - running_acc: 0.1478 - lr: 0.00006 - epoch_loss: 2.2570 - epoch_reg: 0.000000 - epoch_acc: 0.1500 - valid_loss: 2.2576 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.3307 s\n",
      "Epoch 136\n",
      "[====================] 50/50: - running_loss: 2.2578 - running_reg: 0.000000 - running_acc: 0.1728 - lr: 0.00006 - epoch_loss: 2.2618 - epoch_reg: 0.000000 - epoch_acc: 0.1669 - valid_loss: 2.2571 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.3741 s\n",
      "Epoch 137\n",
      "[====================] 50/50: - running_loss: 2.2547 - running_reg: 0.000000 - running_acc: 0.1744 - lr: 0.00006 - epoch_loss: 2.2586 - epoch_reg: 0.000000 - epoch_acc: 0.1719 - valid_loss: 2.2544 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.3597 s\n",
      "Epoch 138\n",
      "[====================] 50/50: - running_loss: 2.2550 - running_reg: 0.000000 - running_acc: 0.1582 - lr: 0.00006 - epoch_loss: 2.2439 - epoch_reg: 0.000000 - epoch_acc: 0.1775 - valid_loss: 2.2574 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.3403 s\n",
      "Epoch 139\n",
      "[====================] 50/50: - running_loss: 2.2426 - running_reg: 0.000000 - running_acc: 0.1917 - lr: 0.00006 - epoch_loss: 2.2494 - epoch_reg: 0.000000 - epoch_acc: 0.1794 - valid_loss: 2.2618 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.2974 s\n",
      "Epoch 140\n",
      "[====================] 50/50: - running_loss: 2.2413 - running_reg: 0.000000 - running_acc: 0.1642 - lr: 0.00006 - epoch_loss: 2.2373 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2579 - valid_reg: 0.000000 - valid_acc: 0.1505 - epoch_time: 107.3263 s\n",
      "Epoch 141\n",
      "[====================] 50/50: - running_loss: 2.2563 - running_reg: 0.000000 - running_acc: 0.1642 - lr: 0.00006 - epoch_loss: 2.2473 - epoch_reg: 0.000000 - epoch_acc: 0.1769 - valid_loss: 2.2565 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.2697 s\n",
      "Epoch 142\n",
      "[====================] 50/50: - running_loss: 2.2302 - running_reg: 0.000000 - running_acc: 0.1835 - lr: 0.00006 - epoch_loss: 2.2464 - epoch_reg: 0.000000 - epoch_acc: 0.1681 - valid_loss: 2.2592 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.3562 s\n",
      "Epoch 143\n",
      "[====================] 50/50: - running_loss: 2.2415 - running_reg: 0.000000 - running_acc: 0.1801 - lr: 0.00006 - epoch_loss: 2.2512 - epoch_reg: 0.000000 - epoch_acc: 0.1706 - valid_loss: 2.2573 - valid_reg: 0.000000 - valid_acc: 0.1800 - epoch_time: 107.3423 s\n",
      "Epoch 144\n",
      "[====================] 50/50: - running_loss: 2.2506 - running_reg: 0.000000 - running_acc: 0.1697 - lr: 0.00006 - epoch_loss: 2.2464 - epoch_reg: 0.000000 - epoch_acc: 0.1625 - valid_loss: 2.2559 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.2338 s\n",
      "Epoch 145\n",
      "[====================] 50/50: - running_loss: 2.2681 - running_reg: 0.000000 - running_acc: 0.1724 - lr: 0.00006 - epoch_loss: 2.2565 - epoch_reg: 0.000000 - epoch_acc: 0.1756 - valid_loss: 2.2597 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.3138 s\n",
      "Epoch 146\n",
      "[====================] 50/50: - running_loss: 2.2708 - running_reg: 0.000000 - running_acc: 0.1559 - lr: 0.00006 - epoch_loss: 2.2541 - epoch_reg: 0.000000 - epoch_acc: 0.1606 - valid_loss: 2.2583 - valid_reg: 0.000000 - valid_acc: 0.1595 - epoch_time: 107.2970 s\n",
      "Epoch 147\n",
      "[====================] 50/50: - running_loss: 2.2592 - running_reg: 0.000000 - running_acc: 0.1718 - lr: 0.00006 - epoch_loss: 2.2513 - epoch_reg: 0.000000 - epoch_acc: 0.1650 - valid_loss: 2.2566 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.2934 s\n",
      "Epoch 148\n",
      "[====================] 50/50: - running_loss: 2.2380 - running_reg: 0.000000 - running_acc: 0.1800 - lr: 0.00006 - epoch_loss: 2.2374 - epoch_reg: 0.000000 - epoch_acc: 0.1831 - valid_loss: 2.2584 - valid_reg: 0.000000 - valid_acc: 0.1735 - epoch_time: 107.3131 s\n",
      "Epoch 149\n",
      "[====================] 50/50: - running_loss: 2.2407 - running_reg: 0.000000 - running_acc: 0.1807 - lr: 0.00006 - epoch_loss: 2.2434 - epoch_reg: 0.000000 - epoch_acc: 0.1850 - valid_loss: 2.2568 - valid_reg: 0.000000 - valid_acc: 0.1715 - epoch_time: 107.3591 s\n",
      " - test_loss: 2.2527 - test_reg: 0.000000 - test_acc: 0.1870 - test_time: 31.6046 s\n",
      "\n",
      "Total accuracy: 0.1870\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = [  ]\n",
    "\n",
    "for i in range(1): ####!!!!!!!!!!!!!!\n",
    "  path = 'model_to_test_' + str(i) + '.b'\n",
    "\n",
    "  model, criterion, optimizer, schedule_func, scheduler = training_setup()\n",
    "\n",
    "  checkpoint = train_model(model, path, train_dataset, valid_dataset, optimizer, criterion, scheduler, accumulation_steps, 150, 50, skip_eval=60)\n",
    "  \n",
    "  if checkpoint is None:\n",
    "    break\n",
    "  \n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  \n",
    "  _, _, acc = test(model, criterion, test_dataset)\n",
    "  test_accuracy.append(acc)\n",
    "\n",
    "test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "print(f'\\nTotal accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWwi3nEj50LQ"
   },
   "outputs": [],
   "source": [
    "0.1870"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "bpe_listops_setup_lka.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
