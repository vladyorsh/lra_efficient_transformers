{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce3729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lra.layers import *\n",
    "from lra.models import *\n",
    "from lra.setups import *\n",
    "from lra.utils  import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/yorshula/long-range-arena')\n",
    "\n",
    "from lra_benchmarks.listops.input_pipeline import get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa77154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/home/yorshula/lra_release/listops-1000/basic_train.tsv\n",
      "INFO:tensorflow:/home/yorshula/lra_release/listops-1000/basic_val.tsv\n",
      "INFO:tensorflow:/home/yorshula/lra_release/listops-1000/basic_test.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 13:58:17.814034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:17.816578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:17.863772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:17.866204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:17.868213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:17.870253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:17.873541: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-02 13:58:18.503821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:18.505917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:18.507703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:18.509391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:18.510994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:18.512681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:37.711227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:37.713619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:37.715706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:37.717456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:37.719217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:37.721065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 36541 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:01:01.0, compute capability: 8.0\n",
      "2022-05-02 13:58:37.722919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 13:58:37.724699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 36541 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:01:02.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished preprocessing\n",
      "INFO:tensorflow:Building vocab\n",
      "INFO:tensorflow:Processed 0\n",
      "INFO:tensorflow:Processed 1000\n",
      "INFO:tensorflow:Finished processing vocab size=15\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset, test_dataset, encoder = get_datasets(1, 'basic', data_dir='/home/yorshula/lra_release/listops-1000/', batch_size=LISTOPS_SETUP['batch_size'], max_length=LISTOPS_SETUP['max_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a41a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTOPS_SETUP['device'] = 'cuda:1'\n",
    "#LISTOPS_SETUP['model_type'] = ClassificationTransformerSkip\n",
    "\n",
    "def att_factory(hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    \n",
    "    lka = nn.Sequential(\n",
    "        AMGOLU(num_heads, qkv_dim, qkv_dim // num_heads // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False, LAMBDA=0.0),\n",
    "        AMGOLU(num_heads, qkv_dim, qkv_dim // num_heads // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False, LAMBDA=0.0),\n",
    "        AMGOLU(num_heads, qkv_dim, qkv_dim // num_heads // 4, dropout_rate, nn.Sigmoid(), nn.Softplus(), False, LAMBDA=0.0),\n",
    "        \n",
    "        #GatedOrthoKernel(num_heads, hidden_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False, LAMBDA=0.1),\n",
    "        #GatedOrthoKernel(num_heads, hidden_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False, LAMBDA=0.1),\n",
    "        #GatedOrthoKernel(num_heads, hidden_dim, dropout_rate, nn.Sigmoid(), nn.Softplus(), False, LAMBDA=0.1)\n",
    "\n",
    "        #HeadWiseFF(num_heads, qkv_dim, dropout_rate, nn.Softplus(), use_bias=False, LAMBDA=0.1),   \n",
    "    )\n",
    "    return LKAAttention(hidden_dim, qkv_dim, num_heads, dropout_rate, lka)\n",
    "    \n",
    "    #return FtAttention(hidden_dim, qkv_dim, num_heads, dropout_rate)\n",
    "    #return SimpleAttention(hidden_dim, qkv_dim, num_heads, dropout_rate, use_lin=True)\n",
    "    #return SimpleAttention(hidden_dim, qkv_dim, num_heads, dropout_rate, use_lin=False)\n",
    "\n",
    "model, criterion, optimizer, schedule_func, scheduler = training_setup(LISTOPS_SETUP, encoder.vocab_size, att_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db043b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "[====================] 10/10: - running_loss: 2.2850 - running_reg: 0.000000 - running_acc: 0.0948 - lr: 0.00000 - epoch_loss: 2.2835 - epoch_reg: 0.000000 - epoch_acc: 0.0938 - valid_loss: 2.2835 - valid_reg: 0.000000 - valid_acc: 0.1525 - epoch_time: 38.1596 s\n",
      "Epoch 1\n",
      "[=====>--------------] 3/10: - running_loss: 2.2465 - running_reg: 0.000000 - running_acc: 0.2068 - lr: 0.00000"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "LISTOPS_SETUP['device'] = 'cuda:1'\n",
    "#LISTOPS_SETUP['steps'] = 20\n",
    "#LISTOPS_SETUP['eval_period'] = 10\n",
    "\n",
    "test_accuracy = [  ]\n",
    "\n",
    "for i in range(1): ####!!!!!!!!!!!!!!\n",
    "  path = 'model_to_test_' + str(i) + '.b'\n",
    "\n",
    "  model, criterion, optimizer, schedule_func, scheduler = training_setup(LISTOPS_SETUP, encoder.vocab_size, att_factory)\n",
    "\n",
    "  checkpoint = train_listops_model(LISTOPS_SETUP, model, path, train_dataset, valid_dataset, optimizer, criterion, scheduler)\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  \n",
    "  _, _, acc = cls_test(model, criterion, test_dataset, LISTOPS_SETUP['device'])\n",
    "  test_accuracy.append(acc)\n",
    "\n",
    "test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "print(f'\\nTotal accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1065430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationTransformer(\n",
       "  (embed_layer): TEmbedding(\n",
       "    (embedding): Embedding(17, 512, padding_idx=0)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): TBlock(\n",
       "      (layernorm_input): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (layernorm_inter): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): LKAAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (lka): Sequential(\n",
       "          (0): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Softplus(beta=1, threshold=20)\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): TBlock(\n",
       "      (layernorm_input): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (layernorm_inter): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): LKAAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (lka): Sequential(\n",
       "          (0): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Softplus(beta=1, threshold=20)\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TBlock(\n",
       "      (layernorm_input): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (layernorm_inter): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): LKAAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (lka): Sequential(\n",
       "          (0): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Softplus(beta=1, threshold=20)\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): TBlock(\n",
       "      (layernorm_input): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (layernorm_inter): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): LKAAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (lka): Sequential(\n",
       "          (0): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Softplus(beta=1, threshold=20)\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): TBlock(\n",
       "      (layernorm_input): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (layernorm_inter): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): LKAAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (lka): Sequential(\n",
       "          (0): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Softplus(beta=1, threshold=20)\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): TBlock(\n",
       "      (layernorm_input): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (layernorm_inter): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): LKAAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (lka): Sequential(\n",
       "          (0): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Identity()\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AMGOLU(\n",
       "            (orth_weight): HWLinear()\n",
       "            (gate_weight_a): HWLinear()\n",
       "            (gate_weight_b): HWLinear()\n",
       "            (kernel_nonlinearity): Softplus(beta=1, threshold=20)\n",
       "            (gate_nonlinearity): Sigmoid()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (lin): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): TClassifier(\n",
       "    (layernorm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (ffn): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (1): GELU()\n",
       "    )\n",
       "    (output): Linear(in_features=2048, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504b4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
