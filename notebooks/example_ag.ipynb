{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db822979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ag.layers import *\n",
    "from ag.models import *\n",
    "from ag.utils  import *\n",
    "from ag.setups import *\n",
    "\n",
    "from lra.layers import AMGOLU, GatedOrthoKernel, HeadWiseFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6b5f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/yorshula/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04de87af304d4121929f530455f570bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/yorshula/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-af9af74624e7d37b.arrow\n",
      "Loading cached processed dataset at /home/yorshula/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-7c422186a4414477.arrow\n",
      "Loading cached processed dataset at /home/yorshula/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-fd92ef3d539dd8d5.arrow\n",
      "Loading cached processed dataset at /home/yorshula/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-6acea5ac1d887497.arrow\n",
      "Loading cached processed dataset at /home/yorshula/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-5df52efc21155600.arrow\n",
      "Loading cached processed dataset at /home/yorshula/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-1b62a5aa947227df.arrow\n",
      "Loading cached shuffled indices for dataset at /home/yorshula/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-ff8ab8523fda4d52.arrow\n",
      "Loading cached shuffled indices for dataset at /home/yorshula/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-848d1c7a6ad2d82a.arrow\n",
      "Loading cached shuffled indices for dataset at /home/yorshula/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-a22fc43a0c545ac2.arrow\n"
     ]
    }
   ],
   "source": [
    "ag_train, ag_valid, ag_test, tokenizer, b_model = get_data_tokenizer_bert(AG_SETUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa28c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unparsable param: token_type_embeddings.weight\n",
      "Cannot check param: blocks.0.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.0.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.0.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.0.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.0.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.0.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.0.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.0.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.0.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.1.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.1.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.1.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.1.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.1.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.1.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.1.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.1.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.1.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.2.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.2.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.2.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.2.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.2.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.2.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.2.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.2.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.2.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.3.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.3.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.3.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.3.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.3.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.3.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.3.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.3.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.3.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.4.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.4.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.4.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.4.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.4.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.4.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.4.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.4.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.4.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.5.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.5.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.5.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.5.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.5.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.5.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.5.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.5.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.5.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.6.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.6.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.6.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.6.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.6.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.6.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.6.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.6.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.6.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.7.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.7.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.7.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.7.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.7.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.7.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.7.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.7.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.7.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.8.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.8.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.8.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.8.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.8.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.8.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.8.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.8.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.8.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.9.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.9.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.9.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.9.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.9.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.9.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.9.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.9.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.9.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.10.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.10.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.10.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.10.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.10.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.10.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.10.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.10.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.10.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.11.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.11.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.11.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.11.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.11.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.11.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.11.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.11.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.11.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: classifier.output.weight\n",
      "Cannot check param: classifier.output.bias\n"
     ]
    }
   ],
   "source": [
    "#AG_SETUP['model_type'] = AGTransformerSkip\n",
    "\n",
    "def att_factory(hidden_dim, qkv_dim, num_heads, dropout_rate):\n",
    "    \n",
    "    lka = nn.Sequential(\n",
    "        AMGOLU(num_heads, qkv_dim, qkv_dim // num_heads // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False, LAMBDA=0.0),\n",
    "        AMGOLU(num_heads, qkv_dim, qkv_dim // num_heads // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False, LAMBDA=0.0),\n",
    "        AMGOLU(num_heads, qkv_dim, qkv_dim // num_heads // 4, dropout_rate, nn.Sigmoid(), nn.Softplus(), False, LAMBDA=0.0),\n",
    "        \n",
    "        #GatedOrthoKernel(num_heads, hidden_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False, LAMBDA=0.1),\n",
    "        #GatedOrthoKernel(num_heads, hidden_dim, dropout_rate, nn.Sigmoid(), nn.Identity(), False, LAMBDA=0.1),\n",
    "        #GatedOrthoKernel(num_heads, hidden_dim, dropout_rate, nn.Sigmoid(), nn.Softplus(), False, LAMBDA=0.1)\n",
    "\n",
    "        #HeadWiseFF(num_heads, qkv_dim, dropout_rate, nn.Softplus(), use_bias=False, LAMBDA=0.1),   \n",
    "    )\n",
    "    return LKAAttention(hidden_dim, qkv_dim, num_heads, dropout_rate, lka)\n",
    "    \n",
    "    #return FtAttention(hidden_dim, qkv_dim, num_heads, dropout_rate)\n",
    "    #return SimpleAttention(hidden_dim, qkv_dim, num_heads, dropout_rate, use_lin=True)\n",
    "    #return SimpleAttention(hidden_dim, qkv_dim, num_heads, dropout_rate, use_lin=False)\n",
    "\n",
    "model, criterion, optimizer, schedule_func, scheduler = coarse_training_setup(AG_SETUP, tokenizer, weight_donor=b_model, attention_factory=att_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "559e2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer, schedule_func, scheduler = fine_training_setup(AG_SETUP, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a66752e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unparsable param: token_type_embeddings.weight\n",
      "Cannot check param: blocks.0.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.0.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.0.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.0.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.0.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.0.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.0.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.0.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.0.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.1.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.1.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.1.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.1.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.1.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.1.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.1.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.1.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.1.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.2.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.2.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.2.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.2.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.2.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.2.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.2.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.2.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.2.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.3.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.3.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.3.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.3.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.3.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.3.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.3.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.3.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.3.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.4.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.4.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.4.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.4.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.4.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.4.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.4.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.4.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.4.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.5.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.5.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.5.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.5.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.5.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.5.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.5.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.5.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.5.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.6.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.6.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.6.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.6.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.6.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.6.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.6.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.6.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.6.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.7.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.7.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.7.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.7.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.7.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.7.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.7.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.7.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.7.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.8.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.8.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.8.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.8.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.8.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.8.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.8.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.8.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.8.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.9.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.9.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.9.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.9.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.9.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.9.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.9.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.9.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.9.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.10.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.10.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.10.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.10.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.10.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.10.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.10.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.10.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.10.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: blocks.11.attention.lka.0.orth_weight.weight\n",
      "Cannot check param: blocks.11.attention.lka.0.gate_weight_a.weight\n",
      "Cannot check param: blocks.11.attention.lka.0.gate_weight_b.weight\n",
      "Cannot check param: blocks.11.attention.lka.1.orth_weight.weight\n",
      "Cannot check param: blocks.11.attention.lka.1.gate_weight_a.weight\n",
      "Cannot check param: blocks.11.attention.lka.1.gate_weight_b.weight\n",
      "Cannot check param: blocks.11.attention.lka.2.orth_weight.weight\n",
      "Cannot check param: blocks.11.attention.lka.2.gate_weight_a.weight\n",
      "Cannot check param: blocks.11.attention.lka.2.gate_weight_b.weight\n",
      "Cannot check param: classifier.output.weight\n",
      "Cannot check param: classifier.output.bias\n",
      "Epoch 0\n",
      "[====================] 10/10: - running_loss: 1.3811 - running_reg: 0.000000 - running_acc: 0.2789 - lr: 0.00005000 - epoch_loss: 1.3813 - epoch_reg: 0.000000 - epoch_acc: 0.2781 - valid_loss: 1.3988 - valid_reg: 0.000000 - valid_acc: 0.2582 - epoch_time: 494.0757 s\n",
      "Epoch 0\n",
      "[====================] 10/10: - running_loss: 1.3857 - running_reg: 0.000000 - running_acc: 0.2660 - lr: 0.00000000 - epoch_loss: 1.3860 - epoch_reg: 0.000000 - epoch_acc: 0.2656 - valid_loss: 1.3988 - valid_reg: 0.000000 - valid_acc: 0.2582 - epoch_time: 534.5807 s\n",
      " - test_loss: 1.3936 - test_reg: 0.000000 - test_acc: 0.2682 - test_time: 102.1341 s\n",
      "\n",
      "Total accuracy: 0.2682\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "AG_SETUP['device'] = 'cuda:1'\n",
    "\n",
    "AG_SETUP['coarse_periods'] = 1\n",
    "AG_SETUP['coarse_eval_period'] = 10\n",
    "\n",
    "AG_SETUP['fine_periods'] = 1\n",
    "AG_SETUP['fine_eval_period'] = 10\n",
    "\n",
    "test_accuracy = [  ]\n",
    "\n",
    "for i in range(1): ####!!!!!!!!!!!!!!\n",
    "  path = 'model_to_test_' + str(i) + '.b'\n",
    "\n",
    "  #Tune only learnable kernels + embeddings + cls head\n",
    "  model, criterion, optimizer, schedule_func, scheduler = coarse_training_setup(AG_SETUP, tokenizer, weight_donor=b_model, attention_factory=att_factory)\n",
    "\n",
    "  checkpoint = train_ag_model(AG_SETUP, model, path, ag_train, ag_valid, optimizer, criterion, scheduler,\n",
    "                             AG_SETUP['coarse_periods'], AG_SETUP['coarse_eval_period'], AG_SETUP['coarse_skip_eval'])\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "  #Fine-tune the rest\n",
    "  model, criterion, optimizer, schedule_func, scheduler = fine_training_setup(AG_SETUP, model)\n",
    "  checkpoint = train_ag_model(AG_SETUP, model, path, ag_train, ag_valid, optimizer, criterion, scheduler,\n",
    "                             AG_SETUP['coarse_periods'], AG_SETUP['coarse_eval_period'], AG_SETUP['coarse_skip_eval'])\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "  _, _, acc = test_ag_model(AG_SETUP, model, criterion, ag_test)\n",
    "  test_accuracy.append(acc)\n",
    "\n",
    "test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "print(f'\\nTotal accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179014c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
